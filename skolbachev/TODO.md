**TODO list**
- FIX TOKENIZATION MULTITHREADING IN SCRIPT
- https://www.kaggle.com/jdpaletto/glove-global-vectors-for-word-representation/data (twitter embeddings)
- https://gist.github.com/cbaziotis/7ef97ccf71cbc14366835198c09809d2
- https://github.com/richliao/textClassifier/blob/master/textClassifierHATT.py
- https://github.com/thundergolfer/Insults
- BPE
- https://blog.openai.com/unsupervised-sentiment-neuron/
- https://github.com/Donskov7/toxic_comments/blob/master/data/correct_words.csv
- https://www.kaggle.com/nicapotato/bad-bad-words
- https://github.com/hannw/nlstm

**External data**
- https://conversationai.github.io/ | https://www.kaggle.com/eoveson/conversationaidataset (annotated)
- https://figshare.com/projects/Wikipedia_Talk/16731 (wiki talks datasets including both annotated and not)
- https://www.youtube.com/watch?v=5di0KlKl0fE (SpaCy's video on classification Reddit comments (insulted or not) with links to the data)
- http://daviseford.com/shittalk/