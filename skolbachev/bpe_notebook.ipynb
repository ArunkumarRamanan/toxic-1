{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-14T12:49:48.939293Z",
     "start_time": "2018-03-14T12:49:47.096628Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 7961730\n"
     ]
    }
   ],
   "source": [
    "# import importlib\n",
    "# import local_utils; importlib.reload(local_utils)\n",
    "from local_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-14T12:49:53.167209Z",
     "start_time": "2018-03-14T12:49:48.941019Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "explanation why the edits made under my username hardcore metallica fan were reverted? they weren't vandalisms, just closure on some gas after i voted at new york dolls fac. and please don't remove the template from the talk page since I'm retired now. \n",
      "\n",
      "Processed:\n",
      "explanation why the edits made under my username hardcore metallica fan were reverted? they weren't vandalisms, just closure on some gas after i voted at new york dolls fac. and please don't remove the template from the talk page since i'm retired now. \n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "ids, comments, Y, test_ids, test_comments, inx2label, label2inx = load_data(True)\n",
    "Y_wblank = np.concatenate([Y, np.expand_dims((~Y.any(axis=1)).astype(int), 1)], axis=1)\n",
    "print(\"Original:\\n\" + comments[0])\n",
    "print()\n",
    "\n",
    "comments = Parallel(n_jobs=cpu_cores)(delayed(preprocess)(text, True) for text in comments)\n",
    "test_comments = Parallel(n_jobs=cpu_cores)(delayed(preprocess)(text, True) for text in test_comments)\n",
    "print(\"Processed:\\n\" + comments[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-14T12:49:53.178648Z",
     "start_time": "2018-03-14T12:49:53.173464Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# bad_comments = np.extract(Y.sum(axis=1) > 0, comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-14T12:49:53.207316Z",
     "start_time": "2018-03-14T12:49:53.184122Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# # SentencePiece\n",
    "\n",
    "# with open('bpe/train.txt', 'w', encoding='utf-8') as file:\n",
    "#     for i in range(0, len(comments)):\n",
    "#         file.write(comments[i]+'\\n')\n",
    "\n",
    "# with open('bpe/test.txt', 'w', encoding='utf-8') as file:\n",
    "#     for i in range(0, len(test_comments)):\n",
    "#         file.write(test_comments[i]+'\\n')\n",
    "\n",
    "# spm_train --input=train.txt,test.txt --model_prefix=toxic --vocab_size=500 --model_type=unigram --num_threads 8\n",
    "# spm_export_vocab --model=toxic.model --output=toxic.vocab.txt\n",
    "\n",
    "# spm_encode --model=toxic.model --output_format=piece < train.txt > train_piece.txt\n",
    "# spm_encode --model=toxic.model --output_format=piece < test.txt > test_piece.txt\n",
    "\n",
    "# spm_encode --model=toxic.model --output_format=id < train.txt > train_id.txt\n",
    "# spm_encode --model=toxic.model --output_format=id < test.txt > test_id.txt\n",
    "\n",
    "# with open('bpe/train_piece.txt', 'r', encoding='utf-8') as file:\n",
    "#     piece_seq = [line.rstrip('\\n') for line in file]\n",
    "        \n",
    "# with open('bpe/test_piece.txt', 'r', encoding='utf-8') as file:\n",
    "#     test_piece_seq = [line.rstrip('\\n') for line in file]\n",
    "\n",
    "# with open('bpe/train_id.txt', 'r', encoding='utf-8') as file:\n",
    "#     id_seq = []\n",
    "#     for line in file:\n",
    "#         id_seq.append([int(i) for i in line.rstrip('\\n').split(' ')])    \n",
    "    \n",
    "# with open('bpe/test_id.txt', 'r', encoding='utf-8') as file:\n",
    "#     test_id_seq = []\n",
    "#     for line in file:\n",
    "#         test_id_seq.append([int(i) for i in line.rstrip('\\n').split(' ')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-14T12:50:06.520126Z",
     "start_time": "2018-03-14T12:49:53.212779Z"
    }
   },
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load(\"bpe/toxic.model\")\n",
    "\n",
    "def encode_ids(text):\n",
    "    return np.array(sp.EncodeAsIds(text))+1 # leaving 0 for padding\n",
    "\n",
    "def encode_pieces(text):\n",
    "    return sp.EncodeAsPieces(text)\n",
    "\n",
    "id_seq = Parallel(n_jobs=cpu_cores)(delayed(encode_ids)(comment) for comment in comments)\n",
    "test_id_seq = Parallel(n_jobs=cpu_cores)(delayed(encode_ids)(comment) for comment in test_comments)\n",
    "\n",
    "piece_seq = Parallel(n_jobs=cpu_cores)(delayed(encode_pieces)(comment) for comment in comments[:100])\n",
    "# test_piece_seq = Parallel(n_jobs=cpu_cores)(delayed(encode_pieces)(comment) for comment in test_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-14T12:50:09.143633Z",
     "start_time": "2018-03-14T12:50:06.522542Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[147.37829472  66.31955489]\n",
      "[249.71726691  53.14835103]\n",
      "646.8128285488364\n",
      "896.5300954620701\n"
     ]
    }
   ],
   "source": [
    "doc_len = []\n",
    "doc_ulen = []\n",
    "for seq in id_seq:\n",
    "    doc_len.append(len(seq))\n",
    "    doc_ulen.append(len(set(seq)))\n",
    "    \n",
    "for seq in test_id_seq:\n",
    "    doc_len.append(len(seq))\n",
    "    doc_ulen.append(len(set(seq)))\n",
    "    \n",
    "meta = np.stack([np.array(doc_len), np.array(doc_ulen)], axis=-1)\n",
    "meta_mean = meta.mean(axis=0)\n",
    "meta_std = meta.std(axis=0)\n",
    "meta = (meta - meta_mean)/meta_std\n",
    "\n",
    "print(meta_mean)\n",
    "print(meta_std)\n",
    "print(meta_mean[0]+2*meta_std[0])\n",
    "print(meta_mean[0]+3*meta_std[0])\n",
    "\n",
    "X_meta = meta[:len(comments)]\n",
    "test_X_meta = meta[len(comments):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-14T12:50:10.398390Z",
     "start_time": "2018-03-14T12:50:09.144571Z"
    }
   },
   "outputs": [],
   "source": [
    "max_len = 900\n",
    "# max_len = int(meta_mean[0]+3*meta_std[0])\n",
    "\n",
    "# PAD_TOKEN = \"_PAD_\"\n",
    "# pad_vec = np.zeros((1, emb_size))\n",
    "\n",
    "# from gensim.models import KeyedVectors\n",
    "# model = KeyedVectors.load_word2vec_format(\"bpe/bpemb/en.wiki.bpe.op\"+str(num_op)+\".d\"+str(emb_size)+\".w2v.bin\", binary=True)\n",
    "# emb_vectors = np.concatenate([pad_vec, model.syn0])\n",
    "# print(emb_vectors.shape)\n",
    "\n",
    "X = pad_sequences(id_seq, max_len)\n",
    "test_X = pad_sequences(test_id_seq, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-14T12:50:10.752084Z",
     "start_time": "2018-03-14T12:50:10.401761Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 143613, valid: 15958\n"
     ]
    }
   ],
   "source": [
    "# Train/Valid splitting\n",
    "trn_inx, val_inx = stratified_sampling(Y, 0.1, seed)\n",
    "\n",
    "print(\"train: {}, valid: {}\".format(len(trn_inx), len(val_inx)))\n",
    "# plot_stratified_sampling(Y, trn_inx, val_inx, inx2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-14T12:50:10.833373Z",
     "start_time": "2018-03-14T12:50:10.753369Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 900)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embs (Embedding)                (None, 900, 128)     64000       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 900, 128)     49280       embs[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 900, 128)     65664       embs[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 900, 128)     82048       embs[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 900, 512)     0           embs[0][0]                       \n",
      "                                                                 conv1d_1[0][0]                   \n",
      "                                                                 conv1d_2[0][0]                   \n",
      "                                                                 conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "attention_weighted_average_1 (A (None, 512)          512         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 6)            3078        attention_weighted_average_1[0][0\n",
      "==================================================================================================\n",
      "Total params: 264,582\n",
      "Trainable params: 264,582\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model description\n",
    "model_name = 'bpe_500_model'\n",
    "def getExpModel(num_words, emb_size, input_shape=X.shape[1], classes=Y.shape[1]):\n",
    "\n",
    "    x_input = Input(shape=(input_shape,))\n",
    "    \n",
    "    emb = Embedding(num_words, emb_size, trainable=True, input_length=input_shape, \n",
    "                    embeddings_regularizer=l1_l2(l1=1e-20),\n",
    "                    name='embs')(x_input)\n",
    "#     emb = SpatialDropout1D(0.25)(emb)\n",
    "        \n",
    "#     rnn = Bidirectional(CuDNNGRU(32, return_sequences=True))(emb)\n",
    "    \n",
    "    cnn1 = Conv1D(filters=128, kernel_size=3, activation='relu', padding='same')(emb)\n",
    "    cnn2 = Conv1D(filters=128, kernel_size=4, activation='relu', padding='same')(emb)\n",
    "    cnn3 = Conv1D(filters=128, kernel_size=5, activation='relu', padding='same')(emb)\n",
    "    \n",
    "    x = concatenate([emb, cnn1, cnn2, cnn3])\n",
    "    x = AttentionWeightedAverage()(x)\n",
    "#     x = Dropout(0.25)(x)\n",
    "\n",
    "    x_output = Dense(classes, activation='sigmoid')(x)\n",
    "    return Model(inputs=x_input, outputs=x_output)\n",
    "\n",
    "model = getExpModel(num_words=500, emb_size=128)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-14T12:50:10.837869Z",
     "start_time": "2018-03-14T12:50:10.834374Z"
    }
   },
   "outputs": [],
   "source": [
    "model_checkpoint = ModelCheckpoint(models_dir+model_name+'.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=6, verbose=1, mode='auto')\n",
    "lr_schedule = LearningRateScheduler(lr_change, verbose=1)\n",
    "lr_reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=1, min_lr=0.0001, verbose=1)\n",
    "# tensorboard = TensorBoard(log_dir='logs', write_graph=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-14T12:50:10.920699Z",
     "start_time": "2018-03-14T12:50:10.838805Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "val_batch_size = 1024\n",
    "weights = getClassWeights(Y, mu=0.5)\n",
    "\n",
    "# trn_seq = StratifiedFeatureSequence(X[trn_inx], Y[trn_inx], batch_size)\n",
    "# trn_seq = FeatureSequence(trn_X, trn_X_meta, trn_Y, batch_size, shuffle=True)\n",
    "trn_seq = FeatureSequence(X[trn_inx], Y[trn_inx], batch_size, shuffle=True)\n",
    "val_seq = FeatureSequence(X[val_inx], Y[val_inx], val_batch_size)\n",
    "roc_auc_eval = RocAucEvaluation(X[val_inx], Y[val_inx], batch_size=val_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-14T12:57:33.223134Z",
     "start_time": "2018-03-14T12:50:10.921872Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "561/561 [==============================] - 55s 98ms/step - loss: 0.1710 - val_loss: 0.1391\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.13911, saving model to models/bpe_500_model.h5\n",
      "ROC-AUC: 0.37293964\n",
      "\n",
      "\n",
      "Epoch 2/50\n",
      "561/561 [==============================] - 54s 96ms/step - loss: 0.1418 - val_loss: 0.1380\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.13911 to 0.13803, saving model to models/bpe_500_model.h5\n",
      "ROC-AUC: 0.51106823\n",
      "\n",
      "\n",
      "Epoch 3/50\n",
      "561/561 [==============================] - 54s 96ms/step - loss: 0.3442 - val_loss: 14.4751\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "ROC-AUC: 0.90783814\n",
      "\n",
      "\n",
      "Epoch 4/50\n",
      "561/561 [==============================] - 53s 95ms/step - loss: 1552.2986 - val_loss: 4338.0865\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0009000000427477062.\n",
      "ROC-AUC: 0.50341743\n",
      "\n",
      "\n",
      "Epoch 5/50\n",
      "561/561 [==============================] - 53s 95ms/step - loss: 4319.9593 - val_loss: 4304.5794\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0008100000384729356.\n",
      "ROC-AUC: 0.50342060\n",
      "\n",
      "\n",
      "Epoch 6/50\n",
      "561/561 [==============================] - 53s 95ms/step - loss: 4293.0119 - val_loss: 4282.0052\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0007290000503417104.\n",
      "ROC-AUC: 0.50342106\n",
      "\n",
      "\n",
      "Epoch 7/50\n",
      "561/561 [==============================] - 53s 95ms/step - loss: 4272.7828 - val_loss: 4263.7611\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0006561000715009868.\n",
      "ROC-AUC: 0.50341495\n",
      "\n",
      "\n",
      "Epoch 8/50\n",
      "561/561 [==============================] - 53s 95ms/step - loss: 4255.9086 - val_loss: 4248.1909\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0005904900433961303.\n",
      "ROC-AUC: 0.50339074\n",
      "\n",
      "\n",
      "Epoch 00008: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4714375d68>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs=50\n",
    "clr = CyclicLR(base_lr=0.001, max_lr=0.003, step_size=len(trn_seq), mode='triangular2')\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=optimizers.Nadam(0.001))\n",
    "# model.compile(loss=\"binary_crossentropy\", optimizer=optimizers.RMSprop(0.003))\n",
    "# model.compile(loss=art_loss, optimizer=optimizers.Nadam())\n",
    "\n",
    "model.fit_generator(\n",
    "    generator=trn_seq, steps_per_epoch=len(trn_seq),\n",
    "    validation_data=val_seq, validation_steps=len(val_seq),\n",
    "    initial_epoch=0, epochs=epochs, shuffle=False, verbose=1,\n",
    "#     callbacks=[model_checkpoint, clr, early_stop, roc_auc_eval],\n",
    "    callbacks=[model_checkpoint, lr_reduce, early_stop, roc_auc_eval],\n",
    "    use_multiprocessing=False, workers=cpu_cores, max_queue_size=8*cpu_cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-14T12:57:33.342337Z",
     "start_time": "2018-03-14T12:57:33.224115Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-4f76a9dad686>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-14T12:57:33.342859Z",
     "start_time": "2018-03-14T12:49:47.103Z"
    }
   },
   "outputs": [],
   "source": [
    "del model\n",
    "model = load_model(models_dir+model_name+'.h5', compile=True, \n",
    "                   custom_objects={'Attention':Attention, 'AttentionWeightedAverage':AttentionWeightedAverage, 'art_loss':art_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-14T12:57:33.343387Z",
     "start_time": "2018-03-14T12:49:47.105Z"
    }
   },
   "outputs": [],
   "source": [
    "Y_trn_pred = model.predict(X[trn_inx], batch_size=1024, verbose=0)\n",
    "losses = compute_losses(Y[trn_inx], Y_trn_pred, eps=1e-5)\n",
    "\n",
    "trn_loss = sum(losses)/len(losses)\n",
    "trn_auc = metrics.roc_auc_score(Y[trn_inx], Y_trn_pred)\n",
    "\n",
    "print()\n",
    "print(\"avg_loss: {}\".format(trn_loss))\n",
    "print(\"ROC AUC: {}\".format(trn_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-14T12:57:33.343957Z",
     "start_time": "2018-03-14T12:49:47.106Z"
    }
   },
   "outputs": [],
   "source": [
    "Y_val_pred = model.predict(X[val_inx], batch_size=1024, verbose=0)\n",
    "losses = compute_losses(Y[val_inx], Y_val_pred, eps=1e-5)\n",
    "\n",
    "val_loss = sum(losses)/len(losses)\n",
    "val_auc = metrics.roc_auc_score(Y[val_inx], Y_val_pred)\n",
    "\n",
    "print()\n",
    "print(\"avg_loss: {}\".format(val_loss))\n",
    "print(\"ROC AUC: {}\".format(val_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-14T12:57:33.344504Z",
     "start_time": "2018-03-14T12:49:47.107Z"
    }
   },
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-14T12:57:33.345006Z",
     "start_time": "2018-03-14T12:49:47.110Z"
    }
   },
   "outputs": [],
   "source": [
    "submission_name = 'bpe_submission_'+str(round(val_loss, 5))+'_'+str(round(val_auc, 5))+'.csv'\n",
    "\n",
    "sample_submission = pd.read_csv(data_dir + 'sample_submission.csv')\n",
    "test_pred = model.predict(test_X, batch_size=1024, verbose=1)\n",
    "sample_submission[inx2label] = test_pred\n",
    "sample_submission.to_csv(results_dir+submission_name, index=False)\n",
    "\n",
    "FileLink(results_dir+submission_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-14T12:57:33.345520Z",
     "start_time": "2018-03-14T12:49:47.111Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# from gensim.models.doc2vec import Doc2Vec, TaggedDocument, KeyedVectors\n",
    "\n",
    "# def build_tagged_docs(docs):\n",
    "#     return [TaggedDocument(tokens, [tag]) for tag, tokens in enumerate(docs)]\n",
    "\n",
    "# d2v_corpus = build_tagged_docs(text_analyzer.transform_to_tokens(piece_seq)+text_analyzer.transform_to_tokens(test_piece_seq))\n",
    "# print(len(d2v_corpus))\n",
    "\n",
    "# d2v_model = Doc2Vec(dm=0, dbow_words=1, size=300, window=12, min_count=0, iter=30, workers=cpu_cores)\n",
    "\n",
    "# %time d2v_model.build_vocab(d2v_corpus)\n",
    "# print(len(d2v_model.wv.vocab))\n",
    "\n",
    "# %time d2v_model.train(d2v_corpus, total_examples=d2v_model.corpus_count, epochs=d2v_model.iter)\n",
    "# d2v_model.save('bpe/bpe_d2v_model')\n",
    "\n",
    "# from gensim.models.doc2vec import Doc2Vec\n",
    "# d2v_model = Doc2Vec.load('bpe/bpe_d2v_model')\n",
    "\n",
    "# emb_matrix = [np.zeros(300)]\n",
    "# for w in text_analyzer.emb_words[1:]:\n",
    "#     emb_matrix.append(d2v_model.wv[w])\n",
    "# #     emb_matrix.append(d2v_model.wv[w]/np.linalg.norm(d2v_model.wv[w]))\n",
    "    \n",
    "# emb_matrix = np.stack(emb_matrix)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
