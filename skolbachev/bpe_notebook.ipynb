{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T09:25:51.701105Z",
     "start_time": "2018-02-18T09:25:50.567093Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 7961730\n"
     ]
    }
   ],
   "source": [
    "# import importlib\n",
    "# import local_utils; importlib.reload(local_utils)\n",
    "from local_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T09:25:52.672588Z",
     "start_time": "2018-02-18T09:25:51.702134Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Load Data\n",
    "ids, comments, Y, test_ids, test_comments, inx2label, label2inx = load_data()\n",
    "Y_wblank = np.concatenate([Y, np.expand_dims((~Y.any(axis=1)).astype(int), 1)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-18T09:36:06.156324Z",
     "start_time": "2018-02-18T09:36:05.854893Z"
    }
   },
   "outputs": [],
   "source": [
    "# bad_comments = np.extract(Y.sum(axis=1) > 0, comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-17T19:34:20.298264Z",
     "start_time": "2018-02-17T19:34:18.059399Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    return re.sub(\"\\t|\\n\", \" \", re.sub(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\", \" \", unidecode.unidecode(text.lower())))\n",
    "\n",
    "comments = Parallel(n_jobs=cpu_cores)(delayed(preprocess)(text) for text in comments)\n",
    "test_comments = Parallel(n_jobs=cpu_cores)(delayed(preprocess)(text) for text in test_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-17T19:34:20.307785Z",
     "start_time": "2018-02-17T19:34:20.300229Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# # SentencePiece\n",
    "\n",
    "# with open('bpe/train.txt', 'w', encoding='utf-8') as file:\n",
    "#     for i in range(0, len(comments)):\n",
    "#         file.write(comments[i]+'\\n')\n",
    "\n",
    "# with open('bpe/test.txt', 'w', encoding='utf-8') as file:\n",
    "#     for i in range(0, len(test_comments)):\n",
    "#         file.write(test_comments[i]+'\\n')\n",
    "\n",
    "# spm_train --input=train.txt,test.txt --model_prefix=toxic --vocab_size=8000 --model_type=unigram --num_threads 8\n",
    "# spm_export_vocab --model=toxic.model --output=toxic.vocab.txt\n",
    "\n",
    "# spm_encode --model=toxic.model --output_format=piece < train.txt > train_piece.txt\n",
    "# spm_encode --model=toxic.model --output_format=piece < test.txt > test_piece.txt\n",
    "\n",
    "# spm_encode --model=toxic.model --output_format=id < train.txt > train_id.txt\n",
    "# spm_encode --model=toxic.model --output_format=id < test.txt > test_id.txt\n",
    "\n",
    "# with open('bpe/train_piece.txt', 'r', encoding='utf-8') as file:\n",
    "#     piece_seq = [line.rstrip('\\n') for line in file]\n",
    "        \n",
    "# with open('bpe/test_piece.txt', 'r', encoding='utf-8') as file:\n",
    "#     test_piece_seq = [line.rstrip('\\n') for line in file]\n",
    "\n",
    "# with open('bpe/train_id.txt', 'r', encoding='utf-8') as file:\n",
    "#     id_seq = []\n",
    "#     for line in file:\n",
    "#         id_seq.append([int(i) for i in line.rstrip('\\n').split(' ')])    \n",
    "    \n",
    "# with open('bpe/test_id.txt', 'r', encoding='utf-8') as file:\n",
    "#     test_id_seq = []\n",
    "#     for line in file:\n",
    "#         test_id_seq.append([int(i) for i in line.rstrip('\\n').split(' ')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-17T19:34:34.098192Z",
     "start_time": "2018-02-17T19:34:20.309168Z"
    }
   },
   "outputs": [],
   "source": [
    "num_op = 50000\n",
    "\n",
    "import sentencepiece as spm\n",
    "sp = spm.SentencePieceProcessor()\n",
    "# sp.Load(\"bpe/bpemb/en.wiki.bpe.op\"+str(num_op)+\".model\")\n",
    "sp.Load(\"bpe/toxic.model\")\n",
    "\n",
    "def encode_ids(text):\n",
    "    return np.array(sp.EncodeAsIds(text))+1\n",
    "\n",
    "def encode_pieces(text):\n",
    "    return sp.EncodeAsPieces(text)\n",
    "\n",
    "id_seq = Parallel(n_jobs=cpu_cores)(delayed(encode_ids)(comment) for comment in comments)\n",
    "test_id_seq = Parallel(n_jobs=cpu_cores)(delayed(encode_ids)(comment) for comment in test_comments)\n",
    "\n",
    "piece_seq = Parallel(n_jobs=cpu_cores)(delayed(encode_pieces)(comment) for comment in comments[:100])\n",
    "# test_piece_seq = Parallel(n_jobs=cpu_cores)(delayed(encode_pieces)(comment) for comment in test_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-17T19:34:36.167973Z",
     "start_time": "2018-02-17T19:34:34.100020Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[94.04877612 56.04054551]\n",
      "[158.14612833  59.17649288]\n",
      "410.34103277846043\n",
      "568.4871611075118\n"
     ]
    }
   ],
   "source": [
    "doc_len = []\n",
    "doc_ulen = []\n",
    "for seq in id_seq:\n",
    "    doc_len.append(len(seq))\n",
    "    doc_ulen.append(len(set(seq)))\n",
    "    \n",
    "for seq in test_id_seq:\n",
    "    doc_len.append(len(seq))\n",
    "    doc_ulen.append(len(set(seq)))\n",
    "    \n",
    "meta = np.stack([np.array(doc_len), np.array(doc_ulen)], axis=-1)\n",
    "meta_mean = meta.mean(axis=0)\n",
    "meta_std = meta.std(axis=0)\n",
    "meta = (meta - meta_mean)/meta_std\n",
    "\n",
    "print(meta_mean)\n",
    "print(meta_std)\n",
    "print(meta_mean[0]+2*meta_std[0])\n",
    "print(meta_mean[0]+3*meta_std[0])\n",
    "\n",
    "X_meta = meta[:len(comments)]\n",
    "test_X_meta = meta[len(comments):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-17T19:34:37.216123Z",
     "start_time": "2018-02-17T19:34:36.169261Z"
    }
   },
   "outputs": [],
   "source": [
    "max_len = 600\n",
    "# max_len = int(meta_mean[0]+3*meta_std[0])\n",
    "emb_size = 50\n",
    "\n",
    "# PAD_TOKEN = \"_PAD_\"\n",
    "# pad_vec = np.zeros((1, emb_size))\n",
    "\n",
    "# from gensim.models import KeyedVectors\n",
    "# model = KeyedVectors.load_word2vec_format(\"bpe/bpemb/en.wiki.bpe.op\"+str(num_op)+\".d\"+str(emb_size)+\".w2v.bin\", binary=True)\n",
    "# emb_vectors = np.concatenate([pad_vec, model.syn0])\n",
    "# print(emb_vectors.shape)\n",
    "\n",
    "X = pad_sequences(id_seq, max_len)\n",
    "test_X = pad_sequences(test_id_seq, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-17T19:34:37.530146Z",
     "start_time": "2018-02-17T19:34:37.217331Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 143613, valid: 15958\n"
     ]
    }
   ],
   "source": [
    "# Train/Valid splitting\n",
    "trn_inx, val_inx = stratified_sampling(Y, 0.1, seed)\n",
    "\n",
    "print(\"train: {}, valid: {}\".format(len(trn_inx), len(val_inx)))\n",
    "# plot_stratified_sampling(Y, trn_inx, val_inx, inx2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-17T19:34:37.685623Z",
     "start_time": "2018-02-17T19:34:37.531145Z"
    },
    "code_folding": [
     0,
     2,
     40,
     50,
     63,
     77,
     87
    ]
   },
   "outputs": [],
   "source": [
    "# dpcnn\n",
    "from keras.initializers import RandomNormal\n",
    "def dpcnn(embedding_matrix=None, embedding_size=emb_size,\n",
    "          maxlen=max_len, max_features=8001,\n",
    "          filter_nr=64, kernel_size=3, repeat_block=2, dropout_convo=0.25,\n",
    "          dense_size=256, repeat_dense=0, dropout_dense=0.25,\n",
    "          l2_reg_convo=0.0, l2_reg_dense=0.0, use_prelu=True,\n",
    "          trainable_embedding=True, use_batch_norm=True):\n",
    "    \"\"\"\n",
    "    Note:\n",
    "        Implementation of http://ai.tencent.com/ailab/media/publications/ACL3-Brady.pdf\n",
    "        post activation is used instead of pre-activation, could be worth exploring\n",
    "    \"\"\"\n",
    "\n",
    "    input_text = Input(shape=(maxlen,))\n",
    "    if embedding_matrix is not None:\n",
    "        embedding = Embedding(max_features, embedding_size, weights=[embedding_matrix], trainable=trainable_embedding)(input_text)\n",
    "    else:\n",
    "        embedding = Embedding(max_features, embedding_size)(input_text)\n",
    "\n",
    "    embedding = SpatialDropout1D(0.15)(embedding)\n",
    "    \n",
    "    x = _convolutional_block(filter_nr, kernel_size, use_batch_norm, use_prelu, dropout_convo, l2_reg_convo)(embedding)\n",
    "    x = _convolutional_block(filter_nr, kernel_size, use_batch_norm, use_prelu, dropout_convo, l2_reg_convo)(x)\n",
    "    if embedding_size == filter_nr:\n",
    "        x = add([embedding, x])\n",
    "    else:\n",
    "        embedding_resized = _shape_matching_layer(filter_nr, use_prelu, dropout_convo, l2_reg_convo)(embedding)\n",
    "        x = add([embedding_resized, x])\n",
    "    for _ in range(repeat_block):\n",
    "        x = _dpcnn_block(filter_nr, kernel_size, use_batch_norm, use_prelu, dropout_convo, l2_reg_convo)(x)\n",
    "\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    for _ in range(repeat_dense):\n",
    "        x = _dense_block(dense_size, use_batch_norm, use_prelu, dropout_dense, l2_reg_dense)(x)\n",
    "    predictions = Dense(6, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = Model(inputs=input_text, outputs=predictions)\n",
    "    return model\n",
    "\n",
    "def _convolutional_block(filter_nr, kernel_size, use_batch_norm, use_prelu, dropout, l2_reg):\n",
    "    def f(x):\n",
    "        x = Conv1D(filter_nr, kernel_size=kernel_size, padding='same', activation='linear',\n",
    "                   kernel_initializer=RandomNormal(mean=0.0, stddev=0.001),\n",
    "                   kernel_regularizer=regularizers.l2(l2_reg))(x)\n",
    "        x = _bn_relu_dropout_block(use_batch_norm, use_prelu, dropout)(x)\n",
    "        return x\n",
    "\n",
    "    return f\n",
    "\n",
    "def _bn_relu_dropout_block(use_batch_norm, use_prelu, dropout):\n",
    "    def f(x):\n",
    "        if use_batch_norm:\n",
    "            x = BatchNormalization()(x)\n",
    "        if use_prelu:\n",
    "            x = PReLU()(x)\n",
    "        else:\n",
    "            x = Activation('relu')(x)\n",
    "        x = Dropout(dropout)(x)\n",
    "        return x\n",
    "\n",
    "    return f\n",
    "\n",
    "def _shape_matching_layer(filter_nr, use_prelu, dropout, l2_reg):\n",
    "    def f(x):\n",
    "        x = Conv1D(filter_nr, kernel_size=1, padding='same', activation='linear',\n",
    "                   kernel_initializer=RandomNormal(mean=0.0, stddev=0.001),\n",
    "                   kernel_regularizer=regularizers.l2(l2_reg))(x)\n",
    "        if use_prelu:\n",
    "            x = PReLU()(x)\n",
    "        else:\n",
    "            x = Activation('relu')(x)\n",
    "        x = Dropout(dropout)(x)\n",
    "        return x\n",
    "\n",
    "    return f\n",
    "\n",
    "def _dpcnn_block(filter_nr, kernel_size, use_batch_norm, use_prelu, dropout, l2_reg):\n",
    "    def f(x):\n",
    "        x = MaxPooling1D(pool_size=3, strides=2)(x)\n",
    "        main = _convolutional_block(filter_nr, kernel_size, use_batch_norm, use_prelu, dropout, l2_reg)(x)\n",
    "        main = _convolutional_block(filter_nr, kernel_size, use_batch_norm, use_prelu, dropout, l2_reg)(main)\n",
    "        x = add([main, x])\n",
    "        return x\n",
    "\n",
    "    return f\n",
    "\n",
    "def _dense_block(dense_size, use_batch_norm, use_prelu, dropout, l2_reg):\n",
    "    def f(x):\n",
    "        x = Dense(dense_size, activation='linear',\n",
    "                  kernel_regularizer=regularizers.l2(l2_reg))(x)\n",
    "        x = _bn_relu_dropout_block(use_batch_norm, use_prelu, dropout)(x)\n",
    "        return x\n",
    "\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-17T19:34:38.181497Z",
     "start_time": "2018-02-17T19:34:37.686562Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 600)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embs (Embedding)                (None, 600, 20)      160020      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 600, 20)      0           embs[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 600, 20)      0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 600, 40)      5040        spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_2 (SpatialDro (None, 600, 40)      0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 600, 40)      7440        spatial_dropout1d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_3 (SpatialDro (None, 600, 40)      0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 600, 100)     0           spatial_dropout1d_1[0][0]        \n",
      "                                                                 spatial_dropout1d_2[0][0]        \n",
      "                                                                 spatial_dropout1d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "attention_weighted_average_1 (A (None, 100)          100         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 6)            606         attention_weighted_average_1[0][0\n",
      "==================================================================================================\n",
      "Total params: 173,206\n",
      "Trainable params: 173,206\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model description\n",
    "# model_name = 'bpe_op'+str(num_op)+'_d'+str(emb_size)+'_model'\n",
    "model_name = 'bpe_8000_model'\n",
    "def getDMModel(num_words, emb_size=emb_size, input_shape=X.shape[1], classes=Y.shape[1]):\n",
    "\n",
    "    x_input = Input(shape=(input_shape,))\n",
    "    \n",
    "    emb = Embedding(num_words, emb_size, trainable=True, input_length=input_shape, \n",
    "#                     embeddings_regularizer=l1_l2(l2=1e-6),\n",
    "                    name='embs')(x_input)\n",
    "    emb = Activation('tanh')(emb)\n",
    "    emb = SpatialDropout1D(0.3)(emb)\n",
    "        \n",
    "    rnn1 = Bidirectional(CuDNNGRU(100, return_sequences=True))(emb)\n",
    "    rnn1 = SpatialDropout1D(0.3)(rnn1)\n",
    "    rnn2 = Bidirectional(CuDNNGRU(100, return_sequences=True))(rnn1)\n",
    "    rnn2 = SpatialDropout1D(0.3)(rnn2)\n",
    "    x = concatenate([emb, rnn1, rnn2])\n",
    "    x = AttentionWeightedAverage()(x)\n",
    "\n",
    "    x_output = Dense(classes, activation='sigmoid')(x)\n",
    "    return Model(inputs=x_input, outputs=x_output)\n",
    "\n",
    "model = getDMModel(num_words=8001)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-17T19:34:38.197027Z",
     "start_time": "2018-02-17T19:34:38.182570Z"
    }
   },
   "outputs": [],
   "source": [
    "model_loss_checkpoint = ModelCheckpoint(models_dir+model_name+'_loss.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "model_auc_checkpoint = ModelCheckpoint(models_dir+model_name+'_auc.h5', monitor='val_auc_roc', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "def lr_change(i, lr): \n",
    "    if (i == 0): return lr\n",
    "#     if (i == 3): return 0.001\n",
    "    return lr*0.85\n",
    "\n",
    "lr_schedule = LearningRateScheduler(lr_change, verbose=1)\n",
    "lr_reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.7, patience=1, min_lr=0.0001, verbose=1)\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=4, verbose=1, mode='auto')\n",
    "tensorboard = TensorBoard(log_dir='logs', write_graph=False)\n",
    "roc_auc_callback = ROCAUCCallback(X[val_inx], Y[val_inx], 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-17T19:35:07.602500Z",
     "start_time": "2018-02-17T19:34:38.198384Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size=512\n",
    "trn_seq = StratifiedFeatureSequence(X[trn_inx], Y[trn_inx], batch_size)\n",
    "# trn_seq = FeatureSequence(X[trn_inx], X_meta[trn_inx], Y[trn_inx], batch_size, shuffle=True)\n",
    "val_seq = FeatureSequence(X[val_inx], X_meta[val_inx], Y[val_inx], batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-17T19:35:07.606276Z",
     "start_time": "2018-02-17T19:35:07.603544Z"
    }
   },
   "outputs": [],
   "source": [
    "def getWeights(Y, mu=0.5):\n",
    "    return np.array([w for w in np.log(mu*Y.shape[0]/Y.sum(axis=0))])\n",
    "weights = getWeights(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-17T19:35:07.650633Z",
     "start_time": "2018-02-17T19:35:07.607156Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def roc_auc_loss(y_true, y_pred):\n",
    "    \"\"\" ROC AUC Score.\n",
    "    Approximates the Area Under Curve score, using approximation based on\n",
    "    the Wilcoxon-Mann-Whitney U statistic.\n",
    "    Yan, L., Dodier, R., Mozer, M. C., & Wolniewicz, R. (2003).\n",
    "    Optimizing Classifier Performance via an Approximation to the Wilcoxon-Mann-Whitney Statistic.\n",
    "    Measures overall performance for a full range of threshold levels.\n",
    "    Arguments:\n",
    "        y_pred: `Tensor`. Predicted values.\n",
    "        y_true: `Tensor` . Targets (labels), a probability distribution.\n",
    "    \"\"\"\n",
    "    with tf.name_scope(\"RocAucScore\"):\n",
    "\n",
    "        pos = tf.boolean_mask(y_pred, tf.cast(y_true, tf.bool))\n",
    "        neg = tf.boolean_mask(y_pred, ~tf.cast(y_true, tf.bool))\n",
    "\n",
    "        pos = tf.expand_dims(pos, 0)\n",
    "        neg = tf.expand_dims(neg, 1)\n",
    "\n",
    "        # original paper suggests performance is robust to exact parameter choice\n",
    "        gamma = 0.2\n",
    "        p     = 3 # 3\n",
    "\n",
    "        difference = tf.zeros_like(pos * neg) + pos - neg - gamma\n",
    "\n",
    "        masked = tf.boolean_mask(difference, difference < 0.0)\n",
    "\n",
    "        return tf.reduce_sum(tf.pow(-masked, p))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizers.RMSprop(0.003))\n",
    "# model.compile(loss=roc_auc_loss, optimizer=optimizers.RMSprop(0.003, clipnorm=1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-17T19:48:30.415034Z",
     "start_time": "2018-02-17T19:35:07.651589Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.003000000026077032.\n",
      "281/281 [==============================] - 53s 188ms/step - loss: 593.5352 - val_loss: 176.3427\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 176.34269, saving model to models/bpe_8000_model_loss.h5\n",
      "roc-auc_val: 0.97140664                                                                                                    \n",
      "Epoch 2/15\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.002550000022165477.\n",
      "281/281 [==============================] - 52s 185ms/step - loss: 191.1583 - val_loss: 153.8260\n",
      "\n",
      "Epoch 00002: val_loss improved from 176.34269 to 153.82601, saving model to models/bpe_8000_model_loss.h5\n",
      "roc-auc_val: 0.97421041                                                                                                    \n",
      "Epoch 3/15\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.0021675000782124696.\n",
      "281/281 [==============================] - 52s 186ms/step - loss: 161.0992 - val_loss: 137.8450\n",
      "\n",
      "Epoch 00003: val_loss improved from 153.82601 to 137.84505, saving model to models/bpe_8000_model_loss.h5\n",
      "roc-auc_val: 0.97677784                                                                                                    \n",
      "Epoch 4/15\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0018423750763759017.\n",
      "281/281 [==============================] - 52s 186ms/step - loss: 143.2202 - val_loss: 126.0292\n",
      "\n",
      "Epoch 00004: val_loss improved from 137.84505 to 126.02923, saving model to models/bpe_8000_model_loss.h5\n",
      "roc-auc_val: 0.97921053                                                                                                    \n",
      "Epoch 5/15\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.001566018775338307.\n",
      "281/281 [==============================] - 52s 185ms/step - loss: 129.2814 - val_loss: 122.4862\n",
      "\n",
      "Epoch 00005: val_loss improved from 126.02923 to 122.48616, saving model to models/bpe_8000_model_loss.h5\n",
      "roc-auc_val: 0.97968266                                                                                                    \n",
      "Epoch 6/15\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.0013311159738805145.\n",
      "281/281 [==============================] - 52s 186ms/step - loss: 121.2358 - val_loss: 118.1165\n",
      "\n",
      "Epoch 00006: val_loss improved from 122.48616 to 118.11653, saving model to models/bpe_8000_model_loss.h5\n",
      "roc-auc_val: 0.98032961                                                                                                    \n",
      "Epoch 7/15\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.0011314485629554838.\n",
      "281/281 [==============================] - 52s 186ms/step - loss: 113.7918 - val_loss: 117.8396\n",
      "\n",
      "Epoch 00007: val_loss improved from 118.11653 to 117.83965, saving model to models/bpe_8000_model_loss.h5\n",
      "roc-auc_val: 0.98039908                                                                                                    \n",
      "Epoch 8/15\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0009617312636692076.\n",
      "281/281 [==============================] - 52s 186ms/step - loss: 111.5036 - val_loss: 115.8534\n",
      "\n",
      "Epoch 00008: val_loss improved from 117.83965 to 115.85343, saving model to models/bpe_8000_model_loss.h5\n",
      "roc-auc_val: 0.98027249                                                                                                    \n",
      "Epoch 9/15\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.000817471559275873.\n",
      "281/281 [==============================] - 52s 186ms/step - loss: 107.5143 - val_loss: 115.4176\n",
      "\n",
      "Epoch 00009: val_loss improved from 115.85343 to 115.41763, saving model to models/bpe_8000_model_loss.h5\n",
      "roc-auc_val: 0.98048541                                                                                                    \n",
      "Epoch 10/15\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.0006948508031200618.\n",
      "281/281 [==============================] - 52s 186ms/step - loss: 104.8265 - val_loss: 113.9348\n",
      "\n",
      "Epoch 00010: val_loss improved from 115.41763 to 113.93481, saving model to models/bpe_8000_model_loss.h5\n",
      "roc-auc_val: 0.98092695                                                                                                    \n",
      "Epoch 11/15\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.0005906231875997036.\n",
      "281/281 [==============================] - 52s 186ms/step - loss: 103.5307 - val_loss: 112.0246\n",
      "\n",
      "Epoch 00011: val_loss improved from 113.93481 to 112.02464, saving model to models/bpe_8000_model_loss.h5\n",
      "roc-auc_val: 0.98132571                                                                                                    \n",
      "Epoch 12/15\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.000502029704512097.\n",
      "281/281 [==============================] - 52s 186ms/step - loss: 100.7997 - val_loss: 113.5966\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "roc-auc_val: 0.9813601                                                                                                    \n",
      "Epoch 13/15\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.0004267252661520615.\n",
      "281/281 [==============================] - 52s 186ms/step - loss: 98.3006 - val_loss: 114.5585\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "roc-auc_val: 0.98126643                                                                                                    \n",
      "Epoch 14/15\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.0003627164638601243.\n",
      "281/281 [==============================] - 52s 186ms/step - loss: 99.4549 - val_loss: 113.2787\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "roc-auc_val: 0.98146064                                                                                                    \n",
      "Epoch 15/15\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.00030830899922875684.\n",
      "281/281 [==============================] - 52s 186ms/step - loss: 97.7018 - val_loss: 114.3452\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "roc-auc_val: 0.98149985                                                                                                    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3901d88668>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs=15\n",
    "model.fit_generator(\n",
    "    generator=trn_seq, steps_per_epoch=len(trn_seq),\n",
    "    validation_data=val_seq, validation_steps=len(val_seq),\n",
    "    initial_epoch=0, epochs=epochs, shuffle=False, verbose=1,\n",
    "#     class_weight=weights,\n",
    "    callbacks=[model_loss_checkpoint, lr_schedule, roc_auc_callback],\n",
    "    use_multiprocessing=False, workers=cpu_cores, max_queue_size=4*cpu_cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-17T19:48:31.551317Z",
     "start_time": "2018-02-17T19:48:30.416461Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "del model\n",
    "model = load_model(models_dir+model_name+'_loss.h5', compile=True, \n",
    "                   custom_objects={'Attention':Attention, 'AttentionWeightedAverage':AttentionWeightedAverage, \n",
    "                                   'focal_loss':focal_loss, 'roc_auc_loss':roc_auc_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-17T19:48:42.560859Z",
     "start_time": "2018-02-17T19:48:31.552282Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "avg_loss: 0.22226397930351885\n",
      "ROC AUC: 0.9890397796329746\n"
     ]
    }
   ],
   "source": [
    "Y_trn_pred = model.predict(X[trn_inx], batch_size=1024, verbose=0)\n",
    "losses = compute_losses(Y[trn_inx], Y_trn_pred, eps=1e-5)\n",
    "\n",
    "trn_loss = sum(losses)/len(losses)\n",
    "trn_auc = metrics.roc_auc_score(Y[trn_inx], Y_trn_pred)\n",
    "\n",
    "print()\n",
    "print(\"avg_loss: {}\".format(trn_loss))\n",
    "print(\"ROC AUC: {}\".format(trn_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-17T19:48:44.640545Z",
     "start_time": "2018-02-17T19:48:42.561834Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "avg_loss: 0.22331672980188522\n",
      "ROC AUC: 0.9813257089824635\n"
     ]
    }
   ],
   "source": [
    "Y_val_pred = model.predict(X[val_inx], batch_size=512, verbose=0)\n",
    "losses = compute_losses(Y[val_inx], Y_val_pred, eps=1e-5)\n",
    "\n",
    "val_loss = sum(losses)/len(losses)\n",
    "val_auc = metrics.roc_auc_score(Y[val_inx], Y_val_pred)\n",
    "\n",
    "print()\n",
    "print(\"avg_loss: {}\".format(val_loss))\n",
    "print(\"ROC AUC: {}\".format(val_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-17T19:48:57.237857Z",
     "start_time": "2018-02-17T19:48:44.641598Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153164/153164 [==============================] - 11s 75us/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href='results/bpe_submission_0.22332_0.98133.csv' target='_blank'>results/bpe_submission_0.22332_0.98133.csv</a><br>"
      ],
      "text/plain": [
       "/src/DL/skolbachev/toxic/results/bpe_submission_0.22332_0.98133.csv"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_name = 'bpe_submission_'+str(round(val_loss, 5))+'_'+str(round(val_auc, 5))+'.csv'\n",
    "\n",
    "sample_submission = pd.read_csv(data_dir + 'sample_submission.csv')\n",
    "test_pred = model.predict(test_X, batch_size=1024, verbose=1)\n",
    "sample_submission[inx2label] = test_pred\n",
    "sample_submission.to_csv(results_dir+submission_name, index=False)\n",
    "\n",
    "FileLink(results_dir+submission_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-17T19:48:57.243271Z",
     "start_time": "2018-02-17T19:48:57.239179Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# from gensim.models.doc2vec import Doc2Vec, TaggedDocument, KeyedVectors\n",
    "\n",
    "# def build_tagged_docs(docs):\n",
    "#     return [TaggedDocument(tokens, [tag]) for tag, tokens in enumerate(docs)]\n",
    "\n",
    "# d2v_corpus = build_tagged_docs(text_analyzer.transform_to_tokens(piece_seq)+text_analyzer.transform_to_tokens(test_piece_seq))\n",
    "# print(len(d2v_corpus))\n",
    "\n",
    "# d2v_model = Doc2Vec(dm=0, dbow_words=1, size=300, window=12, min_count=0, iter=30, workers=cpu_cores)\n",
    "\n",
    "# %time d2v_model.build_vocab(d2v_corpus)\n",
    "# print(len(d2v_model.wv.vocab))\n",
    "\n",
    "# %time d2v_model.train(d2v_corpus, total_examples=d2v_model.corpus_count, epochs=d2v_model.iter)\n",
    "# d2v_model.save('bpe/bpe_d2v_model')\n",
    "\n",
    "# from gensim.models.doc2vec import Doc2Vec\n",
    "# d2v_model = Doc2Vec.load('bpe/bpe_d2v_model')\n",
    "\n",
    "# emb_matrix = [np.zeros(300)]\n",
    "# for w in text_analyzer.emb_words[1:]:\n",
    "#     emb_matrix.append(d2v_model.wv[w])\n",
    "# #     emb_matrix.append(d2v_model.wv[w]/np.linalg.norm(d2v_model.wv[w]))\n",
    "    \n",
    "# emb_matrix = np.stack(emb_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
