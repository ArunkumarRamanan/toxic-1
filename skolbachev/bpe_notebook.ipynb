{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T18:51:33.077667Z",
     "start_time": "2018-03-13T18:51:31.295997Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 7961730\n"
     ]
    }
   ],
   "source": [
    "# import importlib\n",
    "# import local_utils; importlib.reload(local_utils)\n",
    "from local_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T18:51:37.332483Z",
     "start_time": "2018-03-13T18:51:33.079389Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "explanation why the edits made under my username hardcore metallica fan were reverted? they weren't vandalisms, just closure on some gas after i voted at new york dolls fac. and please don't remove the template from the talk page since I'm retired now. \n",
      "\n",
      "Processed:\n",
      "explanation why the edits made under my username hardcore metallica fan were reverted? they weren't vandalisms, just closure on some gas after i voted at new york dolls fac. and please don't remove the template from the talk page since i'm retired now. \n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "ids, comments, Y, test_ids, test_comments, inx2label, label2inx = load_data(True)\n",
    "Y_wblank = np.concatenate([Y, np.expand_dims((~Y.any(axis=1)).astype(int), 1)], axis=1)\n",
    "print(\"Original:\\n\" + comments[0])\n",
    "print()\n",
    "\n",
    "comments = Parallel(n_jobs=cpu_cores)(delayed(preprocess)(text, True) for text in comments)\n",
    "test_comments = Parallel(n_jobs=cpu_cores)(delayed(preprocess)(text, True) for text in test_comments)\n",
    "print(\"Processed:\\n\" + comments[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T18:51:37.342119Z",
     "start_time": "2018-03-13T18:51:37.337458Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# bad_comments = np.extract(Y.sum(axis=1) > 0, comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T18:51:37.369377Z",
     "start_time": "2018-03-13T18:51:37.347637Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# # SentencePiece\n",
    "\n",
    "# with open('bpe/train.txt', 'w', encoding='utf-8') as file:\n",
    "#     for i in range(0, len(comments)):\n",
    "#         file.write(comments[i]+'\\n')\n",
    "\n",
    "# with open('bpe/test.txt', 'w', encoding='utf-8') as file:\n",
    "#     for i in range(0, len(test_comments)):\n",
    "#         file.write(test_comments[i]+'\\n')\n",
    "\n",
    "# spm_train --input=train.txt,test.txt --model_prefix=toxic --vocab_size=500 --model_type=unigram --num_threads 8\n",
    "# spm_export_vocab --model=toxic.model --output=toxic.vocab.txt\n",
    "\n",
    "# spm_encode --model=toxic.model --output_format=piece < train.txt > train_piece.txt\n",
    "# spm_encode --model=toxic.model --output_format=piece < test.txt > test_piece.txt\n",
    "\n",
    "# spm_encode --model=toxic.model --output_format=id < train.txt > train_id.txt\n",
    "# spm_encode --model=toxic.model --output_format=id < test.txt > test_id.txt\n",
    "\n",
    "# with open('bpe/train_piece.txt', 'r', encoding='utf-8') as file:\n",
    "#     piece_seq = [line.rstrip('\\n') for line in file]\n",
    "        \n",
    "# with open('bpe/test_piece.txt', 'r', encoding='utf-8') as file:\n",
    "#     test_piece_seq = [line.rstrip('\\n') for line in file]\n",
    "\n",
    "# with open('bpe/train_id.txt', 'r', encoding='utf-8') as file:\n",
    "#     id_seq = []\n",
    "#     for line in file:\n",
    "#         id_seq.append([int(i) for i in line.rstrip('\\n').split(' ')])    \n",
    "    \n",
    "# with open('bpe/test_id.txt', 'r', encoding='utf-8') as file:\n",
    "#     test_id_seq = []\n",
    "#     for line in file:\n",
    "#         test_id_seq.append([int(i) for i in line.rstrip('\\n').split(' ')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T18:51:49.889393Z",
     "start_time": "2018-03-13T18:51:37.375094Z"
    }
   },
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load(\"bpe/toxic.model\")\n",
    "\n",
    "def encode_ids(text):\n",
    "    return np.array(sp.EncodeAsIds(text))+1 # leaving 0 for padding\n",
    "\n",
    "def encode_pieces(text):\n",
    "    return sp.EncodeAsPieces(text)\n",
    "\n",
    "id_seq = Parallel(n_jobs=cpu_cores)(delayed(encode_ids)(comment) for comment in comments)\n",
    "test_id_seq = Parallel(n_jobs=cpu_cores)(delayed(encode_ids)(comment) for comment in test_comments)\n",
    "\n",
    "piece_seq = Parallel(n_jobs=cpu_cores)(delayed(encode_pieces)(comment) for comment in comments[:100])\n",
    "# test_piece_seq = Parallel(n_jobs=cpu_cores)(delayed(encode_pieces)(comment) for comment in test_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T18:51:52.543857Z",
     "start_time": "2018-03-13T18:51:49.891385Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[147.37829472  66.31955489]\n",
      "[249.71726691  53.14835103]\n",
      "646.8128285488364\n",
      "896.5300954620701\n"
     ]
    }
   ],
   "source": [
    "doc_len = []\n",
    "doc_ulen = []\n",
    "for seq in id_seq:\n",
    "    doc_len.append(len(seq))\n",
    "    doc_ulen.append(len(set(seq)))\n",
    "    \n",
    "for seq in test_id_seq:\n",
    "    doc_len.append(len(seq))\n",
    "    doc_ulen.append(len(set(seq)))\n",
    "    \n",
    "meta = np.stack([np.array(doc_len), np.array(doc_ulen)], axis=-1)\n",
    "meta_mean = meta.mean(axis=0)\n",
    "meta_std = meta.std(axis=0)\n",
    "meta = (meta - meta_mean)/meta_std\n",
    "\n",
    "print(meta_mean)\n",
    "print(meta_std)\n",
    "print(meta_mean[0]+2*meta_std[0])\n",
    "print(meta_mean[0]+3*meta_std[0])\n",
    "\n",
    "X_meta = meta[:len(comments)]\n",
    "test_X_meta = meta[len(comments):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T18:51:53.765496Z",
     "start_time": "2018-03-13T18:51:52.544896Z"
    }
   },
   "outputs": [],
   "source": [
    "max_len = 900\n",
    "# max_len = int(meta_mean[0]+3*meta_std[0])\n",
    "\n",
    "# PAD_TOKEN = \"_PAD_\"\n",
    "# pad_vec = np.zeros((1, emb_size))\n",
    "\n",
    "# from gensim.models import KeyedVectors\n",
    "# model = KeyedVectors.load_word2vec_format(\"bpe/bpemb/en.wiki.bpe.op\"+str(num_op)+\".d\"+str(emb_size)+\".w2v.bin\", binary=True)\n",
    "# emb_vectors = np.concatenate([pad_vec, model.syn0])\n",
    "# print(emb_vectors.shape)\n",
    "\n",
    "X = pad_sequences(id_seq, max_len)\n",
    "test_X = pad_sequences(test_id_seq, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T18:51:54.092937Z",
     "start_time": "2018-03-13T18:51:53.766519Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 143613, valid: 15958\n"
     ]
    }
   ],
   "source": [
    "# Train/Valid splitting\n",
    "trn_inx, val_inx = stratified_sampling(Y, 0.1, seed)\n",
    "\n",
    "print(\"train: {}, valid: {}\".format(len(trn_inx), len(val_inx)))\n",
    "# plot_stratified_sampling(Y, trn_inx, val_inx, inx2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T18:51:54.170133Z",
     "start_time": "2018-03-13T18:51:54.093933Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 900)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embs (Embedding)                (None, 900, 128)     64000       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 900, 128)     49280       embs[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 900, 128)     65664       embs[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 900, 128)     82048       embs[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 900, 512)     0           embs[0][0]                       \n",
      "                                                                 conv1d_1[0][0]                   \n",
      "                                                                 conv1d_2[0][0]                   \n",
      "                                                                 conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "attention_weighted_average_1 (A (None, 512)          512         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 6)            3078        attention_weighted_average_1[0][0\n",
      "==================================================================================================\n",
      "Total params: 264,582\n",
      "Trainable params: 264,582\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model description\n",
    "model_name = 'bpe_500_model'\n",
    "def getExpModel(num_words, emb_size, input_shape=X.shape[1], classes=Y.shape[1]):\n",
    "\n",
    "    x_input = Input(shape=(input_shape,))\n",
    "    \n",
    "    emb = Embedding(num_words, emb_size, trainable=True, input_length=input_shape, \n",
    "                    embeddings_regularizer=l1_l2(l1=1e-15),\n",
    "                    name='embs')(x_input)\n",
    "#     emb = SpatialDropout1D(0.25)(emb)\n",
    "        \n",
    "#     rnn = Bidirectional(CuDNNGRU(32, return_sequences=True))(emb)\n",
    "    \n",
    "    cnn1 = Conv1D(filters=128, kernel_size=3, activation='relu', padding='same')(emb)\n",
    "    cnn2 = Conv1D(filters=128, kernel_size=4, activation='relu', padding='same')(emb)\n",
    "    cnn3 = Conv1D(filters=128, kernel_size=5, activation='relu', padding='same')(emb)\n",
    "    \n",
    "    x = concatenate([emb, cnn1, cnn2, cnn3])\n",
    "    x = AttentionWeightedAverage()(x)\n",
    "#     x = Dropout(0.25)(x)\n",
    "\n",
    "    x_output = Dense(classes, activation='sigmoid')(x)\n",
    "    return Model(inputs=x_input, outputs=x_output)\n",
    "\n",
    "model = getExpModel(num_words=500, emb_size=128)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T18:51:54.174565Z",
     "start_time": "2018-03-13T18:51:54.171114Z"
    }
   },
   "outputs": [],
   "source": [
    "model_checkpoint = ModelCheckpoint(models_dir+model_name+'.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=6, verbose=1, mode='auto')\n",
    "lr_schedule = LearningRateScheduler(lr_change, verbose=1)\n",
    "lr_reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=1, min_lr=0.0001, verbose=1)\n",
    "# tensorboard = TensorBoard(log_dir='logs', write_graph=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T18:51:54.255764Z",
     "start_time": "2018-03-13T18:51:54.175446Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "val_batch_size = 1024\n",
    "weights = getClassWeights(Y, mu=0.5)\n",
    "\n",
    "# trn_seq = StratifiedFeatureSequence(X[trn_inx], Y[trn_inx], batch_size)\n",
    "# trn_seq = FeatureSequence(trn_X, trn_X_meta, trn_Y, batch_size, shuffle=True)\n",
    "trn_seq = FeatureSequence(X[trn_inx], Y[trn_inx], batch_size, shuffle=True)\n",
    "val_seq = FeatureSequence(X[val_inx], Y[val_inx], val_batch_size)\n",
    "roc_auc_eval = RocAucEvaluation(X[val_inx], Y[val_inx], batch_size=val_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T19:38:04.471502Z",
     "start_time": "2018-03-13T18:51:54.256912Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "561/561 [==============================] - 54s 97ms/step - loss: 0.1709 - val_loss: 0.1391\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.13911, saving model to models/bpe_500_model.h5\n",
      "ROC-AUC: 0.37338236\n",
      "\n",
      "\n",
      "Epoch 2/50\n",
      "561/561 [==============================] - 53s 95ms/step - loss: 0.1418 - val_loss: 0.1380\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.13911 to 0.13803, saving model to models/bpe_500_model.h5\n",
      "ROC-AUC: 0.50515662\n",
      "\n",
      "\n",
      "Epoch 3/50\n",
      "561/561 [==============================] - 54s 95ms/step - loss: 0.1401 - val_loss: 0.1282\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.13803 to 0.12816, saving model to models/bpe_500_model.h5\n",
      "ROC-AUC: 0.80959213\n",
      "\n",
      "\n",
      "Epoch 4/50\n",
      "561/561 [==============================] - 53s 95ms/step - loss: 0.0941 - val_loss: 0.0817\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.12816 to 0.08169, saving model to models/bpe_500_model.h5\n",
      "ROC-AUC: 0.94227211\n",
      "\n",
      "\n",
      "Epoch 5/50\n",
      "561/561 [==============================] - 54s 96ms/step - loss: 0.0774 - val_loss: 0.0761\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.08169 to 0.07609, saving model to models/bpe_500_model.h5\n",
      "ROC-AUC: 0.94787027\n",
      "\n",
      "\n",
      "Epoch 6/50\n",
      "561/561 [==============================] - 54s 95ms/step - loss: 0.0729 - val_loss: 0.0719\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.07609 to 0.07189, saving model to models/bpe_500_model.h5\n",
      "ROC-AUC: 0.95644517\n",
      "\n",
      "\n",
      "Epoch 7/50\n",
      "561/561 [==============================] - 54s 96ms/step - loss: 0.0702 - val_loss: 0.0702\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.07189 to 0.07021, saving model to models/bpe_500_model.h5\n",
      "ROC-AUC: 0.95322236\n",
      "\n",
      "\n",
      "Epoch 8/50\n",
      "561/561 [==============================] - 54s 96ms/step - loss: 0.0684 - val_loss: 0.0679\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.07021 to 0.06792, saving model to models/bpe_500_model.h5\n",
      "ROC-AUC: 0.96150166\n",
      "\n",
      "\n",
      "Epoch 9/50\n",
      "561/561 [==============================] - 54s 96ms/step - loss: 0.0670 - val_loss: 0.0667\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.06792 to 0.06669, saving model to models/bpe_500_model.h5\n",
      "ROC-AUC: 0.96186829\n",
      "\n",
      "\n",
      "Epoch 10/50\n",
      "561/561 [==============================] - 54s 96ms/step - loss: 0.0662 - val_loss: 0.0661\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.06669 to 0.06606, saving model to models/bpe_500_model.h5\n",
      "ROC-AUC: 0.96258736\n",
      "\n",
      "\n",
      "Epoch 11/50\n",
      "561/561 [==============================] - 54s 96ms/step - loss: 0.0651 - val_loss: 0.0649\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.06606 to 0.06489, saving model to models/bpe_500_model.h5\n",
      "ROC-AUC: 0.96313197\n",
      "\n",
      "\n",
      "Epoch 12/50\n",
      "561/561 [==============================] - 54s 96ms/step - loss: 0.0646 - val_loss: 0.0659\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "ROC-AUC: 0.96101757\n",
      "\n",
      "\n",
      "Epoch 13/50\n",
      "561/561 [==============================] - 54s 96ms/step - loss: 0.0642 - val_loss: 0.0638\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.06489 to 0.06384, saving model to models/bpe_500_model.h5\n",
      "ROC-AUC: 0.96348529\n",
      "\n",
      "\n",
      "Epoch 14/50\n",
      "561/561 [==============================] - 54s 96ms/step - loss: 0.0635 - val_loss: 0.0646\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "ROC-AUC: 0.96482320\n",
      "\n",
      "\n",
      "Epoch 15/50\n",
      "561/561 [==============================] - 54s 96ms/step - loss: 0.0631 - val_loss: 0.0631\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.06384 to 0.06312, saving model to models/bpe_500_model.h5\n",
      "ROC-AUC: 0.96569932\n",
      "\n",
      "\n",
      "Epoch 16/50\n",
      "561/561 [==============================] - 54s 96ms/step - loss: 0.0629 - val_loss: 0.0629\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.06312 to 0.06290, saving model to models/bpe_500_model.h5\n",
      "ROC-AUC: 0.96359445\n",
      "\n",
      "\n",
      "Epoch 17/50\n",
      "561/561 [==============================] - 54s 96ms/step - loss: 0.0623 - val_loss: 0.0617\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.06290 to 0.06168, saving model to models/bpe_500_model.h5\n",
      "ROC-AUC: 0.96584219\n",
      "\n",
      "\n",
      "Epoch 18/50\n",
      "561/561 [==============================] - 54s 96ms/step - loss: 0.0619 - val_loss: 0.0616\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.06168 to 0.06163, saving model to models/bpe_500_model.h5\n",
      "ROC-AUC: 0.96839277\n",
      "\n",
      "\n",
      "Epoch 19/50\n",
      "561/561 [==============================] - 54s 96ms/step - loss: 0.0617 - val_loss: 0.0626\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0009000000427477062.\n",
      "ROC-AUC: 0.96702035\n",
      "\n",
      "\n",
      "Epoch 20/50\n",
      "561/561 [==============================] - 54s 96ms/step - loss: 0.0608 - val_loss: 0.0623\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0008100000384729356.\n",
      "ROC-AUC: 0.96701688\n",
      "\n",
      "\n",
      "Epoch 21/50\n",
      "561/561 [==============================] - 54s 96ms/step - loss: 0.0597 - val_loss: 0.0600\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.06163 to 0.05995, saving model to models/bpe_500_model.h5\n",
      "ROC-AUC: 0.96980766\n",
      "\n",
      "\n",
      "Epoch 22/50\n",
      "561/561 [==============================] - 54s 96ms/step - loss: 0.0595 - val_loss: 0.0600\n",
      "\n",
      "Epoch 00022: val_loss did not improve\n",
      "ROC-AUC: 0.96817136\n",
      "\n",
      "\n",
      "Epoch 23/50\n",
      "561/561 [==============================] - 54s 96ms/step - loss: 0.0595 - val_loss: 0.0597\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.05995 to 0.05972, saving model to models/bpe_500_model.h5\n",
      "ROC-AUC: 0.97063295\n",
      "\n",
      "\n",
      "Epoch 24/50\n",
      "561/561 [==============================] - 54s 96ms/step - loss: 0.0591 - val_loss: 0.0610\n",
      "\n",
      "Epoch 00024: val_loss did not improve\n",
      "ROC-AUC: 0.96756056\n",
      "\n",
      "\n",
      "Epoch 25/50\n",
      "561/561 [==============================] - 53s 95ms/step - loss: 0.0589 - val_loss: 0.0611\n",
      "\n",
      "Epoch 00025: val_loss did not improve\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.0007290000503417104.\n",
      "ROC-AUC: 0.96724752\n",
      "\n",
      "\n",
      "Epoch 26/50\n",
      "561/561 [==============================] - 54s 96ms/step - loss: 0.0580 - val_loss: 0.0597\n",
      "\n",
      "Epoch 00026: val_loss did not improve\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.0006561000715009868.\n",
      "ROC-AUC: 0.96949424\n",
      "\n",
      "\n",
      "Epoch 27/50\n",
      "561/561 [==============================] - 54s 96ms/step - loss: 0.0572 - val_loss: 0.0586\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.05972 to 0.05859, saving model to models/bpe_500_model.h5\n",
      "ROC-AUC: 0.96780517\n",
      "\n",
      "\n",
      "Epoch 28/50\n",
      "561/561 [==============================] - 54s 96ms/step - loss: 0.0570 - val_loss: 0.0583\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.05859 to 0.05832, saving model to models/bpe_500_model.h5\n",
      "ROC-AUC: 0.97194876\n",
      "\n",
      "\n",
      "Epoch 29/50\n",
      "561/561 [==============================] - 54s 96ms/step - loss: 0.0569 - val_loss: 0.0579\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.05832 to 0.05789, saving model to models/bpe_500_model.h5\n",
      "ROC-AUC: 0.96847340\n",
      "\n",
      "\n",
      "Epoch 30/50\n",
      "561/561 [==============================] - 54s 96ms/step - loss: 0.0567 - val_loss: 0.0587\n",
      "\n",
      "Epoch 00030: val_loss did not improve\n",
      "ROC-AUC: 0.97086633\n",
      "\n",
      "\n",
      "Epoch 31/50\n",
      "561/561 [==============================] - 54s 96ms/step - loss: 0.0567 - val_loss: 0.0577\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.05789 to 0.05775, saving model to models/bpe_500_model.h5\n",
      "ROC-AUC: 0.97041976\n",
      "\n",
      "\n",
      "Epoch 32/50\n",
      "561/561 [==============================] - 54s 95ms/step - loss: 0.0563 - val_loss: 0.0575\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.05775 to 0.05753, saving model to models/bpe_500_model.h5\n",
      "ROC-AUC: 0.97136946\n",
      "\n",
      "\n",
      "Epoch 33/50\n",
      "561/561 [==============================] - 54s 96ms/step - loss: 0.0563 - val_loss: 0.0584\n",
      "\n",
      "Epoch 00033: val_loss did not improve\n",
      "ROC-AUC: 0.97058390\n",
      "\n",
      "\n",
      "Epoch 34/50\n",
      "561/561 [==============================] - 54s 96ms/step - loss: 0.0562 - val_loss: 0.0579\n",
      "\n",
      "Epoch 00034: val_loss did not improve\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 0.0005904900433961303.\n",
      "ROC-AUC: 0.96976782\n",
      "\n",
      "\n",
      "Epoch 35/50\n",
      "561/561 [==============================] - 54s 96ms/step - loss: 0.0554 - val_loss: 0.0575\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.05753 to 0.05747, saving model to models/bpe_500_model.h5\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.0005314410547725857.\n",
      "ROC-AUC: 0.97334361\n",
      "\n",
      "\n",
      "Epoch 36/50\n",
      "561/561 [==============================] - 54s 96ms/step - loss: 0.0546 - val_loss: 0.0560\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.05747 to 0.05596, saving model to models/bpe_500_model.h5\n",
      "ROC-AUC: 0.97226235\n",
      "\n",
      "\n",
      "Epoch 37/50\n",
      "561/561 [==============================] - 54s 95ms/step - loss: 0.0545 - val_loss: 0.0568\n",
      "\n",
      "Epoch 00037: val_loss did not improve\n",
      "ROC-AUC: 0.97144751\n",
      "\n",
      "\n",
      "Epoch 38/50\n",
      "561/561 [==============================] - 54s 96ms/step - loss: 0.0544 - val_loss: 0.0567\n",
      "\n",
      "Epoch 00038: val_loss did not improve\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 0.00047829695977270604.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC: 0.97278412\n",
      "\n",
      "\n",
      "Epoch 39/50\n",
      "561/561 [==============================] - 54s 96ms/step - loss: 0.0538 - val_loss: 0.0567\n",
      "\n",
      "Epoch 00039: val_loss did not improve\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 0.0004304672533180565.\n",
      "ROC-AUC: 0.97283819\n",
      "\n",
      "\n",
      "Epoch 40/50\n",
      "561/561 [==============================] - 54s 96ms/step - loss: 0.0531 - val_loss: 0.0558\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.05596 to 0.05576, saving model to models/bpe_500_model.h5\n",
      "ROC-AUC: 0.97402480\n",
      "\n",
      "\n",
      "Epoch 41/50\n",
      "561/561 [==============================] - 54s 96ms/step - loss: 0.0530 - val_loss: 0.0552\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.05576 to 0.05522, saving model to models/bpe_500_model.h5\n",
      "ROC-AUC: 0.97330703\n",
      "\n",
      "\n",
      "Epoch 42/50\n",
      "561/561 [==============================] - 54s 96ms/step - loss: 0.0528 - val_loss: 0.0546\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.05522 to 0.05462, saving model to models/bpe_500_model.h5\n",
      "ROC-AUC: 0.97372208\n",
      "\n",
      "\n",
      "Epoch 43/50\n",
      "561/561 [==============================] - 54s 96ms/step - loss: 0.0528 - val_loss: 0.0554\n",
      "\n",
      "Epoch 00043: val_loss did not improve\n",
      "ROC-AUC: 0.97321889\n",
      "\n",
      "\n",
      "Epoch 44/50\n",
      "561/561 [==============================] - 54s 96ms/step - loss: 0.0527 - val_loss: 0.0544\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.05462 to 0.05439, saving model to models/bpe_500_model.h5\n",
      "ROC-AUC: 0.97440630\n",
      "\n",
      "\n",
      "Epoch 45/50\n",
      "561/561 [==============================] - 54s 96ms/step - loss: 0.0525 - val_loss: 0.0556\n",
      "\n",
      "Epoch 00045: val_loss did not improve\n",
      "ROC-AUC: 0.97377413\n",
      "\n",
      "\n",
      "Epoch 46/50\n",
      "561/561 [==============================] - 54s 96ms/step - loss: 0.0526 - val_loss: 0.0548\n",
      "\n",
      "Epoch 00046: val_loss did not improve\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 0.00038742052274756136.\n",
      "ROC-AUC: 0.97442439\n",
      "\n",
      "\n",
      "Epoch 47/50\n",
      "561/561 [==============================] - 54s 96ms/step - loss: 0.0520 - val_loss: 0.0544\n",
      "\n",
      "Epoch 00047: val_loss did not improve\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 0.0003486784757114947.\n",
      "ROC-AUC: 0.97313080\n",
      "\n",
      "\n",
      "Epoch 48/50\n",
      "561/561 [==============================] - 54s 96ms/step - loss: 0.0513 - val_loss: 0.0543\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.05439 to 0.05434, saving model to models/bpe_500_model.h5\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 0.00031381062290165574.\n",
      "ROC-AUC: 0.97202572\n",
      "\n",
      "\n",
      "Epoch 49/50\n",
      "561/561 [==============================] - 54s 96ms/step - loss: 0.0508 - val_loss: 0.0538\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.05434 to 0.05377, saving model to models/bpe_500_model.h5\n",
      "ROC-AUC: 0.97337185\n",
      "\n",
      "\n",
      "Epoch 50/50\n",
      "561/561 [==============================] - 54s 96ms/step - loss: 0.0507 - val_loss: 0.0537\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.05377 to 0.05370, saving model to models/bpe_500_model.h5\n",
      "ROC-AUC: 0.97320660\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fea1c583f98>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs=50\n",
    "clr = CyclicLR(base_lr=0.001, max_lr=0.003, step_size=len(trn_seq), mode='triangular2')\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=optimizers.Nadam(0.001))\n",
    "# model.compile(loss=\"binary_crossentropy\", optimizer=optimizers.RMSprop(0.003))\n",
    "# model.compile(loss=art_loss, optimizer=optimizers.Nadam())\n",
    "\n",
    "model.fit_generator(\n",
    "    generator=trn_seq, steps_per_epoch=len(trn_seq),\n",
    "    validation_data=val_seq, validation_steps=len(val_seq),\n",
    "    initial_epoch=0, epochs=epochs, shuffle=False, verbose=1,\n",
    "#     callbacks=[model_checkpoint, clr, early_stop, roc_auc_eval],\n",
    "    callbacks=[model_checkpoint, lr_reduce, early_stop, roc_auc_eval],\n",
    "    use_multiprocessing=False, workers=cpu_cores, max_queue_size=8*cpu_cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T19:38:04.581101Z",
     "start_time": "2018-03-13T19:38:04.472471Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-4f76a9dad686>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T19:38:04.581617Z",
     "start_time": "2018-03-13T18:51:31.301Z"
    }
   },
   "outputs": [],
   "source": [
    "del model\n",
    "model = load_model(models_dir+model_name+'.h5', compile=True, \n",
    "                   custom_objects={'Attention':Attention, 'AttentionWeightedAverage':AttentionWeightedAverage, 'art_loss':art_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T19:38:04.582267Z",
     "start_time": "2018-03-13T18:51:31.303Z"
    }
   },
   "outputs": [],
   "source": [
    "Y_trn_pred = model.predict(X[trn_inx], batch_size=1024, verbose=0)\n",
    "losses = compute_losses(Y[trn_inx], Y_trn_pred, eps=1e-5)\n",
    "\n",
    "trn_loss = sum(losses)/len(losses)\n",
    "trn_auc = metrics.roc_auc_score(Y[trn_inx], Y_trn_pred)\n",
    "\n",
    "print()\n",
    "print(\"avg_loss: {}\".format(trn_loss))\n",
    "print(\"ROC AUC: {}\".format(trn_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T19:38:04.582826Z",
     "start_time": "2018-03-13T18:51:31.305Z"
    }
   },
   "outputs": [],
   "source": [
    "Y_val_pred = model.predict(X[val_inx], batch_size=1024, verbose=0)\n",
    "losses = compute_losses(Y[val_inx], Y_val_pred, eps=1e-5)\n",
    "\n",
    "val_loss = sum(losses)/len(losses)\n",
    "val_auc = metrics.roc_auc_score(Y[val_inx], Y_val_pred)\n",
    "\n",
    "print()\n",
    "print(\"avg_loss: {}\".format(val_loss))\n",
    "print(\"ROC AUC: {}\".format(val_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T19:38:04.583393Z",
     "start_time": "2018-03-13T18:51:31.307Z"
    }
   },
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T19:38:04.583991Z",
     "start_time": "2018-03-13T18:51:31.309Z"
    }
   },
   "outputs": [],
   "source": [
    "submission_name = 'bpe_submission_'+str(round(val_loss, 5))+'_'+str(round(val_auc, 5))+'.csv'\n",
    "\n",
    "sample_submission = pd.read_csv(data_dir + 'sample_submission.csv')\n",
    "test_pred = model.predict(test_X, batch_size=1024, verbose=1)\n",
    "sample_submission[inx2label] = test_pred\n",
    "sample_submission.to_csv(results_dir+submission_name, index=False)\n",
    "\n",
    "FileLink(results_dir+submission_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T19:38:04.584547Z",
     "start_time": "2018-03-13T18:51:31.311Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# from gensim.models.doc2vec import Doc2Vec, TaggedDocument, KeyedVectors\n",
    "\n",
    "# def build_tagged_docs(docs):\n",
    "#     return [TaggedDocument(tokens, [tag]) for tag, tokens in enumerate(docs)]\n",
    "\n",
    "# d2v_corpus = build_tagged_docs(text_analyzer.transform_to_tokens(piece_seq)+text_analyzer.transform_to_tokens(test_piece_seq))\n",
    "# print(len(d2v_corpus))\n",
    "\n",
    "# d2v_model = Doc2Vec(dm=0, dbow_words=1, size=300, window=12, min_count=0, iter=30, workers=cpu_cores)\n",
    "\n",
    "# %time d2v_model.build_vocab(d2v_corpus)\n",
    "# print(len(d2v_model.wv.vocab))\n",
    "\n",
    "# %time d2v_model.train(d2v_corpus, total_examples=d2v_model.corpus_count, epochs=d2v_model.iter)\n",
    "# d2v_model.save('bpe/bpe_d2v_model')\n",
    "\n",
    "# from gensim.models.doc2vec import Doc2Vec\n",
    "# d2v_model = Doc2Vec.load('bpe/bpe_d2v_model')\n",
    "\n",
    "# emb_matrix = [np.zeros(300)]\n",
    "# for w in text_analyzer.emb_words[1:]:\n",
    "#     emb_matrix.append(d2v_model.wv[w])\n",
    "# #     emb_matrix.append(d2v_model.wv[w]/np.linalg.norm(d2v_model.wv[w]))\n",
    "    \n",
    "# emb_matrix = np.stack(emb_matrix)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
