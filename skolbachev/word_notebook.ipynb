{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T21:20:30.489020Z",
     "start_time": "2018-03-13T21:20:29.248012Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 7961730\n"
     ]
    }
   ],
   "source": [
    "# import importlib\n",
    "# import local_utils; importlib.reload(local_utils)\n",
    "from local_utils import *\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T21:20:34.457356Z",
     "start_time": "2018-03-13T21:20:30.490157Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "explanation why the edits made under my username hardcore metallica fan were reverted? they weren't vandalisms, just closure on some gas after i voted at new york dolls fac. and please don't remove the template from the talk page since I'm retired now. \n",
      "\n",
      "Processed:\n",
      "explanation why the edits made under my username hardcore metallica fan were reverted? they weren't vandalisms, just closure on some gas after i voted at new york dolls fac. and please don't remove the template from the talk page since I'm retired now. \n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "ids, comments, Y, test_ids, test_comments, inx2label, label2inx = load_data(True)\n",
    "Y_wblank = np.concatenate([Y, np.expand_dims((~Y.any(axis=1)).astype(int), 1)], axis=1)\n",
    "print(\"Original:\\n\" + comments[0])\n",
    "print()\n",
    "\n",
    "comments = Parallel(n_jobs=cpu_cores)(delayed(preprocess)(text, False) for text in comments)\n",
    "test_comments = Parallel(n_jobs=cpu_cores)(delayed(preprocess)(text, False) for text in test_comments)\n",
    "print(\"Processed:\\n\" + comments[0])\n",
    "\n",
    "# comments_fr, comments_de, comments_es = load_augmented_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T21:20:37.226210Z",
     "start_time": "2018-03-13T21:20:34.462465Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "vectors, inx2word, word2inx = load_embs()\n",
    "# vectors, inx2word, word2inx = load_embs(embs_dir='data/', embs_name='glove-twitter-improved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T21:20:37.228862Z",
     "start_time": "2018-03-13T21:20:37.227198Z"
    }
   },
   "outputs": [],
   "source": [
    "# def tokenize(text):\n",
    "#     return deepmoji_tokenizer.tokenize(text, word2inx)\n",
    "\n",
    "# docs = Parallel(n_jobs=cpu_cores)(delayed(tokenize)(text) for text in comments + test_comments)\n",
    "# pickle.dump(docs, open('data/tokenized_comments.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T21:20:39.545099Z",
     "start_time": "2018-03-13T21:20:37.229719Z"
    }
   },
   "outputs": [],
   "source": [
    "docs = pickle.load(open('data/tokenized_comments.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T21:21:08.675168Z",
     "start_time": "2018-03-13T21:20:39.546093Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docs: 312735\n",
      "Selected words: 172905\n",
      "Processed OOV words: 4533\n",
      "mean_len: 74.75018466113482\n",
      "mean_len + 2*std: 313.1402682213584\n",
      "mean_len + 3*std: 432.33531000147013\n"
     ]
    }
   ],
   "source": [
    "max_len = 450\n",
    "text_analyzer = TextAnalyzer(word2inx, vectors, max_len=max_len, process_oov_words=True, oov_min_doc_hits=5)\n",
    "seq, meta = text_analyzer.fit_on_docs(docs)\n",
    "\n",
    "X = seq[:len(comments)]\n",
    "test_X = seq[len(comments):]\n",
    "\n",
    "meta_mean = meta.mean(axis=0)\n",
    "meta_std = meta.std(axis=0)\n",
    "meta = (meta - meta_mean)/meta_std\n",
    "\n",
    "print(\"mean_len: {}\".format(meta_mean[0]))\n",
    "print(\"mean_len + 2*std: {}\".format(meta_mean[0]+2*meta_std[0]))\n",
    "print(\"mean_len + 3*std: {}\".format(meta_mean[0]+3*meta_std[0]))\n",
    "\n",
    "X_meta = meta[:len(comments)]\n",
    "test_X_meta = meta[len(comments):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T21:21:08.998298Z",
     "start_time": "2018-03-13T21:21:08.676167Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 143613, valid: 15958\n"
     ]
    }
   ],
   "source": [
    "# Train/Valid splitting\n",
    "trn_inx, val_inx = stratified_sampling(Y, 0.1, seed)\n",
    "\n",
    "print(\"train: {}, valid: {}\".format(len(trn_inx), len(val_inx)))\n",
    "# plot_stratified_sampling(Y, trn_inx, val_inx, inx2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T21:21:09.065827Z",
     "start_time": "2018-03-13T21:21:08.999318Z"
    }
   },
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "ros = RandomOverSampler(random_state=seed)\n",
    "trn_inx = ros.fit_sample(trn_inx.reshape(-1, 1), Y_wblank[trn_inx,6])[0].flatten()\n",
    "\n",
    "# rus = RandomUnderSampler(return_indices=False)\n",
    "# trn_inx = rus.fit_sample(trn_inx.reshape(-1, 1), Y_wblank[trn_inx,6])[0].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T21:21:09.068720Z",
     "start_time": "2018-03-13T21:21:09.066939Z"
    }
   },
   "outputs": [],
   "source": [
    "# fr_seq, fr_meta = text_analyzer.transform_texts(comments_fr)\n",
    "# de_seq, de_meta = text_analyzer.transform_texts(comments_de)\n",
    "# es_seq, es_meta = text_analyzer.transform_texts(comments_es)\n",
    "\n",
    "# trn_X = np.concatenate([X[trn_inx], fr_seq[trn_inx], de_seq[trn_inx], es_seq[trn_inx]])\n",
    "# trn_X_meta = np.concatenate([X_meta[trn_inx], fr_meta[trn_inx], de_meta[trn_inx], es_meta[trn_inx]])\n",
    "# trn_Y = np.concatenate([Y[trn_inx], Y[trn_inx], Y[trn_inx], Y[trn_inx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T21:21:09.074830Z",
     "start_time": "2018-03-13T21:21:09.069551Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# CNN\n",
    "# def cnn_block(x, filters, kernel_size, attention=0):\n",
    "#     cnn = Conv1D(filters=filters, kernel_size=kernel_size, activation='relu')(x)\n",
    "    \n",
    "#     if attention == 0: cnn = GlobalMaxPooling1D()(cnn)\n",
    "#     elif attention == 1: cnn = AttentionWeightedAverage()(cnn)\n",
    "#     elif attention == 2: cnn = Attention()(cnn)\n",
    "\n",
    "#     return cnn\n",
    "\n",
    "# def getCNNModel(input_shape, classes, num_words, emb_size, emb_matrix,\n",
    "#                 attention=0, dense=False, emb_trainable=False):\n",
    "\n",
    "#     x_input = Input(shape=(input_shape,))\n",
    "    \n",
    "#     emb = Embedding(num_words, emb_size, weights=[emb_matrix], trainable=emb_trainable, name='embs')(x_input)\n",
    "#     emb = SpatialDropout1D(0.15)(emb)\n",
    "        \n",
    "#     cnn1 = cnn_block(emb, 100, 3, attention=attention)\n",
    "#     cnn2 = cnn_block(emb, 100, 4, attention=attention)\n",
    "#     cnn3 = cnn_block(emb, 100, 5, attention=attention)\n",
    "#     x = concatenate([cnn1, cnn2, cnn3])\n",
    "\n",
    "#     x = Dropout(0.15)(x)\n",
    "    \n",
    "#     if dense: \n",
    "#         x = Dense(50, activation='relu')(x)\n",
    "#         x = Dropout(0.15)(x)\n",
    "    \n",
    "#     x_output = Dense(classes, activation='sigmoid')(x)\n",
    "#     return Model(inputs=x_input, outputs=x_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T21:21:09.099847Z",
     "start_time": "2018-03-13T21:21:09.075694Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# CNN-LSTM\n",
    "def getCNNLSTMModel(input_shape, classes, num_words, emb_size, emb_matrix,\n",
    "                    attention=0, dense=False, emb_trainable=False):\n",
    "\n",
    "    x_input = Input(shape=(input_shape,))\n",
    "\n",
    "    emb = Embedding(num_words, emb_size, weights=[emb_matrix], trainable=emb_trainable, name='embs')(x_input)\n",
    "    emb = SpatialDropout1D(0.3)(emb)\n",
    "#     rnn = SpatialDropout1D(0.15)(rnn)\n",
    "    \n",
    "    cnn1 = Conv1D(filters=64, kernel_size=3, activation='relu', padding='same')(emb)\n",
    "    cnn2 = Conv1D(filters=64, kernel_size=4, activation='relu', padding='same')(emb)\n",
    "    cnn3 = Conv1D(filters=64, kernel_size=5, activation='relu', padding='same')(emb)\n",
    "#     cnn4 = Conv1D(filters=64, kernel_size=6, activation='relu', padding='same')(emb)\n",
    "    \n",
    "    x = concatenate([cnn1, cnn2, cnn3])\n",
    "    x = Bidirectional(CuDNNGRU(64, return_sequences=True))(x)\n",
    "    \n",
    "    if attention == 1: x = AttentionWeightedAverage()(x)\n",
    "    elif attention == 2: x = Attention()(x)\n",
    "    else: x = GlobalMaxPooling1D()(x)\n",
    "    \n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    if dense: \n",
    "        x = Dense(32, activation='relu')(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "    \n",
    "    x_output = Dense(classes, activation='sigmoid')(x)\n",
    "    return Model(inputs=x_input, outputs=x_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T21:21:09.123449Z",
     "start_time": "2018-03-13T21:21:09.100823Z"
    },
    "code_folding": [
     0,
     1
    ]
   },
   "outputs": [],
   "source": [
    "# MaxPool2D\n",
    "def get2xBiCuDNNGRUMaxPool2DModel(input_shape, classes, num_words, emb_size, emb_matrix,\n",
    "                                attention=0, dense=False, emb_trainable=False):\n",
    "\n",
    "    x_input = Input(shape=(input_shape,))\n",
    "    \n",
    "    emb = Embedding(num_words, emb_size, weights=[emb_matrix], trainable=emb_trainable, name='embs')(x_input)\n",
    "    emb = SpatialDropout1D(0.3)(emb)\n",
    "        \n",
    "    rnn1 = Bidirectional(CuDNNGRU(64, return_sequences=True))(emb)\n",
    "    rnn2 = Bidirectional(CuDNNGRU(64, return_sequences=True))(rnn1)\n",
    "    x = concatenate([rnn1, rnn2])\n",
    "\n",
    "    if attention == 1: x1 = AttentionWeightedAverage()(x)\n",
    "    elif attention == 2: x1 = Attention()(x)\n",
    "    else: x1 = GlobalMaxPooling1D()(x)\n",
    "        \n",
    "    if attention == 1: x2 = AttentionWeightedAverage()(Permute((2, 1))(x))\n",
    "    elif attention == 2: x2 = Attention()(Permute((2, 1))(x))\n",
    "    else: x2 = GlobalMaxPooling1D()(Permute((2, 1))(x))\n",
    "        \n",
    "    x = concatenate([x1, x2])\n",
    "    \n",
    "    if dense: \n",
    "        x = Dense(32, activation='relu')(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "    \n",
    "    x_output = Dense(classes, activation='sigmoid')(x)\n",
    "    return Model(inputs=x_input, outputs=x_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T21:21:09.658980Z",
     "start_time": "2018-03-13T21:21:09.124492Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# VDCNN\n",
    "from keras.models import Model\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Input, Dense, Dropout, Lambda, Activation\n",
    "from keras.layers.pooling import MaxPooling1D\n",
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import tensorflow as tf\n",
    "\n",
    "num_filters_default = [64,128,256] # from VDCNN paper\n",
    "\n",
    "\n",
    "def VDCNN_model(input_shape,num_classes,num_words,emb_size,emb_matrix,num_filters=num_filters_default,top_k=8,emb_trainable=False):\n",
    "\n",
    "    inputs = Input(shape=(input_shape, ), dtype='int32', name='inputs')\n",
    "\n",
    "    embedded_sent = Embedding(num_words, emb_size, weights=[emb_matrix], trainable=emb_trainable, name='embs')(inputs)\n",
    "    embedded_sent = SpatialDropout1D(0.3)(embedded_sent)\n",
    "    \n",
    "    conv = Conv1D(filters=64, kernel_size=3, strides=2, padding=\"same\")(embedded_sent)\n",
    "\n",
    "    for i in range(len(num_filters)):\n",
    "        conv = ConvBlockVDCNN(conv.get_shape().as_list()[1:], num_filters[i])(conv)\n",
    "        conv = MaxPooling1D(pool_size=3, strides=2, padding=\"same\")(conv)\n",
    "        \n",
    "    def k_max_pooling(x):\n",
    "        x = tf.transpose(x, [0, 2, 1])\n",
    "        k_max = tf.nn.top_k(x, k=top_k)\n",
    "        return tf.reshape(k_max[0], (-1, num_filters[-1] * top_k))\n",
    "\n",
    "    k_max = Lambda(k_max_pooling,output_shape=(num_filters[-1] * top_k,))(conv)\n",
    "    k_max = Dropout(0.3)(k_max)\n",
    "\n",
    "    # fully-connected layers\n",
    "    fc1 = Dropout(0.3)(Dense(256, activation='relu', kernel_initializer='he_normal')(k_max))\n",
    "    fc2 = Dropout(0.3)(Dense(128, activation='relu', kernel_initializer='he_normal')(fc1))\n",
    "    fc3 = Dense(num_classes, activation='sigmoid')(fc2)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=fc3)\n",
    "    return model\n",
    "\n",
    "class ConvBlockVDCNN(object):\n",
    "\n",
    "    def __init__(self, input_shape, num_filters):\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Conv1D(filters=num_filters, kernel_size=3, strides=1, padding=\"same\", input_shape=input_shape))\n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(Activation('relu'))\n",
    "\n",
    "        self.model.add(Conv1D(filters=num_filters, kernel_size=3, strides=1, padding=\"same\"))\n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(Activation('relu'))\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        return self.model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T21:21:09.667016Z",
     "start_time": "2018-03-13T21:21:09.659951Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# def Conv2D_block(reshape, sequence_length, embedding_dim):\n",
    "#     filter_sizes = [3,4,5]\n",
    "#     num_filters = 32\n",
    "\n",
    "#     conv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], embedding_dim), padding='valid', kernel_initializer='he_uniform', activation='relu')(reshape)\n",
    "#     conv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[1], embedding_dim), padding='valid', kernel_initializer='he_uniform', activation='relu')(reshape)\n",
    "#     conv_2 = Conv2D(num_filters, kernel_size=(filter_sizes[2], embedding_dim), padding='valid', kernel_initializer='he_uniform', activation='relu')(reshape)\n",
    "\n",
    "#     maxpool_0 = MaxPool2D(pool_size=(sequence_length - filter_sizes[0] + 1, 1), strides=(1,1), padding='valid')(conv_0)\n",
    "#     maxpool_1 = MaxPool2D(pool_size=(sequence_length - filter_sizes[1] + 1, 1), strides=(1,1), padding='valid')(conv_1)\n",
    "#     maxpool_2 = MaxPool2D(pool_size=(sequence_length - filter_sizes[2] + 1, 1), strides=(1,1), padding='valid')(conv_2)\n",
    "\n",
    "#     concatenated_tensor = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2])\n",
    "#     flatten = Flatten()(concatenated_tensor)\n",
    "#     return flatten\n",
    "\n",
    "\n",
    "# def Art_CNN(maxlen, max_features, embed_size, embedding_matrix):\n",
    "#     sequence_input = Input(shape=(maxlen,), dtype='int32')\n",
    "#     embedding = Embedding(max_features, embed_size, weights=[embedding_matrix],input_length=maxlen,trainable=False)(sequence_input)\n",
    "#     x = SpatialDropout1D(0.2)(embedding)\n",
    "#     reshape = Reshape((maxlen,embed_size,1))(x)\n",
    "#     x = Conv2D_block(reshape,maxlen,embed_size)\n",
    "#     x = Dense(256, activation=\"relu\")(x)\n",
    "#     x = Dropout(0.2)(x)\n",
    "#     preds = Dense(6, activation='sigmoid')(x)\n",
    "#     model = Model(sequence_input, preds)\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T21:21:09.682743Z",
     "start_time": "2018-03-13T21:21:09.667822Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Current exp model\n",
    "def getExpModel0(input_shape, classes, num_words, emb_size, emb_matrix, emb_dropout=0.5,\n",
    "                 attention=0, dense=False, emb_trainable=False):\n",
    "\n",
    "    x_input = Input(shape=(input_shape,))\n",
    "    \n",
    "    emb = Embedding(num_words, emb_size, weights=[emb_matrix], trainable=emb_trainable, name='embs')(x_input)\n",
    "    emb = SpatialDropout1D(emb_dropout)(emb)\n",
    "        \n",
    "    rnn1 = Bidirectional(CuDNNGRU(64, return_sequences=True))(emb)\n",
    "    rnn2 = Bidirectional(CuDNNGRU(64, return_sequences=True))(rnn1)\n",
    "    x = concatenate([rnn1, rnn2])\n",
    "\n",
    "    if attention == 1: x = AttentionWeightedAverage()(x)\n",
    "    elif attention == 2: x = Attention()(x)\n",
    "    else: x = GlobalMaxPooling1D()(x)\n",
    "    \n",
    "    if dense: \n",
    "        x = Dense(32, activation='relu')(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "    \n",
    "    x_output = Dense(classes, activation='sigmoid')(x)\n",
    "    return Model(inputs=x_input, outputs=x_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T21:21:10.441778Z",
     "start_time": "2018-03-13T21:21:09.683614Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 450)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embs (Embedding)                (None, 450, 300)     51871500    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 450, 300)     0           embs[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 450, 128)     140544      spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 450, 128)     74496       bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 450, 256)     0           bidirectional_1[0][0]            \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 256)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 6)            1542        global_max_pooling1d_1[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 52,088,082\n",
      "Trainable params: 216,582\n",
      "Non-trainable params: 51,871,500\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# import importlib\n",
    "# importlib.reload(toxic.models)\n",
    "# from toxic.models import *\n",
    "\n",
    "model_name = 'exp_model'\n",
    "\n",
    "model = getModel0(input_shape=X.shape[1], classes=Y.shape[1], num_words=len(text_analyzer.inx2emb), \n",
    "                  emb_size=text_analyzer.emb_size, emb_matrix=text_analyzer.emb_vectors,\n",
    "                  emb_dropout=0.25, attention=1, dense=False, emb_trainable=False, gru=True)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T21:21:10.446111Z",
     "start_time": "2018-03-13T21:21:10.442727Z"
    }
   },
   "outputs": [],
   "source": [
    "model_checkpoint = ModelCheckpoint(models_dir+model_name+'.h5', monitor='val_auc', mode='min', verbose=1, save_best_only=True)\n",
    "early_stop = EarlyStopping(monitor='val_auc', min_delta=0, patience=6, verbose=1, mode='auto')\n",
    "lr_schedule = LearningRateScheduler(lr_change, verbose=1)\n",
    "lr_reduce = ReduceLROnPlateau(monitor='val_auc', factor=0.9, patience=1, min_lr=0.0001, verbose=1)\n",
    "# tensorboard = TensorBoard(log_dir='logs', write_graph=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T21:21:10.530993Z",
     "start_time": "2018-03-13T21:21:10.446928Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "val_batch_size = 1024\n",
    "weights = getClassWeights(Y, mu=0.5)\n",
    "\n",
    "# trn_seq = StratifiedFeatureSequence(X[trn_inx], Y[trn_inx], batch_size)\n",
    "trn_seq = FeatureSequence(X[trn_inx], Y[trn_inx], batch_size, shuffle=True)\n",
    "val_seq = FeatureSequence(X[val_inx], Y[val_inx], val_batch_size)\n",
    "roc_auc_eval = RocAucEvaluation(X[val_inx], Y[val_inx], batch_size=val_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T21:37:31.802270Z",
     "start_time": "2018-03-13T21:21:10.532119Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "504/504 [==============================] - 107s 212ms/step - loss: 0.2347 - val_loss: 0.0470\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04704, saving model to models/exp_model.h5\n",
      "ROC-AUC: 0.98208613\n",
      "\n",
      "\n",
      "Epoch 2/10\n",
      "504/504 [==============================] - 107s 212ms/step - loss: 0.1571 - val_loss: 0.0567\n",
      "\n",
      "Epoch 00002: val_loss did not improve\n",
      "ROC-AUC: 0.99021264\n",
      "\n",
      "\n",
      "Epoch 3/10\n",
      "504/504 [==============================] - 107s 212ms/step - loss: 0.1384 - val_loss: 0.0448\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04704 to 0.04484, saving model to models/exp_model.h5\n",
      "ROC-AUC: 0.99044431\n",
      "\n",
      "\n",
      "Epoch 4/10\n",
      "504/504 [==============================] - 107s 212ms/step - loss: 0.1274 - val_loss: 0.0455\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "ROC-AUC: 0.99038494\n",
      "\n",
      "\n",
      "Epoch 5/10\n",
      "504/504 [==============================] - 107s 212ms/step - loss: 0.1226 - val_loss: 0.0456\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "ROC-AUC: 0.98996003\n",
      "\n",
      "\n",
      "Epoch 6/10\n",
      "504/504 [==============================] - 107s 212ms/step - loss: 0.1192 - val_loss: 0.0516\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "ROC-AUC: 0.99007668\n",
      "\n",
      "\n",
      "Epoch 7/10\n",
      "504/504 [==============================] - 107s 212ms/step - loss: 0.1114 - val_loss: 0.0494\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "ROC-AUC: 0.98970209\n",
      "\n",
      "\n",
      "Epoch 8/10\n",
      "504/504 [==============================] - 107s 212ms/step - loss: 0.1028 - val_loss: 0.0485\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "ROC-AUC: 0.98938482\n",
      "\n",
      "\n",
      "Epoch 9/10\n",
      "504/504 [==============================] - 107s 212ms/step - loss: 0.0984 - val_loss: 0.0484\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "ROC-AUC: 0.98900708\n",
      "\n",
      "\n",
      "Epoch 00009: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efcd4a26828>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs=10\n",
    "def focal_loss_keras(y_true, y_pred):\n",
    "    return focal_loss(y_true, y_pred, 10.0, 0.5)\n",
    "\n",
    "clr = CyclicLR(base_lr=0.0001, max_lr=0.003, step_size=2*len(trn_seq), mode='triangular2')\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=optimizers.Nadam())\n",
    "# model.compile(loss=focal_loss_keras, optimizer=optimizers.Nadam())\n",
    "# model.compile(loss=art_loss, optimizer=optimizers.Nadam())\n",
    "\n",
    "model.fit_generator(\n",
    "    generator=trn_seq, steps_per_epoch=len(trn_seq),\n",
    "    validation_data=val_seq, validation_steps=len(val_seq),\n",
    "    initial_epoch=0, epochs=epochs, shuffle=False, verbose=1,\n",
    "    callbacks=[model_checkpoint, clr, early_stop, roc_auc_eval],\n",
    "#     callbacks=[model_checkpoint, lr_reduce, early_stop, roc_auc_eval],\n",
    "    use_multiprocessing=False, workers=cpu_cores, max_queue_size=8*cpu_cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T21:37:31.804922Z",
     "start_time": "2018-03-13T21:37:31.803295Z"
    }
   },
   "outputs": [],
   "source": [
    "# matplotlib.rcParams['figure.figsize'] = (24,8)\n",
    "\n",
    "# plt.xlabel('Learning Rate')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.plot(clr.history['lr'], clr.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T21:37:32.827351Z",
     "start_time": "2018-03-13T21:37:31.805784Z"
    }
   },
   "outputs": [],
   "source": [
    "del model\n",
    "model = load_model(models_dir+model_name+'.h5', compile=True, \n",
    "                   custom_objects={'Attention':Attention, 'AttentionWeightedAverage':AttentionWeightedAverage, \n",
    "                                   'focal_loss_keras':focal_loss_keras})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T21:38:05.287356Z",
     "start_time": "2018-03-13T21:37:32.828360Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic: 0.17744088693090795\n",
      "severe_toxic: 0.09128749009140003\n",
      "obscene: 0.14845180407995165\n",
      "threat: 0.026537311782091955\n",
      "insult: 0.20833600629779853\n",
      "identity_hate: 0.06739826207544598\n",
      "\n",
      "\n",
      "avg_loss: 0.11990862687626601\n",
      "ROC AUC: 0.9772393096861949\n"
     ]
    }
   ],
   "source": [
    "Y_trn_pred = model.predict(X[trn_inx], batch_size=1024, verbose=0)\n",
    "losses = compute_losses(Y[trn_inx], Y_trn_pred, eps=1e-5)\n",
    "for label, label_loss in zip(inx2label, losses):\n",
    "    print(\"{}: {}\".format(label, label_loss))\n",
    "print()\n",
    "\n",
    "trn_loss = sum(losses)/len(losses)\n",
    "trn_auc = metrics.roc_auc_score(Y[trn_inx], Y_trn_pred)\n",
    "\n",
    "print()\n",
    "print(\"avg_loss: {}\".format(trn_loss))\n",
    "print(\"ROC AUC: {}\".format(trn_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T21:38:07.312778Z",
     "start_time": "2018-03-13T21:38:05.288413Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic: 0.12450868663083339\n",
      "severe_toxic: 0.019486582241236408\n",
      "obscene: 0.04114499264214632\n",
      "threat: 0.007003558013638073\n",
      "insult: 0.05870610764051668\n",
      "identity_hate: 0.018173781678836168\n",
      "\n",
      "\n",
      "avg_loss: 0.04483728480786784\n",
      "ROC AUC: 0.9904443144007488\n"
     ]
    }
   ],
   "source": [
    "Y_val_pred = model.predict(X[val_inx], batch_size=1024, verbose=0)\n",
    "losses = compute_losses(Y[val_inx], Y_val_pred, eps=1e-5)\n",
    "for label, label_loss in zip(inx2label, losses):\n",
    "    print(\"{}: {}\".format(label, label_loss))\n",
    "print()\n",
    "\n",
    "val_loss = sum(losses)/len(losses)\n",
    "val_auc = metrics.roc_auc_score(Y[val_inx], Y_val_pred)\n",
    "\n",
    "print()\n",
    "print(\"avg_loss: {}\".format(val_loss))\n",
    "print(\"ROC AUC: {}\".format(val_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T21:38:07.456911Z",
     "start_time": "2018-03-13T21:38:07.313715Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-4f76a9dad686>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T21:38:07.457400Z",
     "start_time": "2018-03-13T21:20:29.288Z"
    }
   },
   "outputs": [],
   "source": [
    "epochs1=epochs+8\n",
    "clr = CyclicLR(base_lr=0.0001, max_lr=0.001, step_size=2*len(trn_seq), mode='triangular2')\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=4, verbose=1, mode='auto')\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=optimizers.Adam())\n",
    "# model.compile(loss=art_loss, optimizer=optimizers.Adam())\n",
    "model.fit_generator(\n",
    "    generator=trn_seq, steps_per_epoch=len(trn_seq),\n",
    "    validation_data=val_seq, validation_steps=len(val_seq),\n",
    "    initial_epoch=epochs, epochs=epochs1, shuffle=False, verbose=1,\n",
    "#     class_weight=weights,\n",
    "    callbacks=[model_checkpoint, clr, early_stop, roc_auc_eval],\n",
    "#     callbacks=[model_checkpoint, lr_reduce, early_stop, roc_auc_eval],\n",
    "    use_multiprocessing=False, workers=cpu_cores, max_queue_size=8*cpu_cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T21:38:07.457921Z",
     "start_time": "2018-03-13T21:20:29.291Z"
    }
   },
   "outputs": [],
   "source": [
    "del model\n",
    "model = load_model(models_dir+model_name+'.h5', compile=True, \n",
    "                   custom_objects={'Attention':Attention, 'AttentionWeightedAverage':AttentionWeightedAverage, 'art_loss':art_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T21:38:07.458444Z",
     "start_time": "2018-03-13T21:20:29.293Z"
    }
   },
   "outputs": [],
   "source": [
    "Y_val_pred = model.predict(X[val_inx], batch_size=1024, verbose=0)\n",
    "losses = compute_losses(Y[val_inx], Y_val_pred, eps=1e-5)\n",
    "for label, label_loss in zip(inx2label, losses):\n",
    "    print(\"{}: {}\".format(label, label_loss))\n",
    "print()\n",
    "\n",
    "val_loss = sum(losses)/len(losses)\n",
    "val_auc = metrics.roc_auc_score(Y[val_inx], Y_val_pred)\n",
    "\n",
    "print()\n",
    "print(\"avg_loss: {}\".format(val_loss))\n",
    "print(\"ROC AUC: {}\".format(val_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T21:38:07.458983Z",
     "start_time": "2018-03-13T21:20:29.297Z"
    }
   },
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T21:38:07.459494Z",
     "start_time": "2018-03-13T21:20:29.299Z"
    }
   },
   "outputs": [],
   "source": [
    "# np.save(\"data/new_preprocessing_Y_val_pred.npy\", Y_val_pred)\n",
    "# np.save(\"data/new_preprocessing_val_inx.npy\", val_inx)\n",
    "# FileLink(\"data/new_preprocessing_val_inx.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T21:38:07.460068Z",
     "start_time": "2018-03-13T21:20:29.301Z"
    }
   },
   "outputs": [],
   "source": [
    "submission_name = 'exp__focal_loss_a1.0_g0.5__fasttext__d0.3__submission_'+str(round(val_loss, 5))+'_'+str(round(val_auc, 5))+'.csv'\n",
    "\n",
    "sample_submission = pd.read_csv(data_dir + 'sample_submission.csv')\n",
    "test_pred = model.predict(test_X, batch_size=1024, verbose=1)\n",
    "sample_submission[inx2label] = test_pred\n",
    "sample_submission.to_csv(results_dir+submission_name, index=False)\n",
    "\n",
    "FileLink(results_dir+submission_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T21:38:07.460640Z",
     "start_time": "2018-03-13T21:20:29.303Z"
    }
   },
   "outputs": [],
   "source": [
    "# pseudo\n",
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T21:38:07.461181Z",
     "start_time": "2018-03-13T21:20:29.305Z"
    }
   },
   "outputs": [],
   "source": [
    "model_loss_checkpoint = ModelCheckpoint(models_dir+model_name+'_pseudo.h5', monitor='val_loss', verbose=1, mode='min', save_best_only=True)\n",
    "model.compile(optimizer=optimizers.Nadam(0.0001), loss='binary_crossentropy')\n",
    "\n",
    "ps_epochs = 3\n",
    "for ps_inx in range(0, ps_epochs):  \n",
    "    test_Y = model.predict(test_X, batch_size=1024, verbose=1)\n",
    "    \n",
    "    trn_ps_seq = PseudoFeatureSequence(X[trn_inx], X_meta[trn_inx], Y[trn_inx], 182, \n",
    "                                       test_X, np.zeros((test_X.shape[0], 2)), test_Y, 74,  \n",
    "                                       shuffle=True)\n",
    "    model.fit_generator(\n",
    "        generator=trn_ps_seq, steps_per_epoch=len(trn_ps_seq),  \n",
    "        validation_data=val_seq, validation_steps=len(val_seq),\n",
    "        initial_epoch=epochs+ps_inx, epochs=epochs+ps_inx+1, \n",
    "        shuffle=False, verbose=1,\n",
    "        class_weight=weights,\n",
    "        callbacks=[model_loss_checkpoint, roc_auc_eval],\n",
    "        use_multiprocessing=False, workers=cpu_cores, max_queue_size=4*cpu_cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T21:38:07.461742Z",
     "start_time": "2018-03-13T21:20:29.307Z"
    }
   },
   "outputs": [],
   "source": [
    "Y_val_pred = model.predict(X[val_inx], batch_size=512, verbose=0)\n",
    "losses = compute_losses(Y[val_inx], Y_val_pred, eps=1e-5)\n",
    "\n",
    "val_loss = sum(losses)/len(losses)\n",
    "val_auc = metrics.roc_auc_score(Y[val_inx], Y_val_pred)\n",
    "\n",
    "print()\n",
    "print(\"avg_loss: {}\".format(val_loss))\n",
    "print(\"ROC AUC: {}\".format(val_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T21:38:07.462292Z",
     "start_time": "2018-03-13T21:20:29.309Z"
    }
   },
   "outputs": [],
   "source": [
    "submission_name = 'fasttext__gru__max_pool2d__submission_'+str(round(val_loss, 5))+'_'+str(round(val_auc, 5))+'_pseudo'+ps_epochs+'.csv'\n",
    "\n",
    "sample_submission = pd.read_csv(data_dir + 'sample_submission.csv')\n",
    "test_pred = model.predict(test_X, batch_size=1024, verbose=1)\n",
    "sample_submission[inx2label] = test_pred\n",
    "sample_submission.to_csv(results_dir+submission_name, index=False)\n",
    "\n",
    "FileLink(results_dir+submission_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T21:38:07.462851Z",
     "start_time": "2018-03-13T21:20:29.311Z"
    }
   },
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T21:38:07.463403Z",
     "start_time": "2018-03-13T21:20:29.313Z"
    }
   },
   "outputs": [],
   "source": [
    "# oov_word_hits = {}\n",
    "# oov_doc_hits = {}\n",
    "# for w in text_analyzer._oov_vec.keys():\n",
    "#     oov_word_hits[w] = text_analyzer.word_hits[w]\n",
    "#     oov_doc_hits[w] = text_analyzer.doc_hits[w]\n",
    "\n",
    "# pickle.dump(oov_word_hits, open('data/oov_word_hits.pkl', 'wb'))\n",
    "# pickle.dump(oov_doc_hits, open('data/oov_doc_hits.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
