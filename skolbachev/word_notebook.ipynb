{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-28T22:38:48.298773Z",
     "start_time": "2018-02-28T22:38:48.290862Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO compare base model without Dense dropout + deepmoji tokenizer vs base one with new opt and clr\n",
    "# Add lemmatizer and remover doubled characters etc. to deepmoji tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-28T22:58:08.754304Z",
     "start_time": "2018-02-28T22:58:07.032007Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 7961730\n"
     ]
    }
   ],
   "source": [
    "# import importlib\n",
    "# import local_utils; importlib.reload(local_utils)\n",
    "from local_utils import *\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-28T22:58:12.967299Z",
     "start_time": "2018-02-28T22:58:08.756049Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "Explanation\n",
      "Why the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27\n",
      "\n",
      "Processed:\n",
      "Explanation Why the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now. \n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "ids, comments, Y, test_ids, test_comments, inx2label, label2inx = load_data()\n",
    "Y_wblank = np.concatenate([Y, np.expand_dims((~Y.any(axis=1)).astype(int), 1)], axis=1)\n",
    "print(\"Original:\\n\" + comments[0])\n",
    "print()\n",
    "\n",
    "comments = Parallel(n_jobs=cpu_cores)(delayed(preprocess)(text, False) for text in comments)\n",
    "test_comments = Parallel(n_jobs=cpu_cores)(delayed(preprocess)(text, False) for text in test_comments)\n",
    "print(\"Processed:\\n\" + comments[0])\n",
    "\n",
    "# comments_fr, comments_de, comments_es = load_augmented_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-28T22:58:15.641324Z",
     "start_time": "2018-02-28T22:58:12.970951Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# vectors, inx2word, word2inx = load_embs(embs_name='glove-twitter-200d-27B')\n",
    "vectors, inx2word, word2inx = load_embs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-28T22:59:06.363112Z",
     "start_time": "2018-02-28T22:58:15.642424Z"
    }
   },
   "outputs": [],
   "source": [
    "# def tokenize(text):\n",
    "#     return deepmoji_tokenizer.tokenize(text, vocab=word2inx)\n",
    "\n",
    "# docs = Parallel(n_jobs=cpu_cores)(delayed(tokenize)(text) for text in comments + test_comments)\n",
    "# pickle.dump(docs, open('data/tokenized_comments.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-28T23:04:57.521733Z",
     "start_time": "2018-02-28T23:04:54.412611Z"
    }
   },
   "outputs": [],
   "source": [
    "docs = pickle.load(open('data/tokenized_comments.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-28T22:59:06.544230Z",
     "start_time": "2018-02-28T22:59:06.366082Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'cpu_cores'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-93e053760cc1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmax_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m460\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m text_analyzer = TextAnalyzer(word2inx, vectors, max_len=max_len,\n\u001b[0;32m----> 3\u001b[0;31m                              process_oov_words=True, oov_min_doc_hits=5, cpu_cores=cpu_cores)\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_analyzer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_on_docs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomments\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtest_comments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'cpu_cores'"
     ]
    }
   ],
   "source": [
    "max_len = 460\n",
    "text_analyzer = TextAnalyzer(word2inx, vectors, max_len=max_len, process_oov_words=True, oov_min_doc_hits=5)\n",
    "seq, meta = text_analyzer.fit_on_docs(comments + test_comments)\n",
    "\n",
    "X = seq[:len(comments)]\n",
    "test_X = seq[len(comments):]\n",
    "\n",
    "meta_mean = meta.mean(axis=0)\n",
    "meta_std = meta.std(axis=0)\n",
    "meta = (meta - meta_mean)/meta_std\n",
    "\n",
    "print(\"mean_len: {}\".format(meta_mean[0]))\n",
    "print(\"mean_len + 2*std: {}\".format(meta_mean[0]+2*meta_std[0]))\n",
    "print(\"mean_len + 3*std: {}\".format(meta_mean[0]+3*meta_std[0]))\n",
    "\n",
    "X_meta = meta[:len(comments)]\n",
    "test_X_meta = meta[len(comments):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-28T20:37:04.658897Z",
     "start_time": "2018-02-28T20:37:04.347044Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 143613, valid: 15958\n"
     ]
    }
   ],
   "source": [
    "# Train/Valid splitting\n",
    "trn_inx, val_inx = stratified_sampling(Y, 0.1, seed)\n",
    "\n",
    "print(\"train: {}, valid: {}\".format(len(trn_inx), len(val_inx)))\n",
    "# plot_stratified_sampling(Y, trn_inx, val_inx, inx2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-28T20:37:04.661626Z",
     "start_time": "2018-02-28T20:37:04.659859Z"
    }
   },
   "outputs": [],
   "source": [
    "# fr_seq, fr_meta = text_analyzer.transform_texts(comments_fr)\n",
    "# de_seq, de_meta = text_analyzer.transform_texts(comments_de)\n",
    "# es_seq, es_meta = text_analyzer.transform_texts(comments_es)\n",
    "\n",
    "# trn_X = np.concatenate([X[trn_inx], fr_seq[trn_inx], de_seq[trn_inx], es_seq[trn_inx]])\n",
    "# trn_X_meta = np.concatenate([X_meta[trn_inx], fr_meta[trn_inx], de_meta[trn_inx], es_meta[trn_inx]])\n",
    "# trn_Y = np.concatenate([Y[trn_inx], Y[trn_inx], Y[trn_inx], Y[trn_inx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-28T20:37:04.670111Z",
     "start_time": "2018-02-28T20:37:04.662448Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# CNN\n",
    "# def cnn_block(x, filters, kernel_size, attention=0):\n",
    "#     cnn = Conv1D(filters=filters, kernel_size=kernel_size, activation='relu')(x)\n",
    "    \n",
    "#     if attention == 0: cnn = GlobalMaxPooling1D()(cnn)\n",
    "#     elif attention == 1: cnn = AttentionWeightedAverage()(cnn)\n",
    "#     elif attention == 2: cnn = Attention()(cnn)\n",
    "\n",
    "#     return cnn\n",
    "\n",
    "# def getCNNModel(input_shape, classes, num_words, emb_size, emb_matrix,\n",
    "#                 attention=0, dense=False, emb_trainable=False):\n",
    "\n",
    "#     x_input = Input(shape=(input_shape,))\n",
    "    \n",
    "#     emb = Embedding(num_words, emb_size, weights=[emb_matrix], trainable=emb_trainable, name='embs')(x_input)\n",
    "#     emb = SpatialDropout1D(0.15)(emb)\n",
    "        \n",
    "#     cnn1 = cnn_block(emb, 100, 3, attention=attention)\n",
    "#     cnn2 = cnn_block(emb, 100, 4, attention=attention)\n",
    "#     cnn3 = cnn_block(emb, 100, 5, attention=attention)\n",
    "#     x = concatenate([cnn1, cnn2, cnn3])\n",
    "\n",
    "#     x = Dropout(0.15)(x)\n",
    "    \n",
    "#     if dense: \n",
    "#         x = Dense(50, activation='relu')(x)\n",
    "#         x = Dropout(0.15)(x)\n",
    "    \n",
    "#     x_output = Dense(classes, activation='sigmoid')(x)\n",
    "#     return Model(inputs=x_input, outputs=x_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-28T20:37:04.693921Z",
     "start_time": "2018-02-28T20:37:04.671085Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# CNN-LSTM\n",
    "def getCNNLSTMModel(input_shape, classes, num_words, emb_size, emb_matrix,\n",
    "                    attention=0, dense=False, emb_trainable=False):\n",
    "\n",
    "    x_input = Input(shape=(input_shape,))\n",
    "\n",
    "    emb = Embedding(num_words, emb_size, weights=[emb_matrix], trainable=emb_trainable, name='embs')(x_input)\n",
    "    emb = SpatialDropout1D(0.3)(emb)\n",
    "#     rnn = SpatialDropout1D(0.15)(rnn)\n",
    "    \n",
    "    cnn1 = Conv1D(filters=64, kernel_size=3, activation='relu', padding='same')(emb)\n",
    "    cnn2 = Conv1D(filters=64, kernel_size=4, activation='relu', padding='same')(emb)\n",
    "    cnn3 = Conv1D(filters=64, kernel_size=5, activation='relu', padding='same')(emb)\n",
    "#     cnn4 = Conv1D(filters=64, kernel_size=6, activation='relu', padding='same')(emb)\n",
    "    \n",
    "    x = concatenate([cnn1, cnn2, cnn3])\n",
    "    x = Bidirectional(CuDNNGRU(64, return_sequences=True))(x)\n",
    "    \n",
    "    if attention == 1: x = AttentionWeightedAverage()(x)\n",
    "    elif attention == 2: x = Attention()(x)\n",
    "    else: x = GlobalMaxPooling1D()(x)\n",
    "    \n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    if dense: \n",
    "        x = Dense(32, activation='relu')(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "    \n",
    "    x_output = Dense(classes, activation='sigmoid')(x)\n",
    "    return Model(inputs=x_input, outputs=x_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-28T20:37:04.717084Z",
     "start_time": "2018-02-28T20:37:04.694822Z"
    },
    "code_folding": [
     0,
     1
    ]
   },
   "outputs": [],
   "source": [
    "# MaxPool2D\n",
    "def get2xBiCuDNNGRUMaxPool2DModel(input_shape, classes, num_words, emb_size, emb_matrix,\n",
    "                                attention=0, dense=False, emb_trainable=False):\n",
    "\n",
    "    x_input = Input(shape=(input_shape,))\n",
    "    \n",
    "    emb = Embedding(num_words, emb_size, weights=[emb_matrix], trainable=emb_trainable, name='embs')(x_input)\n",
    "    emb = SpatialDropout1D(0.3)(emb)\n",
    "        \n",
    "    rnn1 = Bidirectional(CuDNNGRU(64, return_sequences=True))(emb)\n",
    "    rnn2 = Bidirectional(CuDNNGRU(64, return_sequences=True))(rnn1)\n",
    "    x = concatenate([rnn1, rnn2])\n",
    "\n",
    "    if attention == 1: x1 = AttentionWeightedAverage()(x)\n",
    "    elif attention == 2: x1 = Attention()(x)\n",
    "    else: x1 = GlobalMaxPooling1D()(x)\n",
    "        \n",
    "    if attention == 1: x2 = AttentionWeightedAverage()(Permute((2, 1))(x))\n",
    "    elif attention == 2: x2 = Attention()(Permute((2, 1))(x))\n",
    "    else: x2 = GlobalMaxPooling1D()(Permute((2, 1))(x))\n",
    "        \n",
    "    x = concatenate([x1, x2])\n",
    "    \n",
    "    if dense: \n",
    "        x = Dense(32, activation='relu')(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "    \n",
    "    x_output = Dense(classes, activation='sigmoid')(x)\n",
    "    return Model(inputs=x_input, outputs=x_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-28T20:37:04.787900Z",
     "start_time": "2018-02-28T20:37:04.717999Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# VDCNN\n",
    "from keras.models import Model\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Input, Dense, Dropout, Lambda, Activation\n",
    "from keras.layers.pooling import MaxPooling1D\n",
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import tensorflow as tf\n",
    "\n",
    "num_filters_default = [64,128,256] # from VDCNN paper\n",
    "\n",
    "\n",
    "def VDCNN_model(input_shape,num_classes,num_words,emb_size,emb_matrix,num_filters=num_filters_default,top_k=8,emb_trainable=False):\n",
    "\n",
    "    inputs = Input(shape=(input_shape, ), dtype='int32', name='inputs')\n",
    "\n",
    "    embedded_sent = Embedding(num_words, emb_size, weights=[emb_matrix], trainable=emb_trainable, name='embs')(inputs)\n",
    "    embedded_sent = SpatialDropout1D(0.3)(embedded_sent)\n",
    "    \n",
    "    conv = Conv1D(filters=64, kernel_size=3, strides=2, padding=\"same\")(embedded_sent)\n",
    "\n",
    "    for i in range(len(num_filters)):\n",
    "        conv = ConvBlockVDCNN(conv.get_shape().as_list()[1:], num_filters[i])(conv)\n",
    "        conv = MaxPooling1D(pool_size=3, strides=2, padding=\"same\")(conv)\n",
    "        \n",
    "    def k_max_pooling(x):\n",
    "        x = tf.transpose(x, [0, 2, 1])\n",
    "        k_max = tf.nn.top_k(x, k=top_k)\n",
    "        return tf.reshape(k_max[0], (-1, num_filters[-1] * top_k))\n",
    "\n",
    "    k_max = Lambda(k_max_pooling,output_shape=(num_filters[-1] * top_k,))(conv)\n",
    "    k_max = Dropout(0.3)(k_max)\n",
    "\n",
    "    # fully-connected layers\n",
    "    fc1 = Dropout(0.3)(Dense(256, activation='relu', kernel_initializer='he_normal')(k_max))\n",
    "    fc2 = Dropout(0.3)(Dense(128, activation='relu', kernel_initializer='he_normal')(fc1))\n",
    "    fc3 = Dense(num_classes, activation='sigmoid')(fc2)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=fc3)\n",
    "    return model\n",
    "\n",
    "class ConvBlockVDCNN(object):\n",
    "\n",
    "    def __init__(self, input_shape, num_filters):\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Conv1D(filters=num_filters, kernel_size=3, strides=1, padding=\"same\", input_shape=input_shape))\n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(Activation('relu'))\n",
    "\n",
    "        self.model.add(Conv1D(filters=num_filters, kernel_size=3, strides=1, padding=\"same\"))\n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(Activation('relu'))\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        return self.model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-28T20:37:04.806290Z",
     "start_time": "2018-02-28T20:37:04.788811Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Current exp model\n",
    "def getExpModel(input_shape=X.shape[1], classes=Y.shape[1], num_words=len(text_analyzer.inx2emb), max_len=max_len,\n",
    "                emb_size=text_analyzer.emb_size, emb_matrix=text_analyzer.emb_vectors,\n",
    "                emb_dropout=0.2, emb_trainable=False, rnn_units=256, conv_filters=32):\n",
    "\n",
    "    x_input = Input(shape=(input_shape,))\n",
    "    \n",
    "    emb = Embedding(num_words, emb_size, weights=[emb_matrix], trainable=emb_trainable, name='embs')(x_input)\n",
    "    emb = SpatialDropout1D(emb_dropout)(emb)\n",
    "    \n",
    "    x = Bidirectional(CuDNNGRU(rnn_units, return_sequences=True))(emb)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Reshape((2 * max_len, rnn_units, 1))(x)\n",
    "    x = Conv2D(conv_filters, (3, 3))(x)\n",
    "    x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "    x = Dense(128, activation=\"relu\")(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(64, activation=\"relu\")(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    x_output = Dense(classes, activation='sigmoid')(x)\n",
    "    return Model(inputs=x_input, outputs=x_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-28T20:37:05.716835Z",
     "start_time": "2018-02-28T20:37:04.807167Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 450)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embs (Embedding)                (None, 450, 300)     47875500    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 450, 300)     0           embs[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 450, 128)     140544      spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 450, 128)     74496       bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 450, 256)     0           bidirectional_1[0][0]            \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 256)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 6)            1542        global_max_pooling1d_1[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 48,092,082\n",
      "Trainable params: 216,582\n",
      "Non-trainable params: 47,875,500\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# import importlib\n",
    "# importlib.reload(toxic.models)\n",
    "# from toxic.models import *\n",
    "\n",
    "model_name = 'exp_model'\n",
    "model = getModel0(input_shape=X.shape[1], classes=Y.shape[1], num_words=len(text_analyzer.inx2emb), \n",
    "                    emb_size=text_analyzer.emb_size, emb_matrix=text_analyzer.emb_vectors,\n",
    "                    emb_dropout=0.5, attention=0, dense=False, emb_trainable=False)\n",
    "\n",
    "# model = getExpModel(input_shape=X.shape[1], classes=Y.shape[1], num_words=len(text_analyzer.inx2emb), max_len=max_len,\n",
    "#                     emb_size=text_analyzer.emb_size, emb_matrix=text_analyzer.emb_vectors,\n",
    "#                     emb_dropout=0.2, emb_trainable=False, rnn_units=256, conv_filters=32)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-28T20:37:05.721189Z",
     "start_time": "2018-02-28T20:37:05.717901Z"
    }
   },
   "outputs": [],
   "source": [
    "model_checkpoint = ModelCheckpoint(models_dir+model_name+'.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=6, verbose=1, mode='auto')\n",
    "lr_schedule = LearningRateScheduler(lr_change, verbose=1)\n",
    "lr_reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=1, min_lr=0.0001, verbose=1)\n",
    "# tensorboard = TensorBoard(log_dir='logs', write_graph=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-28T20:37:05.779098Z",
     "start_time": "2018-02-28T20:37:05.722270Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "val_batch_size = 1024\n",
    "weights = getClassWeights(Y, mu=0.5)\n",
    "\n",
    "# trn_seq = StratifiedFeatureSequence(X[trn_inx], Y[trn_inx], batch_size)\n",
    "# trn_seq = FeatureSequence(trn_X, trn_X_meta, trn_Y, batch_size, shuffle=True)\n",
    "trn_seq = FeatureSequence(X[trn_inx], X_meta[trn_inx], Y[trn_inx], batch_size, shuffle=True)\n",
    "val_seq = FeatureSequence(X[val_inx], X_meta[val_inx], Y[val_inx], val_batch_size)\n",
    "roc_auc_eval = RocAucEvaluation(X[val_inx], Y[val_inx], batch_size=val_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-28T21:11:57.833459Z",
     "start_time": "2018-02-28T20:37:05.780327Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/24\n",
      "561/561 [==============================] - 89s 158ms/step - loss: 0.1283 - val_loss: 0.0482\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04816, saving model to models/exp_model.h5\n",
      "ROC-AUC: 0.97185972\n",
      "\n",
      "\n",
      "Epoch 2/24\n",
      "561/561 [==============================] - 88s 157ms/step - loss: 0.0514 - val_loss: 0.0431\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04816 to 0.04312, saving model to models/exp_model.h5\n",
      "ROC-AUC: 0.98086564\n",
      "\n",
      "\n",
      "Epoch 3/24\n",
      "561/561 [==============================] - 89s 158ms/step - loss: 0.0456 - val_loss: 0.0412\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04312 to 0.04123, saving model to models/exp_model.h5\n",
      "ROC-AUC: 0.98585179\n",
      "\n",
      "\n",
      "Epoch 4/24\n",
      "561/561 [==============================] - 89s 158ms/step - loss: 0.0423 - val_loss: 0.0392\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.04123 to 0.03921, saving model to models/exp_model.h5\n",
      "ROC-AUC: 0.98795185\n",
      "\n",
      "\n",
      "Epoch 5/24\n",
      "561/561 [==============================] - 89s 158ms/step - loss: 0.0409 - val_loss: 0.0394\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "ROC-AUC: 0.98825151\n",
      "\n",
      "\n",
      "Epoch 6/24\n",
      "561/561 [==============================] - 89s 159ms/step - loss: 0.0409 - val_loss: 0.0386\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.03921 to 0.03860, saving model to models/exp_model.h5\n",
      "ROC-AUC: 0.98962767\n",
      "\n",
      "\n",
      "Epoch 7/24\n",
      "561/561 [==============================] - 89s 158ms/step - loss: 0.0398 - val_loss: 0.0379\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.03860 to 0.03792, saving model to models/exp_model.h5\n",
      "ROC-AUC: 0.99069687\n",
      "\n",
      "\n",
      "Epoch 8/24\n",
      "561/561 [==============================] - 89s 158ms/step - loss: 0.0383 - val_loss: 0.0377\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.03792 to 0.03767, saving model to models/exp_model.h5\n",
      "ROC-AUC: 0.99063054\n",
      "\n",
      "\n",
      "Epoch 9/24\n",
      "561/561 [==============================] - 89s 159ms/step - loss: 0.0378 - val_loss: 0.0376\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.03767 to 0.03756, saving model to models/exp_model.h5\n",
      "ROC-AUC: 0.99084809\n",
      "\n",
      "\n",
      "Epoch 10/24\n",
      "561/561 [==============================] - 89s 159ms/step - loss: 0.0379 - val_loss: 0.0378\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "ROC-AUC: 0.99061735\n",
      "\n",
      "\n",
      "Epoch 11/24\n",
      "561/561 [==============================] - 89s 158ms/step - loss: 0.0375 - val_loss: 0.0372\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.03756 to 0.03723, saving model to models/exp_model.h5\n",
      "ROC-AUC: 0.99120699\n",
      "\n",
      "\n",
      "Epoch 12/24\n",
      "561/561 [==============================] - 89s 158ms/step - loss: 0.0365 - val_loss: 0.0370\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.03723 to 0.03701, saving model to models/exp_model.h5\n",
      "ROC-AUC: 0.99124675\n",
      "\n",
      "\n",
      "Epoch 13/24\n",
      "561/561 [==============================] - 89s 159ms/step - loss: 0.0362 - val_loss: 0.0370\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.03701 to 0.03700, saving model to models/exp_model.h5\n",
      "ROC-AUC: 0.99124442\n",
      "\n",
      "\n",
      "Epoch 14/24\n",
      "561/561 [==============================] - 89s 158ms/step - loss: 0.0362 - val_loss: 0.0374\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "ROC-AUC: 0.99127049\n",
      "\n",
      "\n",
      "Epoch 15/24\n",
      "561/561 [==============================] - 89s 159ms/step - loss: 0.0361 - val_loss: 0.0368\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.03700 to 0.03685, saving model to models/exp_model.h5\n",
      "ROC-AUC: 0.99139986\n",
      "\n",
      "\n",
      "Epoch 16/24\n",
      "561/561 [==============================] - 89s 158ms/step - loss: 0.0355 - val_loss: 0.0369\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "ROC-AUC: 0.99145993\n",
      "\n",
      "\n",
      "Epoch 17/24\n",
      "561/561 [==============================] - 89s 158ms/step - loss: 0.0354 - val_loss: 0.0367\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.03685 to 0.03673, saving model to models/exp_model.h5\n",
      "ROC-AUC: 0.99153499\n",
      "\n",
      "\n",
      "Epoch 18/24\n",
      "561/561 [==============================] - 89s 159ms/step - loss: 0.0351 - val_loss: 0.0369\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "ROC-AUC: 0.99147083\n",
      "\n",
      "\n",
      "Epoch 19/24\n",
      "561/561 [==============================] - 89s 158ms/step - loss: 0.0351 - val_loss: 0.0369\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "ROC-AUC: 0.99151867\n",
      "\n",
      "\n",
      "Epoch 20/24\n",
      "561/561 [==============================] - 89s 158ms/step - loss: 0.0348 - val_loss: 0.0368\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n",
      "ROC-AUC: 0.99148515\n",
      "\n",
      "\n",
      "Epoch 21/24\n",
      "561/561 [==============================] - 89s 158ms/step - loss: 0.0348 - val_loss: 0.0368\n",
      "\n",
      "Epoch 00021: val_loss did not improve\n",
      "ROC-AUC: 0.99150771\n",
      "\n",
      "\n",
      "Epoch 22/24\n",
      "561/561 [==============================] - 89s 159ms/step - loss: 0.0347 - val_loss: 0.0369\n",
      "\n",
      "Epoch 00022: val_loss did not improve\n",
      "ROC-AUC: 0.99154190\n",
      "\n",
      "\n",
      "Epoch 23/24\n",
      "561/561 [==============================] - 89s 158ms/step - loss: 0.0344 - val_loss: 0.0367\n",
      "\n",
      "Epoch 00023: val_loss did not improve\n",
      "ROC-AUC: 0.99146742\n",
      "\n",
      "\n",
      "Epoch 00023: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f502e0ddf28>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs=24\n",
    "clr = CyclicLR(base_lr=0.0001, max_lr=0.003, step_size=2*len(trn_seq), mode='triangular2')\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=optimizers.Nadam())\n",
    "# model.compile(loss=\"binary_crossentropy\", optimizer=optimizers.RMSprop())\n",
    "# model.compile(loss=art_loss, optimizer=optimizers.Nadam())\n",
    "\n",
    "model.fit_generator(\n",
    "    generator=trn_seq, steps_per_epoch=len(trn_seq),\n",
    "    validation_data=val_seq, validation_steps=len(val_seq),\n",
    "    initial_epoch=0, epochs=epochs, shuffle=False, verbose=1,\n",
    "#     class_weight=weights,\n",
    "    callbacks=[model_checkpoint, clr, early_stop, roc_auc_eval],\n",
    "#     callbacks=[model_checkpoint, lr_reduce, early_stop, roc_auc_eval],\n",
    "    use_multiprocessing=False, workers=cpu_cores, max_queue_size=8*cpu_cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-28T21:11:57.835975Z",
     "start_time": "2018-02-28T21:11:57.834404Z"
    }
   },
   "outputs": [],
   "source": [
    "# matplotlib.rcParams['figure.figsize'] = (24,8)\n",
    "\n",
    "# plt.xlabel('Learning Rate')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.plot(clr.history['lr'], clr.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-28T21:11:59.023492Z",
     "start_time": "2018-02-28T21:11:57.836821Z"
    }
   },
   "outputs": [],
   "source": [
    "del model\n",
    "model = load_model(models_dir+model_name+'.h5', compile=True, \n",
    "                   custom_objects={'Attention':Attention, 'AttentionWeightedAverage':AttentionWeightedAverage, 'art_loss':art_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-28T21:12:01.100024Z",
     "start_time": "2018-02-28T21:11:59.024476Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "avg_loss: 0.036734037586947994\n",
      "ROC AUC: 0.9915349910492454\n"
     ]
    }
   ],
   "source": [
    "Y_val_pred = model.predict(X[val_inx], batch_size=1024, verbose=0)\n",
    "losses = compute_losses(Y[val_inx], Y_val_pred, eps=1e-5)\n",
    "\n",
    "val_loss = sum(losses)/len(losses)\n",
    "val_auc = metrics.roc_auc_score(Y[val_inx], Y_val_pred)\n",
    "\n",
    "print()\n",
    "print(\"avg_loss: {}\".format(val_loss))\n",
    "print(\"ROC AUC: {}\".format(val_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-28T21:22:37.864306Z",
     "start_time": "2018-02-28T21:12:01.101031Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/32\n",
      "561/561 [==============================] - 89s 158ms/step - loss: 0.0354 - val_loss: 0.0372\n",
      "\n",
      "Epoch 00025: val_loss did not improve\n",
      "ROC-AUC: 0.99129158\n",
      "\n",
      "\n",
      "Epoch 26/32\n",
      "561/561 [==============================] - 89s 158ms/step - loss: 0.0361 - val_loss: 0.0378\n",
      "\n",
      "Epoch 00026: val_loss did not improve\n",
      "ROC-AUC: 0.99106087\n",
      "\n",
      "\n",
      "Epoch 27/32\n",
      "561/561 [==============================] - 89s 158ms/step - loss: 0.0360 - val_loss: 0.0372\n",
      "\n",
      "Epoch 00027: val_loss did not improve\n",
      "ROC-AUC: 0.99140583\n",
      "\n",
      "\n",
      "Epoch 28/32\n",
      "561/561 [==============================] - 89s 158ms/step - loss: 0.0348 - val_loss: 0.0367\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.03673 to 0.03668, saving model to models/exp_model.h5\n",
      "ROC-AUC: 0.99161078\n",
      "\n",
      "\n",
      "Epoch 29/32\n",
      "561/561 [==============================] - 89s 158ms/step - loss: 0.0341 - val_loss: 0.0369\n",
      "\n",
      "Epoch 00029: val_loss did not improve\n",
      "ROC-AUC: 0.99153407\n",
      "\n",
      "\n",
      "Epoch 30/32\n",
      "561/561 [==============================] - 89s 158ms/step - loss: 0.0346 - val_loss: 0.0373\n",
      "\n",
      "Epoch 00030: val_loss did not improve\n",
      "ROC-AUC: 0.99131344\n",
      "\n",
      "\n",
      "Epoch 31/32\n",
      "561/561 [==============================] - 89s 158ms/step - loss: 0.0342 - val_loss: 0.0370\n",
      "\n",
      "Epoch 00031: val_loss did not improve\n",
      "ROC-AUC: 0.99157871\n",
      "\n",
      "\n",
      "Epoch 00031: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f502df5bbe0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs1=epochs+8\n",
    "clr = CyclicLR(base_lr=0.0001, max_lr=0.001, step_size=2*len(trn_seq), mode='triangular2')\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=4, verbose=1, mode='auto')\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=optimizers.Adam())\n",
    "# model.compile(loss=art_loss, optimizer=optimizers.Adam())\n",
    "model.fit_generator(\n",
    "    generator=trn_seq, steps_per_epoch=len(trn_seq),\n",
    "    validation_data=val_seq, validation_steps=len(val_seq),\n",
    "    initial_epoch=epochs, epochs=epochs1, shuffle=False, verbose=1,\n",
    "#     class_weight=weights,\n",
    "    callbacks=[model_checkpoint, clr, early_stop, roc_auc_eval],\n",
    "#     callbacks=[model_checkpoint, lr_reduce, early_stop, roc_auc_eval],\n",
    "    use_multiprocessing=False, workers=cpu_cores, max_queue_size=8*cpu_cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-28T21:22:39.102213Z",
     "start_time": "2018-02-28T21:22:37.866058Z"
    }
   },
   "outputs": [],
   "source": [
    "del model\n",
    "model = load_model(models_dir+model_name+'.h5', compile=True, \n",
    "                   custom_objects={'Attention':Attention, 'AttentionWeightedAverage':AttentionWeightedAverage, 'art_loss':art_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-28T21:22:41.231980Z",
     "start_time": "2018-02-28T21:22:39.103206Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "avg_loss: 0.036683071402173605\n",
      "ROC AUC: 0.9916107846105869\n"
     ]
    }
   ],
   "source": [
    "Y_val_pred = model.predict(X[val_inx], batch_size=1024, verbose=0)\n",
    "losses = compute_losses(Y[val_inx], Y_val_pred, eps=1e-5)\n",
    "\n",
    "val_loss = sum(losses)/len(losses)\n",
    "val_auc = metrics.roc_auc_score(Y[val_inx], Y_val_pred)\n",
    "\n",
    "print()\n",
    "print(\"avg_loss: {}\".format(val_loss))\n",
    "print(\"ROC AUC: {}\".format(val_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-28T21:22:41.368193Z",
     "start_time": "2018-02-28T21:22:41.232919Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-4f76a9dad686>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-28T21:22:41.368708Z",
     "start_time": "2018-02-28T20:35:26.367Z"
    }
   },
   "outputs": [],
   "source": [
    "submission_name = model_name+'__deepmoji_tokenizer__submission_'+str(round(val_loss, 5))+'_'+str(round(val_auc, 5))+'.csv'\n",
    "\n",
    "sample_submission = pd.read_csv(data_dir + 'sample_submission.csv')\n",
    "test_pred = model.predict(test_X, batch_size=1024, verbose=1)\n",
    "sample_submission[inx2label] = test_pred\n",
    "sample_submission.to_csv(results_dir+submission_name, index=False)\n",
    "\n",
    "FileLink(results_dir+submission_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-28T21:22:41.369189Z",
     "start_time": "2018-02-28T20:35:26.368Z"
    }
   },
   "outputs": [],
   "source": [
    "# pseudo\n",
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-28T21:22:41.369725Z",
     "start_time": "2018-02-28T20:35:26.370Z"
    }
   },
   "outputs": [],
   "source": [
    "model_loss_checkpoint = ModelCheckpoint(models_dir+model_name+'_pseudo.h5', monitor='val_loss', verbose=1, mode='min', save_best_only=True)\n",
    "model.compile(optimizer=optimizers.RMSprop(0.0001), loss='binary_crossentropy', metrics=[auc_roc])\n",
    "\n",
    "ps_epochs = 3\n",
    "for ps_inx in range(0, ps_epochs):  \n",
    "    test_Y = model.predict(test_X, batch_size=1024, verbose=1)\n",
    "    \n",
    "    trn_ps_seq = PseudoFeatureSequence(X[trn_inx], X_meta[trn_inx], Y[trn_inx], 182, \n",
    "                                       test_X, np.zeros((test_X.shape[0], 2)), test_Y, 74,  \n",
    "                                       shuffle=True)\n",
    "    model.fit_generator(\n",
    "        generator=trn_ps_seq, steps_per_epoch=len(trn_ps_seq),  \n",
    "        validation_data=val_seq, validation_steps=len(val_seq),\n",
    "        initial_epoch=epochs+ps_inx, epochs=epochs+ps_inx+1, \n",
    "        shuffle=False, verbose=1,\n",
    "        class_weight=weights,\n",
    "        callbacks=[model_loss_checkpoint],\n",
    "        use_multiprocessing=False, workers=cpu_cores, max_queue_size=4*cpu_cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-28T21:22:41.370239Z",
     "start_time": "2018-02-28T20:35:26.372Z"
    }
   },
   "outputs": [],
   "source": [
    "Y_val_pred = model.predict(X[val_inx], batch_size=512, verbose=0)\n",
    "losses = compute_losses(Y[val_inx], Y_val_pred, eps=1e-5)\n",
    "\n",
    "val_loss = sum(losses)/len(losses)\n",
    "val_auc = metrics.roc_auc_score(Y[val_inx], Y_val_pred)\n",
    "\n",
    "print()\n",
    "print(\"avg_loss: {}\".format(val_loss))\n",
    "print(\"ROC AUC: {}\".format(val_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-28T21:22:41.370859Z",
     "start_time": "2018-02-28T20:35:26.373Z"
    }
   },
   "outputs": [],
   "source": [
    "submission_name = 'fasttext__gru__max_pool2d__submission_'+str(round(val_loss, 5))+'_'+str(round(val_auc, 5))+'_pseudo'+ps_epochs+'.csv'\n",
    "\n",
    "sample_submission = pd.read_csv(data_dir + 'sample_submission.csv')\n",
    "test_pred = model.predict(test_X, batch_size=1024, verbose=1)\n",
    "sample_submission[inx2label] = test_pred\n",
    "sample_submission.to_csv(results_dir+submission_name, index=False)\n",
    "\n",
    "FileLink(results_dir+submission_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
