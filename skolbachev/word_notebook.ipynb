{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-01T22:09:37.803019Z",
     "start_time": "2018-03-01T22:09:37.798301Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO compare base model without Dense dropout + deepmoji tokenizer vs base one with new opt and clr\n",
    "# Add lemmatizer and remover doubled characters etc. to deepmoji tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-01T22:09:39.500262Z",
     "start_time": "2018-03-01T22:09:37.805175Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 7961730\n"
     ]
    }
   ],
   "source": [
    "# import importlib\n",
    "# import local_utils; importlib.reload(local_utils)\n",
    "from local_utils import *\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-01T22:09:43.498876Z",
     "start_time": "2018-03-01T22:09:39.504927Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "Explanation\n",
      "Why the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27\n",
      "\n",
      "Processed:\n",
      "Explanation Why the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now. \n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "ids, comments, Y, test_ids, test_comments, inx2label, label2inx = load_data()\n",
    "Y_wblank = np.concatenate([Y, np.expand_dims((~Y.any(axis=1)).astype(int), 1)], axis=1)\n",
    "print(\"Original:\\n\" + comments[0])\n",
    "print()\n",
    "\n",
    "comments = Parallel(n_jobs=cpu_cores)(delayed(preprocess)(text, False) for text in comments)\n",
    "test_comments = Parallel(n_jobs=cpu_cores)(delayed(preprocess)(text, False) for text in test_comments)\n",
    "print(\"Processed:\\n\" + comments[0])\n",
    "\n",
    "# comments_fr, comments_de, comments_es = load_augmented_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-01T22:09:46.253133Z",
     "start_time": "2018-03-01T22:09:43.502072Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# vectors, inx2word, word2inx = load_embs(embs_name='glove-twitter-200d-27B')\n",
    "vectors, inx2word, word2inx = load_embs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-01T22:09:46.255771Z",
     "start_time": "2018-03-01T22:09:46.254147Z"
    }
   },
   "outputs": [],
   "source": [
    "# def tokenize(text):\n",
    "#     return deepmoji_tokenizer.tokenize(text, vocab=word2inx)\n",
    "\n",
    "# docs = Parallel(n_jobs=cpu_cores)(delayed(tokenize)(text) for text in comments + test_comments)\n",
    "# pickle.dump(docs, open('data/tokenized_comments.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-01T22:09:48.644783Z",
     "start_time": "2018-03-01T22:09:46.256676Z"
    }
   },
   "outputs": [],
   "source": [
    "docs = pickle.load(open('data/tokenized_comments.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-01T22:10:24.525035Z",
     "start_time": "2018-03-01T22:09:48.645774Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docs: 312735\n",
      "Selected words: 56595\n",
      "Processed OOV words: 5518\n",
      "mean_len: 79.70446224439222\n",
      "mean_len + 2*std: 332.4378004978217\n",
      "mean_len + 3*std: 458.80446962453647\n"
     ]
    }
   ],
   "source": [
    "max_len = 450\n",
    "text_analyzer = TextAnalyzer(word2inx, vectors, max_len=max_len, process_oov_words=True, oov_min_doc_hits=5)\n",
    "seq, meta = text_analyzer.fit_on_docs(docs)\n",
    "\n",
    "X = seq[:len(comments)]\n",
    "test_X = seq[len(comments):]\n",
    "\n",
    "meta_mean = meta.mean(axis=0)\n",
    "meta_std = meta.std(axis=0)\n",
    "meta = (meta - meta_mean)/meta_std\n",
    "\n",
    "print(\"mean_len: {}\".format(meta_mean[0]))\n",
    "print(\"mean_len + 2*std: {}\".format(meta_mean[0]+2*meta_std[0]))\n",
    "print(\"mean_len + 3*std: {}\".format(meta_mean[0]+3*meta_std[0]))\n",
    "\n",
    "X_meta = meta[:len(comments)]\n",
    "test_X_meta = meta[len(comments):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-01T22:10:24.832227Z",
     "start_time": "2018-03-01T22:10:24.525912Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 143613, valid: 15958\n"
     ]
    }
   ],
   "source": [
    "# Train/Valid splitting\n",
    "trn_inx, val_inx = stratified_sampling(Y, 0.1, seed)\n",
    "\n",
    "print(\"train: {}, valid: {}\".format(len(trn_inx), len(val_inx)))\n",
    "# plot_stratified_sampling(Y, trn_inx, val_inx, inx2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-01T22:10:24.834972Z",
     "start_time": "2018-03-01T22:10:24.833176Z"
    }
   },
   "outputs": [],
   "source": [
    "# fr_seq, fr_meta = text_analyzer.transform_texts(comments_fr)\n",
    "# de_seq, de_meta = text_analyzer.transform_texts(comments_de)\n",
    "# es_seq, es_meta = text_analyzer.transform_texts(comments_es)\n",
    "\n",
    "# trn_X = np.concatenate([X[trn_inx], fr_seq[trn_inx], de_seq[trn_inx], es_seq[trn_inx]])\n",
    "# trn_X_meta = np.concatenate([X_meta[trn_inx], fr_meta[trn_inx], de_meta[trn_inx], es_meta[trn_inx]])\n",
    "# trn_Y = np.concatenate([Y[trn_inx], Y[trn_inx], Y[trn_inx], Y[trn_inx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-01T22:10:24.843099Z",
     "start_time": "2018-03-01T22:10:24.835803Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# CNN\n",
    "# def cnn_block(x, filters, kernel_size, attention=0):\n",
    "#     cnn = Conv1D(filters=filters, kernel_size=kernel_size, activation='relu')(x)\n",
    "    \n",
    "#     if attention == 0: cnn = GlobalMaxPooling1D()(cnn)\n",
    "#     elif attention == 1: cnn = AttentionWeightedAverage()(cnn)\n",
    "#     elif attention == 2: cnn = Attention()(cnn)\n",
    "\n",
    "#     return cnn\n",
    "\n",
    "# def getCNNModel(input_shape, classes, num_words, emb_size, emb_matrix,\n",
    "#                 attention=0, dense=False, emb_trainable=False):\n",
    "\n",
    "#     x_input = Input(shape=(input_shape,))\n",
    "    \n",
    "#     emb = Embedding(num_words, emb_size, weights=[emb_matrix], trainable=emb_trainable, name='embs')(x_input)\n",
    "#     emb = SpatialDropout1D(0.15)(emb)\n",
    "        \n",
    "#     cnn1 = cnn_block(emb, 100, 3, attention=attention)\n",
    "#     cnn2 = cnn_block(emb, 100, 4, attention=attention)\n",
    "#     cnn3 = cnn_block(emb, 100, 5, attention=attention)\n",
    "#     x = concatenate([cnn1, cnn2, cnn3])\n",
    "\n",
    "#     x = Dropout(0.15)(x)\n",
    "    \n",
    "#     if dense: \n",
    "#         x = Dense(50, activation='relu')(x)\n",
    "#         x = Dropout(0.15)(x)\n",
    "    \n",
    "#     x_output = Dense(classes, activation='sigmoid')(x)\n",
    "#     return Model(inputs=x_input, outputs=x_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-01T22:10:24.866980Z",
     "start_time": "2018-03-01T22:10:24.843954Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# CNN-LSTM\n",
    "def getCNNLSTMModel(input_shape, classes, num_words, emb_size, emb_matrix,\n",
    "                    attention=0, dense=False, emb_trainable=False):\n",
    "\n",
    "    x_input = Input(shape=(input_shape,))\n",
    "\n",
    "    emb = Embedding(num_words, emb_size, weights=[emb_matrix], trainable=emb_trainable, name='embs')(x_input)\n",
    "    emb = SpatialDropout1D(0.3)(emb)\n",
    "#     rnn = SpatialDropout1D(0.15)(rnn)\n",
    "    \n",
    "    cnn1 = Conv1D(filters=64, kernel_size=3, activation='relu', padding='same')(emb)\n",
    "    cnn2 = Conv1D(filters=64, kernel_size=4, activation='relu', padding='same')(emb)\n",
    "    cnn3 = Conv1D(filters=64, kernel_size=5, activation='relu', padding='same')(emb)\n",
    "#     cnn4 = Conv1D(filters=64, kernel_size=6, activation='relu', padding='same')(emb)\n",
    "    \n",
    "    x = concatenate([cnn1, cnn2, cnn3])\n",
    "    x = Bidirectional(CuDNNGRU(64, return_sequences=True))(x)\n",
    "    \n",
    "    if attention == 1: x = AttentionWeightedAverage()(x)\n",
    "    elif attention == 2: x = Attention()(x)\n",
    "    else: x = GlobalMaxPooling1D()(x)\n",
    "    \n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    if dense: \n",
    "        x = Dense(32, activation='relu')(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "    \n",
    "    x_output = Dense(classes, activation='sigmoid')(x)\n",
    "    return Model(inputs=x_input, outputs=x_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-01T22:10:24.890364Z",
     "start_time": "2018-03-01T22:10:24.867894Z"
    },
    "code_folding": [
     0,
     1
    ]
   },
   "outputs": [],
   "source": [
    "# MaxPool2D\n",
    "def get2xBiCuDNNGRUMaxPool2DModel(input_shape, classes, num_words, emb_size, emb_matrix,\n",
    "                                attention=0, dense=False, emb_trainable=False):\n",
    "\n",
    "    x_input = Input(shape=(input_shape,))\n",
    "    \n",
    "    emb = Embedding(num_words, emb_size, weights=[emb_matrix], trainable=emb_trainable, name='embs')(x_input)\n",
    "    emb = SpatialDropout1D(0.3)(emb)\n",
    "        \n",
    "    rnn1 = Bidirectional(CuDNNGRU(64, return_sequences=True))(emb)\n",
    "    rnn2 = Bidirectional(CuDNNGRU(64, return_sequences=True))(rnn1)\n",
    "    x = concatenate([rnn1, rnn2])\n",
    "\n",
    "    if attention == 1: x1 = AttentionWeightedAverage()(x)\n",
    "    elif attention == 2: x1 = Attention()(x)\n",
    "    else: x1 = GlobalMaxPooling1D()(x)\n",
    "        \n",
    "    if attention == 1: x2 = AttentionWeightedAverage()(Permute((2, 1))(x))\n",
    "    elif attention == 2: x2 = Attention()(Permute((2, 1))(x))\n",
    "    else: x2 = GlobalMaxPooling1D()(Permute((2, 1))(x))\n",
    "        \n",
    "    x = concatenate([x1, x2])\n",
    "    \n",
    "    if dense: \n",
    "        x = Dense(32, activation='relu')(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "    \n",
    "    x_output = Dense(classes, activation='sigmoid')(x)\n",
    "    return Model(inputs=x_input, outputs=x_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-01T22:10:24.962497Z",
     "start_time": "2018-03-01T22:10:24.891235Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# VDCNN\n",
    "from keras.models import Model\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Input, Dense, Dropout, Lambda, Activation\n",
    "from keras.layers.pooling import MaxPooling1D\n",
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import tensorflow as tf\n",
    "\n",
    "num_filters_default = [64,128,256] # from VDCNN paper\n",
    "\n",
    "\n",
    "def VDCNN_model(input_shape,num_classes,num_words,emb_size,emb_matrix,num_filters=num_filters_default,top_k=8,emb_trainable=False):\n",
    "\n",
    "    inputs = Input(shape=(input_shape, ), dtype='int32', name='inputs')\n",
    "\n",
    "    embedded_sent = Embedding(num_words, emb_size, weights=[emb_matrix], trainable=emb_trainable, name='embs')(inputs)\n",
    "    embedded_sent = SpatialDropout1D(0.3)(embedded_sent)\n",
    "    \n",
    "    conv = Conv1D(filters=64, kernel_size=3, strides=2, padding=\"same\")(embedded_sent)\n",
    "\n",
    "    for i in range(len(num_filters)):\n",
    "        conv = ConvBlockVDCNN(conv.get_shape().as_list()[1:], num_filters[i])(conv)\n",
    "        conv = MaxPooling1D(pool_size=3, strides=2, padding=\"same\")(conv)\n",
    "        \n",
    "    def k_max_pooling(x):\n",
    "        x = tf.transpose(x, [0, 2, 1])\n",
    "        k_max = tf.nn.top_k(x, k=top_k)\n",
    "        return tf.reshape(k_max[0], (-1, num_filters[-1] * top_k))\n",
    "\n",
    "    k_max = Lambda(k_max_pooling,output_shape=(num_filters[-1] * top_k,))(conv)\n",
    "    k_max = Dropout(0.3)(k_max)\n",
    "\n",
    "    # fully-connected layers\n",
    "    fc1 = Dropout(0.3)(Dense(256, activation='relu', kernel_initializer='he_normal')(k_max))\n",
    "    fc2 = Dropout(0.3)(Dense(128, activation='relu', kernel_initializer='he_normal')(fc1))\n",
    "    fc3 = Dense(num_classes, activation='sigmoid')(fc2)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=fc3)\n",
    "    return model\n",
    "\n",
    "class ConvBlockVDCNN(object):\n",
    "\n",
    "    def __init__(self, input_shape, num_filters):\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Conv1D(filters=num_filters, kernel_size=3, strides=1, padding=\"same\", input_shape=input_shape))\n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(Activation('relu'))\n",
    "\n",
    "        self.model.add(Conv1D(filters=num_filters, kernel_size=3, strides=1, padding=\"same\"))\n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(Activation('relu'))\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        return self.model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-01T22:10:24.970992Z",
     "start_time": "2018-03-01T22:10:24.963363Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# def Conv2D_block(reshape, sequence_length, embedding_dim):\n",
    "#     filter_sizes = [3,4,5]\n",
    "#     num_filters = 32\n",
    "\n",
    "#     conv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], embedding_dim), padding='valid', kernel_initializer='he_uniform', activation='relu')(reshape)\n",
    "#     conv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[1], embedding_dim), padding='valid', kernel_initializer='he_uniform', activation='relu')(reshape)\n",
    "#     conv_2 = Conv2D(num_filters, kernel_size=(filter_sizes[2], embedding_dim), padding='valid', kernel_initializer='he_uniform', activation='relu')(reshape)\n",
    "\n",
    "#     maxpool_0 = MaxPool2D(pool_size=(sequence_length - filter_sizes[0] + 1, 1), strides=(1,1), padding='valid')(conv_0)\n",
    "#     maxpool_1 = MaxPool2D(pool_size=(sequence_length - filter_sizes[1] + 1, 1), strides=(1,1), padding='valid')(conv_1)\n",
    "#     maxpool_2 = MaxPool2D(pool_size=(sequence_length - filter_sizes[2] + 1, 1), strides=(1,1), padding='valid')(conv_2)\n",
    "\n",
    "#     concatenated_tensor = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2])\n",
    "#     flatten = Flatten()(concatenated_tensor)\n",
    "#     return flatten\n",
    "\n",
    "\n",
    "# def Art_CNN(maxlen, max_features, embed_size, embedding_matrix):\n",
    "#     sequence_input = Input(shape=(maxlen,), dtype='int32')\n",
    "#     embedding = Embedding(max_features, embed_size, weights=[embedding_matrix],input_length=maxlen,trainable=False)(sequence_input)\n",
    "#     x = SpatialDropout1D(0.2)(embedding)\n",
    "#     reshape = Reshape((maxlen,embed_size,1))(x)\n",
    "#     x = Conv2D_block(reshape,maxlen,embed_size)\n",
    "#     x = Dense(256, activation=\"relu\")(x)\n",
    "#     x = Dropout(0.2)(x)\n",
    "#     preds = Dense(6, activation='sigmoid')(x)\n",
    "#     model = Model(sequence_input, preds)\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-01T22:10:24.989535Z",
     "start_time": "2018-03-01T22:10:24.971863Z"
    },
    "code_folding": [
     0,
     1
    ]
   },
   "outputs": [],
   "source": [
    "# Current exp model\n",
    "def getExpModel(input_shape=X.shape[1], classes=Y.shape[1], num_words=len(text_analyzer.inx2emb), max_len=max_len,\n",
    "                emb_size=text_analyzer.emb_size, emb_matrix=text_analyzer.emb_vectors,\n",
    "                emb_dropout=0.2, emb_trainable=False, rnn_units=256, conv_filters=32):\n",
    "\n",
    "    x_input = Input(shape=(input_shape,))\n",
    "    \n",
    "    emb = Embedding(num_words, emb_size, weights=[emb_matrix], trainable=emb_trainable, name='embs')(x_input)\n",
    "    emb = SpatialDropout1D(emb_dropout)(emb)\n",
    "    \n",
    "    x = Bidirectional(CuDNNGRU(rnn_units, return_sequences=True))(emb)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Reshape((2 * max_len, rnn_units, 1))(x)\n",
    "    x = Conv2D(conv_filters, (3, 3))(x)\n",
    "    x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "    x = Dense(128, activation=\"relu\")(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(64, activation=\"relu\")(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    x_output = Dense(classes, activation='sigmoid')(x)\n",
    "    return Model(inputs=x_input, outputs=x_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-01T22:10:25.620574Z",
     "start_time": "2018-03-01T22:10:24.990413Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 450)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embs (Embedding)                (None, 450, 300)     16978500    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 450, 300)     0           embs[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 450, 128)     140544      spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 450, 128)     74496       bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 450, 256)     0           bidirectional_1[0][0]            \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 256)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 6)            1542        global_max_pooling1d_1[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 17,195,082\n",
      "Trainable params: 216,582\n",
      "Non-trainable params: 16,978,500\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# import importlib\n",
    "# importlib.reload(toxic.models)\n",
    "# from toxic.models import *\n",
    "\n",
    "model_name = 'exp_model'\n",
    "\n",
    "model = getModel0(input_shape=X.shape[1], classes=Y.shape[1], num_words=len(text_analyzer.inx2emb), \n",
    "                  emb_size=text_analyzer.emb_size, emb_matrix=text_analyzer.emb_vectors,\n",
    "                  emb_dropout=0.3, attention=0, dense=False, emb_trainable=False)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-01T22:10:25.625102Z",
     "start_time": "2018-03-01T22:10:25.621594Z"
    }
   },
   "outputs": [],
   "source": [
    "model_checkpoint = ModelCheckpoint(models_dir+model_name+'.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=6, verbose=1, mode='auto')\n",
    "lr_schedule = LearningRateScheduler(lr_change, verbose=1)\n",
    "lr_reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=1, min_lr=0.0001, verbose=1)\n",
    "# tensorboard = TensorBoard(log_dir='logs', write_graph=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-01T22:09:37.810Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "val_batch_size = 1024\n",
    "weights = getClassWeights(Y, mu=0.5)\n",
    "\n",
    "# trn_seq = StratifiedFeatureSequence(X[trn_inx], Y[trn_inx], batch_size)\n",
    "# trn_seq = FeatureSequence(trn_X, trn_X_meta, trn_Y, batch_size, shuffle=True)\n",
    "trn_seq = FeatureSequence(X[trn_inx], X_meta[trn_inx], Y[trn_inx], batch_size, shuffle=True)\n",
    "val_seq = FeatureSequence(X[val_inx], X_meta[val_inx], Y[val_inx], val_batch_size)\n",
    "roc_auc_eval = RocAucEvaluation(X[val_inx], Y[val_inx], batch_size=val_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-01T22:09:37.812Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "561/561 [==============================] - 89s 158ms/step - loss: 0.1269 - val_loss: 0.0485\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04850, saving model to models/exp_model.h5\n",
      "ROC-AUC: 0.96752148\n",
      "\n",
      "\n",
      "Epoch 2/16\n",
      "561/561 [==============================] - 88s 157ms/step - loss: 0.0492 - val_loss: 0.0432\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04850 to 0.04318, saving model to models/exp_model.h5\n",
      "ROC-AUC: 0.98103903\n",
      "\n",
      "\n",
      "Epoch 3/16\n",
      "561/561 [==============================] - 88s 158ms/step - loss: 0.0439 - val_loss: 0.0412\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04318 to 0.04116, saving model to models/exp_model.h5\n",
      "ROC-AUC: 0.98573493\n",
      "\n",
      "\n",
      "Epoch 4/16\n",
      "561/561 [==============================] - 88s 158ms/step - loss: 0.0408 - val_loss: 0.0394\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.04116 to 0.03944, saving model to models/exp_model.h5\n",
      "ROC-AUC: 0.98757508\n",
      "\n",
      "\n",
      "Epoch 5/16\n",
      "561/561 [==============================] - 89s 158ms/step - loss: 0.0398 - val_loss: 0.0400\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "ROC-AUC: 0.98770033\n",
      "\n",
      "\n",
      "Epoch 6/16\n",
      "561/561 [==============================] - 89s 158ms/step - loss: 0.0399 - val_loss: 0.0394\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.03944 to 0.03941, saving model to models/exp_model.h5\n",
      "ROC-AUC: 0.98841084\n",
      "\n",
      "\n",
      "Epoch 7/16\n",
      "561/561 [==============================] - 88s 158ms/step - loss: 0.0388 - val_loss: 0.0388\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.03941 to 0.03882, saving model to models/exp_model.h5\n",
      "ROC-AUC: 0.98957035\n",
      "\n",
      "\n",
      "Epoch 8/16\n",
      "561/561 [==============================] - 89s 158ms/step - loss: 0.0372 - val_loss: 0.0384\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.03882 to 0.03838, saving model to models/exp_model.h5\n",
      "ROC-AUC: 0.98988616\n",
      "\n",
      "\n",
      "Epoch 9/16\n",
      "561/561 [==============================] - 89s 158ms/step - loss: 0.0366 - val_loss: 0.0382\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.03838 to 0.03817, saving model to models/exp_model.h5\n",
      "ROC-AUC: 0.99005890\n",
      "\n",
      "\n",
      "Epoch 10/16\n",
      "561/561 [==============================] - 89s 158ms/step - loss: 0.0368 - val_loss: 0.0387\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "ROC-AUC: 0.98959925\n",
      "\n",
      "\n",
      "Epoch 11/16\n",
      "561/561 [==============================] - 89s 158ms/step - loss: 0.0363 - val_loss: 0.0384\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "ROC-AUC: 0.99016378\n",
      "\n",
      "\n",
      "Epoch 12/16\n",
      "561/561 [==============================] - 89s 158ms/step - loss: 0.0353 - val_loss: 0.0383\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "ROC-AUC: 0.99022141\n",
      "\n",
      "\n",
      "Epoch 13/16\n",
      " 85/561 [===>..........................] - ETA: 1:13 - loss: 0.0347"
     ]
    }
   ],
   "source": [
    "epochs=32\n",
    "clr = CyclicLR(base_lr=0.0001, max_lr=0.003, step_size=2*len(trn_seq), mode='triangular2')\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=optimizers.Nadam())\n",
    "# model.compile(loss=\"binary_crossentropy\", optimizer=optimizers.RMSprop())\n",
    "# model.compile(loss=art_loss, optimizer=optimizers.Nadam())\n",
    "\n",
    "model.fit_generator(\n",
    "    generator=trn_seq, steps_per_epoch=len(trn_seq),\n",
    "    validation_data=val_seq, validation_steps=len(val_seq),\n",
    "    initial_epoch=0, epochs=epochs, shuffle=False, verbose=1,\n",
    "#     class_weight=weights,\n",
    "    callbacks=[model_checkpoint, clr, early_stop, roc_auc_eval],\n",
    "#     callbacks=[model_checkpoint, lr_reduce, early_stop, roc_auc_eval],\n",
    "    use_multiprocessing=False, workers=cpu_cores, max_queue_size=8*cpu_cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-01T22:09:37.813Z"
    }
   },
   "outputs": [],
   "source": [
    "# matplotlib.rcParams['figure.figsize'] = (24,8)\n",
    "\n",
    "# plt.xlabel('Learning Rate')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.plot(clr.history['lr'], clr.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-01T22:09:37.815Z"
    }
   },
   "outputs": [],
   "source": [
    "del model\n",
    "model = load_model(models_dir+model_name+'.h5', compile=True, \n",
    "                   custom_objects={'Attention':Attention, 'AttentionWeightedAverage':AttentionWeightedAverage, 'art_loss':art_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-01T22:09:37.817Z"
    }
   },
   "outputs": [],
   "source": [
    "Y_val_pred = model.predict(X[val_inx], batch_size=1024, verbose=0)\n",
    "losses = compute_losses(Y[val_inx], Y_val_pred, eps=1e-5)\n",
    "\n",
    "val_loss = sum(losses)/len(losses)\n",
    "val_auc = metrics.roc_auc_score(Y[val_inx], Y_val_pred)\n",
    "\n",
    "print()\n",
    "print(\"avg_loss: {}\".format(val_loss))\n",
    "print(\"ROC AUC: {}\".format(val_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-01T22:09:37.818Z"
    }
   },
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-01T22:09:37.820Z"
    }
   },
   "outputs": [],
   "source": [
    "epochs1=epochs+8\n",
    "clr = CyclicLR(base_lr=0.0001, max_lr=0.001, step_size=2*len(trn_seq), mode='triangular2')\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=4, verbose=1, mode='auto')\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=optimizers.Adam())\n",
    "# model.compile(loss=art_loss, optimizer=optimizers.Adam())\n",
    "model.fit_generator(\n",
    "    generator=trn_seq, steps_per_epoch=len(trn_seq),\n",
    "    validation_data=val_seq, validation_steps=len(val_seq),\n",
    "    initial_epoch=epochs, epochs=epochs1, shuffle=False, verbose=1,\n",
    "#     class_weight=weights,\n",
    "    callbacks=[model_checkpoint, clr, early_stop, roc_auc_eval],\n",
    "#     callbacks=[model_checkpoint, lr_reduce, early_stop, roc_auc_eval],\n",
    "    use_multiprocessing=False, workers=cpu_cores, max_queue_size=8*cpu_cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-01T22:09:37.821Z"
    }
   },
   "outputs": [],
   "source": [
    "del model\n",
    "model = load_model(models_dir+model_name+'.h5', compile=True, \n",
    "                   custom_objects={'Attention':Attention, 'AttentionWeightedAverage':AttentionWeightedAverage, 'art_loss':art_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-01T22:09:37.823Z"
    }
   },
   "outputs": [],
   "source": [
    "Y_val_pred = model.predict(X[val_inx], batch_size=1024, verbose=0)\n",
    "losses = compute_losses(Y[val_inx], Y_val_pred, eps=1e-5)\n",
    "\n",
    "val_loss = sum(losses)/len(losses)\n",
    "val_auc = metrics.roc_auc_score(Y[val_inx], Y_val_pred)\n",
    "\n",
    "print()\n",
    "print(\"avg_loss: {}\".format(val_loss))\n",
    "print(\"ROC AUC: {}\".format(val_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-01T22:09:37.824Z"
    }
   },
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-01T22:09:37.826Z"
    }
   },
   "outputs": [],
   "source": [
    "submission_name = model_name+'__deepmoji_tokenizer__submission_'+str(round(val_loss, 5))+'_'+str(round(val_auc, 5))+'.csv'\n",
    "\n",
    "sample_submission = pd.read_csv(data_dir + 'sample_submission.csv')\n",
    "test_pred = model.predict(test_X, batch_size=1024, verbose=1)\n",
    "sample_submission[inx2label] = test_pred\n",
    "sample_submission.to_csv(results_dir+submission_name, index=False)\n",
    "\n",
    "FileLink(results_dir+submission_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-01T22:09:37.827Z"
    }
   },
   "outputs": [],
   "source": [
    "# pseudo\n",
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-01T22:09:37.829Z"
    }
   },
   "outputs": [],
   "source": [
    "model_loss_checkpoint = ModelCheckpoint(models_dir+model_name+'_pseudo.h5', monitor='val_loss', verbose=1, mode='min', save_best_only=True)\n",
    "model.compile(optimizer=optimizers.RMSprop(0.0001), loss='binary_crossentropy', metrics=[auc_roc])\n",
    "\n",
    "ps_epochs = 3\n",
    "for ps_inx in range(0, ps_epochs):  \n",
    "    test_Y = model.predict(test_X, batch_size=1024, verbose=1)\n",
    "    \n",
    "    trn_ps_seq = PseudoFeatureSequence(X[trn_inx], X_meta[trn_inx], Y[trn_inx], 182, \n",
    "                                       test_X, np.zeros((test_X.shape[0], 2)), test_Y, 74,  \n",
    "                                       shuffle=True)\n",
    "    model.fit_generator(\n",
    "        generator=trn_ps_seq, steps_per_epoch=len(trn_ps_seq),  \n",
    "        validation_data=val_seq, validation_steps=len(val_seq),\n",
    "        initial_epoch=epochs+ps_inx, epochs=epochs+ps_inx+1, \n",
    "        shuffle=False, verbose=1,\n",
    "        class_weight=weights,\n",
    "        callbacks=[model_loss_checkpoint],\n",
    "        use_multiprocessing=False, workers=cpu_cores, max_queue_size=4*cpu_cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-01T22:09:37.830Z"
    }
   },
   "outputs": [],
   "source": [
    "Y_val_pred = model.predict(X[val_inx], batch_size=512, verbose=0)\n",
    "losses = compute_losses(Y[val_inx], Y_val_pred, eps=1e-5)\n",
    "\n",
    "val_loss = sum(losses)/len(losses)\n",
    "val_auc = metrics.roc_auc_score(Y[val_inx], Y_val_pred)\n",
    "\n",
    "print()\n",
    "print(\"avg_loss: {}\".format(val_loss))\n",
    "print(\"ROC AUC: {}\".format(val_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-01T22:09:37.833Z"
    }
   },
   "outputs": [],
   "source": [
    "submission_name = 'fasttext__gru__max_pool2d__submission_'+str(round(val_loss, 5))+'_'+str(round(val_auc, 5))+'_pseudo'+ps_epochs+'.csv'\n",
    "\n",
    "sample_submission = pd.read_csv(data_dir + 'sample_submission.csv')\n",
    "test_pred = model.predict(test_X, batch_size=1024, verbose=1)\n",
    "sample_submission[inx2label] = test_pred\n",
    "sample_submission.to_csv(results_dir+submission_name, index=False)\n",
    "\n",
    "FileLink(results_dir+submission_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
