{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-19T12:48:24.948657Z",
     "start_time": "2018-03-19T12:48:23.093367Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 7961730\n"
     ]
    }
   ],
   "source": [
    "# import importlib\n",
    "# import local_utils; importlib.reload(local_utils)\n",
    "from local_utils import *\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-19T12:48:28.818221Z",
     "start_time": "2018-03-19T12:48:24.949680Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "explanation why the edits made under my username hardcore metallica fan were reverted? they weren't vandalisms, just closure on some gas after i voted at new york dolls fac. and please don't remove the template from the talk page since I'm retired now. \n",
      "\n",
      "Processed:\n",
      "explanation why the edits made under my username hardcore metallica fan were reverted? they weren't vandalisms, just closure on some gas after i voted at new york dolls fac. and please don't remove the template from the talk page since I'm retired now. \n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "ids, comments, Y, test_ids, test_comments, inx2label, label2inx = load_data(True)\n",
    "Y_wblank = np.concatenate([Y, np.expand_dims((~Y.any(axis=1)).astype(int), 1)], axis=1)\n",
    "print(\"Original:\\n\" + comments[0])\n",
    "print()\n",
    "\n",
    "comments = Parallel(n_jobs=cpu_cores)(delayed(preprocess)(text, False) for text in comments)\n",
    "test_comments = Parallel(n_jobs=cpu_cores)(delayed(preprocess)(text, False) for text in test_comments)\n",
    "print(\"Processed:\\n\" + comments[0])\n",
    "\n",
    "# comments_fr, comments_de, comments_es = load_augmented_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-19T12:48:28.822110Z",
     "start_time": "2018-03-19T12:48:28.819357Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# def get_coefs(word,*arr):\n",
    "#     return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "# inx2word = []\n",
    "# vectors = []\n",
    "# for o in open('data/talk/vectors_utf8.txt', encoding='utf8'):\n",
    "#     try:\n",
    "#         w,v = get_coefs(*o.strip().split(' '))\n",
    "#         if len(v) == 300:\n",
    "#             inx2word.append(w)\n",
    "#             vectors.append(v)\n",
    "#     except:\n",
    "#         pass\n",
    "\n",
    "# vectors = np.stack(vectors)\n",
    "# barr = bcolz.carray(vectors, rootdir=\"data/talk/glove-cheng-vectors.bcolz\")\n",
    "# barr.flush()\n",
    "\n",
    "# pickle.dump(inx2word, open('data/talk/glove-cheng-words.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-19T12:48:31.709858Z",
     "start_time": "2018-03-19T12:48:28.822961Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "vectors, inx2word, word2inx = load_embs()\n",
    "# vectors, inx2word, word2inx = load_embs(embs_dir='data/talk/', embs_name='glove-cheng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-19T12:48:31.712564Z",
     "start_time": "2018-03-19T12:48:31.710954Z"
    }
   },
   "outputs": [],
   "source": [
    "# def tokenize(text):\n",
    "#     return deepmoji_tokenizer.tokenize(text, word2inx)\n",
    "\n",
    "# docs = Parallel(n_jobs=cpu_cores)(delayed(tokenize)(text) for text in comments + test_comments)\n",
    "# pickle.dump(docs, open('data/tokenized_comments.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-19T12:48:34.064898Z",
     "start_time": "2018-03-19T12:48:31.713504Z"
    }
   },
   "outputs": [],
   "source": [
    "docs = pickle.load(open('data/tokenized_comments.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-19T12:49:03.945027Z",
     "start_time": "2018-03-19T12:48:34.065937Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docs: 312735\n",
      "Selected words: 172905\n",
      "Processed OOV words: 4533\n",
      "mean_len: 74.75018466113482\n",
      "mean_len + 2*std: 313.1402682213584\n",
      "mean_len + 3*std: 432.33531000147013\n"
     ]
    }
   ],
   "source": [
    "max_len = 450\n",
    "text_analyzer = TextAnalyzer(word2inx, vectors, reverse=True, max_len=max_len, process_oov_words=True, oov_min_doc_hits=5)\n",
    "seq, meta = text_analyzer.fit_on_docs(docs)\n",
    "\n",
    "X = seq[:len(comments)]\n",
    "test_X = seq[len(comments):]\n",
    "\n",
    "meta_mean = meta.mean(axis=0)\n",
    "meta_std = meta.std(axis=0)\n",
    "meta = (meta - meta_mean)/meta_std\n",
    "\n",
    "print(\"mean_len: {}\".format(meta_mean[0]))\n",
    "print(\"mean_len + 2*std: {}\".format(meta_mean[0]+2*meta_std[0]))\n",
    "print(\"mean_len + 3*std: {}\".format(meta_mean[0]+3*meta_std[0]))\n",
    "\n",
    "X_meta = meta[:len(comments)]\n",
    "test_X_meta = meta[len(comments):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-19T12:49:04.277491Z",
     "start_time": "2018-03-19T12:49:03.946342Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 143613, valid: 15958\n"
     ]
    }
   ],
   "source": [
    "# Train/Valid splitting\n",
    "trn_inx, val_inx = stratified_sampling(Y, 0.1, seed)\n",
    "\n",
    "print(\"train: {}, valid: {}\".format(len(trn_inx), len(val_inx)))\n",
    "# plot_stratified_sampling(Y, trn_inx, val_inx, inx2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-19T12:49:04.280924Z",
     "start_time": "2018-03-19T12:49:04.278763Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# ros = RandomOverSampler(random_state=seed)\n",
    "# trn_inx = ros.fit_sample(trn_inx.reshape(-1, 1), Y_wblank[trn_inx,6])[0].flatten()\n",
    "\n",
    "# rus = RandomUnderSampler(return_indices=False)\n",
    "# trn_inx = rus.fit_sample(trn_inx.reshape(-1, 1), Y_wblank[trn_inx,6])[0].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-19T12:49:04.292511Z",
     "start_time": "2018-03-19T12:49:04.282206Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# fr_seq, fr_meta = text_analyzer.transform_texts(comments_fr)\n",
    "# de_seq, de_meta = text_analyzer.transform_texts(comments_de)\n",
    "# es_seq, es_meta = text_analyzer.transform_texts(comments_es)\n",
    "\n",
    "# trn_X = np.concatenate([X[trn_inx], fr_seq[trn_inx], de_seq[trn_inx], es_seq[trn_inx]])\n",
    "# trn_X_meta = np.concatenate([X_meta[trn_inx], fr_meta[trn_inx], de_meta[trn_inx], es_meta[trn_inx]])\n",
    "# trn_Y = np.concatenate([Y[trn_inx], Y[trn_inx], Y[trn_inx], Y[trn_inx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-19T12:49:04.298350Z",
     "start_time": "2018-03-19T12:49:04.293780Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# CNN\n",
    "# def cnn_block(x, filters, kernel_size, attention=0):\n",
    "#     cnn = Conv1D(filters=filters, kernel_size=kernel_size, activation='relu')(x)\n",
    "    \n",
    "#     if attention == 0: cnn = GlobalMaxPooling1D()(cnn)\n",
    "#     elif attention == 1: cnn = AttentionWeightedAverage()(cnn)\n",
    "#     elif attention == 2: cnn = Attention()(cnn)\n",
    "\n",
    "#     return cnn\n",
    "\n",
    "# def getCNNModel(input_shape, classes, num_words, emb_size, emb_matrix,\n",
    "#                 attention=0, dense=False, emb_trainable=False):\n",
    "\n",
    "#     x_input = Input(shape=(input_shape,))\n",
    "    \n",
    "#     emb = Embedding(num_words, emb_size, weights=[emb_matrix], trainable=emb_trainable, name='embs')(x_input)\n",
    "#     emb = SpatialDropout1D(0.15)(emb)\n",
    "        \n",
    "#     cnn1 = cnn_block(emb, 100, 3, attention=attention)\n",
    "#     cnn2 = cnn_block(emb, 100, 4, attention=attention)\n",
    "#     cnn3 = cnn_block(emb, 100, 5, attention=attention)\n",
    "#     x = concatenate([cnn1, cnn2, cnn3])\n",
    "\n",
    "#     x = Dropout(0.15)(x)\n",
    "    \n",
    "#     if dense: \n",
    "#         x = Dense(50, activation='relu')(x)\n",
    "#         x = Dropout(0.15)(x)\n",
    "    \n",
    "#     x_output = Dense(classes, activation='sigmoid')(x)\n",
    "#     return Model(inputs=x_input, outputs=x_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-19T12:49:04.322865Z",
     "start_time": "2018-03-19T12:49:04.299671Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# CNN-LSTM\n",
    "def getCNNLSTMModel(input_shape, classes, num_words, emb_size, emb_matrix,\n",
    "                    attention=0, dense=False, emb_trainable=False):\n",
    "\n",
    "    x_input = Input(shape=(input_shape,))\n",
    "\n",
    "    emb = Embedding(num_words, emb_size, weights=[emb_matrix], trainable=emb_trainable, name='embs')(x_input)\n",
    "    emb = SpatialDropout1D(0.3)(emb)\n",
    "#     rnn = SpatialDropout1D(0.15)(rnn)\n",
    "    \n",
    "    cnn1 = Conv1D(filters=64, kernel_size=3, activation='relu', padding='same')(emb)\n",
    "    cnn2 = Conv1D(filters=64, kernel_size=4, activation='relu', padding='same')(emb)\n",
    "    cnn3 = Conv1D(filters=64, kernel_size=5, activation='relu', padding='same')(emb)\n",
    "#     cnn4 = Conv1D(filters=64, kernel_size=6, activation='relu', padding='same')(emb)\n",
    "    \n",
    "    x = concatenate([cnn1, cnn2, cnn3])\n",
    "    x = Bidirectional(CuDNNGRU(64, return_sequences=True))(x)\n",
    "    \n",
    "    if attention == 1: x = AttentionWeightedAverage()(x)\n",
    "    elif attention == 2: x = Attention()(x)\n",
    "    else: x = GlobalMaxPooling1D()(x)\n",
    "    \n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    if dense: \n",
    "        x = Dense(32, activation='relu')(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "    \n",
    "    x_output = Dense(classes, activation='sigmoid')(x)\n",
    "    return Model(inputs=x_input, outputs=x_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-19T12:49:04.347538Z",
     "start_time": "2018-03-19T12:49:04.324301Z"
    },
    "code_folding": [
     0,
     1
    ]
   },
   "outputs": [],
   "source": [
    "# MaxPool2D\n",
    "def get2xBiCuDNNGRUMaxPool2DModel(input_shape, classes, num_words, emb_size, emb_matrix,\n",
    "                                attention=0, dense=False, emb_trainable=False):\n",
    "\n",
    "    x_input = Input(shape=(input_shape,))\n",
    "    \n",
    "    emb = Embedding(num_words, emb_size, weights=[emb_matrix], trainable=emb_trainable, name='embs')(x_input)\n",
    "    emb = SpatialDropout1D(0.3)(emb)\n",
    "        \n",
    "    rnn1 = Bidirectional(CuDNNGRU(64, return_sequences=True))(emb)\n",
    "    rnn2 = Bidirectional(CuDNNGRU(64, return_sequences=True))(rnn1)\n",
    "    x = concatenate([rnn1, rnn2])\n",
    "\n",
    "    if attention == 1: x1 = AttentionWeightedAverage()(x)\n",
    "    elif attention == 2: x1 = Attention()(x)\n",
    "    else: x1 = GlobalMaxPooling1D()(x)\n",
    "        \n",
    "    if attention == 1: x2 = AttentionWeightedAverage()(Permute((2, 1))(x))\n",
    "    elif attention == 2: x2 = Attention()(Permute((2, 1))(x))\n",
    "    else: x2 = GlobalMaxPooling1D()(Permute((2, 1))(x))\n",
    "        \n",
    "    x = concatenate([x1, x2])\n",
    "    \n",
    "    if dense: \n",
    "        x = Dense(32, activation='relu')(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "    \n",
    "    x_output = Dense(classes, activation='sigmoid')(x)\n",
    "    return Model(inputs=x_input, outputs=x_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-19T12:49:04.420170Z",
     "start_time": "2018-03-19T12:49:04.348662Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# VDCNN\n",
    "from keras.models import Model\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Input, Dense, Dropout, Lambda, Activation\n",
    "from keras.layers.pooling import MaxPooling1D\n",
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import tensorflow as tf\n",
    "\n",
    "num_filters_default = [64,128,256] # from VDCNN paper\n",
    "\n",
    "\n",
    "def VDCNN_model(input_shape,num_classes,num_words,emb_size,emb_matrix,num_filters=num_filters_default,top_k=8,emb_trainable=False):\n",
    "\n",
    "    inputs = Input(shape=(input_shape, ), dtype='int32', name='inputs')\n",
    "\n",
    "    embedded_sent = Embedding(num_words, emb_size, weights=[emb_matrix], trainable=emb_trainable, name='embs')(inputs)\n",
    "    embedded_sent = SpatialDropout1D(0.3)(embedded_sent)\n",
    "    \n",
    "    conv = Conv1D(filters=64, kernel_size=3, strides=2, padding=\"same\")(embedded_sent)\n",
    "\n",
    "    for i in range(len(num_filters)):\n",
    "        conv = ConvBlockVDCNN(conv.get_shape().as_list()[1:], num_filters[i])(conv)\n",
    "        conv = MaxPooling1D(pool_size=3, strides=2, padding=\"same\")(conv)\n",
    "        \n",
    "    def k_max_pooling(x):\n",
    "        x = tf.transpose(x, [0, 2, 1])\n",
    "        k_max = tf.nn.top_k(x, k=top_k)\n",
    "        return tf.reshape(k_max[0], (-1, num_filters[-1] * top_k))\n",
    "\n",
    "    k_max = Lambda(k_max_pooling,output_shape=(num_filters[-1] * top_k,))(conv)\n",
    "    k_max = Dropout(0.3)(k_max)\n",
    "\n",
    "    # fully-connected layers\n",
    "    fc1 = Dropout(0.3)(Dense(256, activation='relu', kernel_initializer='he_normal')(k_max))\n",
    "    fc2 = Dropout(0.3)(Dense(128, activation='relu', kernel_initializer='he_normal')(fc1))\n",
    "    fc3 = Dense(num_classes, activation='sigmoid')(fc2)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=fc3)\n",
    "    return model\n",
    "\n",
    "class ConvBlockVDCNN(object):\n",
    "\n",
    "    def __init__(self, input_shape, num_filters):\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Conv1D(filters=num_filters, kernel_size=3, strides=1, padding=\"same\", input_shape=input_shape))\n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(Activation('relu'))\n",
    "\n",
    "        self.model.add(Conv1D(filters=num_filters, kernel_size=3, strides=1, padding=\"same\"))\n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(Activation('relu'))\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        return self.model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-19T12:49:04.428747Z",
     "start_time": "2018-03-19T12:49:04.421392Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# def Conv2D_block(reshape, sequence_length, embedding_dim):\n",
    "#     filter_sizes = [3,4,5]\n",
    "#     num_filters = 32\n",
    "\n",
    "#     conv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], embedding_dim), padding='valid', kernel_initializer='he_uniform', activation='relu')(reshape)\n",
    "#     conv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[1], embedding_dim), padding='valid', kernel_initializer='he_uniform', activation='relu')(reshape)\n",
    "#     conv_2 = Conv2D(num_filters, kernel_size=(filter_sizes[2], embedding_dim), padding='valid', kernel_initializer='he_uniform', activation='relu')(reshape)\n",
    "\n",
    "#     maxpool_0 = MaxPool2D(pool_size=(sequence_length - filter_sizes[0] + 1, 1), strides=(1,1), padding='valid')(conv_0)\n",
    "#     maxpool_1 = MaxPool2D(pool_size=(sequence_length - filter_sizes[1] + 1, 1), strides=(1,1), padding='valid')(conv_1)\n",
    "#     maxpool_2 = MaxPool2D(pool_size=(sequence_length - filter_sizes[2] + 1, 1), strides=(1,1), padding='valid')(conv_2)\n",
    "\n",
    "#     concatenated_tensor = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2])\n",
    "#     flatten = Flatten()(concatenated_tensor)\n",
    "#     return flatten\n",
    "\n",
    "\n",
    "# def Art_CNN(maxlen, max_features, embed_size, embedding_matrix):\n",
    "#     sequence_input = Input(shape=(maxlen,), dtype='int32')\n",
    "#     embedding = Embedding(max_features, embed_size, weights=[embedding_matrix],input_length=maxlen,trainable=False)(sequence_input)\n",
    "#     x = SpatialDropout1D(0.2)(embedding)\n",
    "#     reshape = Reshape((maxlen,embed_size,1))(x)\n",
    "#     x = Conv2D_block(reshape,maxlen,embed_size)\n",
    "#     x = Dense(256, activation=\"relu\")(x)\n",
    "#     x = Dropout(0.2)(x)\n",
    "#     preds = Dense(6, activation='sigmoid')(x)\n",
    "#     model = Model(sequence_input, preds)\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-19T12:49:04.445574Z",
     "start_time": "2018-03-19T12:49:04.430000Z"
    },
    "code_folding": [
     0,
     1
    ]
   },
   "outputs": [],
   "source": [
    "# Current exp model\n",
    "def getExpModel0(input_shape, classes, num_words, emb_size, emb_matrix, emb_dropout=0.5,\n",
    "                 attention=0, dense=False, emb_trainable=False):\n",
    "\n",
    "    x_input = Input(shape=(input_shape,))\n",
    "    \n",
    "    emb = Embedding(num_words, emb_size, weights=[emb_matrix], trainable=emb_trainable, name='embs')(x_input)\n",
    "    emb = SpatialDropout1D(emb_dropout)(emb)\n",
    "        \n",
    "    rnn1 = Bidirectional(CuDNNGRU(64, return_sequences=True))(emb)\n",
    "    rnn2 = Bidirectional(CuDNNGRU(64, return_sequences=True))(rnn1)\n",
    "    x = concatenate([rnn1, rnn2])\n",
    "\n",
    "    if attention == 1: x = AttentionWeightedAverage()(x)\n",
    "    elif attention == 2: x = Attention()(x)\n",
    "    else: x = GlobalMaxPooling1D()(x)\n",
    "    \n",
    "    if dense: \n",
    "        x = Dense(32, activation='relu')(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "    \n",
    "    x_output = Dense(classes, activation='sigmoid')(x)\n",
    "    return Model(inputs=x_input, outputs=x_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-19T12:49:05.183287Z",
     "start_time": "2018-03-19T12:49:04.446832Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 450)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embs (Embedding)                (None, 450, 300)     51871500    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 450, 300)     0           embs[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 450, 128)     140544      spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 450, 128)     74496       bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 450, 256)     0           bidirectional_1[0][0]            \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 256)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 6)            1542        global_max_pooling1d_1[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 52,088,082\n",
      "Trainable params: 216,582\n",
      "Non-trainable params: 51,871,500\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# import importlib\n",
    "# importlib.reload(toxic.models)\n",
    "# from toxic.models import *\n",
    "\n",
    "model_name = 'exp_model'\n",
    "\n",
    "model = getModel0(input_shape=X.shape[1], classes=Y.shape[1], num_words=len(text_analyzer.inx2emb), \n",
    "                  emb_size=text_analyzer.emb_size, emb_matrix=text_analyzer.emb_vectors,\n",
    "                  emb_dropout=0.5, attention=0, dense=False, emb_trainable=False, gru=True)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-19T12:49:05.187883Z",
     "start_time": "2018-03-19T12:49:05.184433Z"
    }
   },
   "outputs": [],
   "source": [
    "model_checkpoint = ModelCheckpoint(models_dir+model_name+'.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=6, verbose=1, mode='min')\n",
    "lr_schedule = LearningRateScheduler(lr_change, verbose=1)\n",
    "lr_reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=1, min_lr=0.0001, verbose=1)\n",
    "# tensorboard = TensorBoard(log_dir='logs', write_graph=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-19T12:49:05.238358Z",
     "start_time": "2018-03-19T12:49:05.188845Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "val_batch_size = 1024\n",
    "weights = getClassWeights(Y, mu=0.5)\n",
    "\n",
    "# trn_seq = StratifiedFeatureSequence(X[trn_inx], Y[trn_inx], batch_size)\n",
    "trn_seq = FeatureSequence(X[trn_inx], Y[trn_inx], batch_size, shuffle=True)\n",
    "val_seq = FeatureSequence(X[val_inx], Y[val_inx], val_batch_size)\n",
    "roc_auc_eval = RocAucEvaluation(X[val_inx], Y[val_inx], batch_size=val_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-19T13:32:51.521308Z",
     "start_time": "2018-03-19T12:49:05.239532Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/32\n",
      "561/561 [==============================] - 88s 157ms/step - loss: 0.1280 - val_loss: 0.0473\n",
      "ROC-AUC: 0.96839956\n",
      "\n",
      "\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04735, saving model to models/exp_model.h5\n",
      "Epoch 2/32\n",
      "561/561 [==============================] - 88s 157ms/step - loss: 0.0508 - val_loss: 0.0424\n",
      "ROC-AUC: 0.98194927\n",
      "\n",
      "\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04735 to 0.04241, saving model to models/exp_model.h5\n",
      "Epoch 3/32\n",
      "561/561 [==============================] - 88s 158ms/step - loss: 0.0449 - val_loss: 0.0400\n",
      "ROC-AUC: 0.98761452\n",
      "\n",
      "\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04241 to 0.04003, saving model to models/exp_model.h5\n",
      "Epoch 4/32\n",
      "561/561 [==============================] - 88s 158ms/step - loss: 0.0414 - val_loss: 0.0382\n",
      "ROC-AUC: 0.98925409\n",
      "\n",
      "\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.04003 to 0.03816, saving model to models/exp_model.h5\n",
      "Epoch 5/32\n",
      "561/561 [==============================] - 88s 158ms/step - loss: 0.0402 - val_loss: 0.0382\n",
      "ROC-AUC: 0.98961268\n",
      "\n",
      "\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/32\n",
      "561/561 [==============================] - 88s 158ms/step - loss: 0.0403 - val_loss: 0.0382\n",
      "ROC-AUC: 0.98975155\n",
      "\n",
      "\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/32\n",
      "561/561 [==============================] - 88s 158ms/step - loss: 0.0393 - val_loss: 0.0377\n",
      "ROC-AUC: 0.99082341\n",
      "\n",
      "\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.03816 to 0.03774, saving model to models/exp_model.h5\n",
      "Epoch 8/32\n",
      "561/561 [==============================] - 89s 158ms/step - loss: 0.0377 - val_loss: 0.0370\n",
      "ROC-AUC: 0.99087927\n",
      "\n",
      "\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.03774 to 0.03704, saving model to models/exp_model.h5\n",
      "Epoch 9/32\n",
      "561/561 [==============================] - 88s 158ms/step - loss: 0.0372 - val_loss: 0.0367\n",
      "ROC-AUC: 0.99102456\n",
      "\n",
      "\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.03704 to 0.03669, saving model to models/exp_model.h5\n",
      "Epoch 10/32\n",
      "561/561 [==============================] - 88s 158ms/step - loss: 0.0374 - val_loss: 0.0372\n",
      "ROC-AUC: 0.99059857\n",
      "\n",
      "\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/32\n",
      "561/561 [==============================] - 89s 158ms/step - loss: 0.0369 - val_loss: 0.0369\n",
      "ROC-AUC: 0.99125835\n",
      "\n",
      "\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/32\n",
      "561/561 [==============================] - 89s 158ms/step - loss: 0.0360 - val_loss: 0.0366\n",
      "ROC-AUC: 0.99136356\n",
      "\n",
      "\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.03669 to 0.03660, saving model to models/exp_model.h5\n",
      "Epoch 13/32\n",
      "561/561 [==============================] - 88s 158ms/step - loss: 0.0357 - val_loss: 0.0365\n",
      "ROC-AUC: 0.99130889\n",
      "\n",
      "\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.03660 to 0.03654, saving model to models/exp_model.h5\n",
      "Epoch 14/32\n",
      "561/561 [==============================] - 88s 158ms/step - loss: 0.0357 - val_loss: 0.0372\n",
      "ROC-AUC: 0.99128979\n",
      "\n",
      "\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/32\n",
      "561/561 [==============================] - 88s 158ms/step - loss: 0.0354 - val_loss: 0.0363\n",
      "ROC-AUC: 0.99145322\n",
      "\n",
      "\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.03654 to 0.03629, saving model to models/exp_model.h5\n",
      "Epoch 16/32\n",
      "561/561 [==============================] - 89s 158ms/step - loss: 0.0349 - val_loss: 0.0363\n",
      "ROC-AUC: 0.99144902\n",
      "\n",
      "\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/32\n",
      "561/561 [==============================] - 88s 158ms/step - loss: 0.0347 - val_loss: 0.0363\n",
      "ROC-AUC: 0.99153139\n",
      "\n",
      "\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.03629 to 0.03626, saving model to models/exp_model.h5\n",
      "Epoch 18/32\n",
      "561/561 [==============================] - 89s 158ms/step - loss: 0.0344 - val_loss: 0.0363\n",
      "ROC-AUC: 0.99145419\n",
      "\n",
      "\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/32\n",
      "561/561 [==============================] - 89s 158ms/step - loss: 0.0345 - val_loss: 0.0365\n",
      "ROC-AUC: 0.99141441\n",
      "\n",
      "\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/32\n",
      "561/561 [==============================] - 89s 158ms/step - loss: 0.0343 - val_loss: 0.0362\n",
      "ROC-AUC: 0.99145280\n",
      "\n",
      "\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.03626 to 0.03623, saving model to models/exp_model.h5\n",
      "Epoch 21/32\n",
      "561/561 [==============================] - 88s 158ms/step - loss: 0.0341 - val_loss: 0.0363\n",
      "ROC-AUC: 0.99147938\n",
      "\n",
      "\n",
      "\n",
      "Epoch 00021: val_loss did not improve\n",
      "Epoch 22/32\n",
      "561/561 [==============================] - 89s 158ms/step - loss: 0.0339 - val_loss: 0.0363\n",
      "ROC-AUC: 0.99146010\n",
      "\n",
      "\n",
      "\n",
      "Epoch 00022: val_loss did not improve\n",
      "Epoch 23/32\n",
      "561/561 [==============================] - 88s 158ms/step - loss: 0.0338 - val_loss: 0.0362\n",
      "ROC-AUC: 0.99145853\n",
      "\n",
      "\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.03623 to 0.03616, saving model to models/exp_model.h5\n",
      "Epoch 24/32\n",
      "561/561 [==============================] - 89s 158ms/step - loss: 0.0336 - val_loss: 0.0366\n",
      "ROC-AUC: 0.99145883\n",
      "\n",
      "\n",
      "\n",
      "Epoch 00024: val_loss did not improve\n",
      "Epoch 25/32\n",
      "561/561 [==============================] - 89s 158ms/step - loss: 0.0333 - val_loss: 0.0364\n",
      "ROC-AUC: 0.99150355\n",
      "\n",
      "\n",
      "\n",
      "Epoch 00025: val_loss did not improve\n",
      "Epoch 26/32\n",
      "561/561 [==============================] - 88s 158ms/step - loss: 0.0334 - val_loss: 0.0362\n",
      "ROC-AUC: 0.99148300\n",
      "\n",
      "\n",
      "\n",
      "Epoch 00026: val_loss did not improve\n",
      "Epoch 27/32\n",
      "561/561 [==============================] - 88s 158ms/step - loss: 0.0332 - val_loss: 0.0363\n",
      "ROC-AUC: 0.99139488\n",
      "\n",
      "\n",
      "\n",
      "Epoch 00027: val_loss did not improve\n",
      "Epoch 28/32\n",
      "561/561 [==============================] - 89s 158ms/step - loss: 0.0333 - val_loss: 0.0363\n",
      "ROC-AUC: 0.99142156\n",
      "\n",
      "\n",
      "\n",
      "Epoch 00028: val_loss did not improve\n",
      "Epoch 29/32\n",
      "561/561 [==============================] - 88s 158ms/step - loss: 0.0331 - val_loss: 0.0364\n",
      "ROC-AUC: 0.99138284\n",
      "\n",
      "\n",
      "\n",
      "Epoch 00029: val_loss did not improve\n",
      "Epoch 00029: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa4d4331be0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs=32\n",
    "def focal_loss_keras(y_true, y_pred):\n",
    "    return focal_loss(y_true, y_pred, 1.0, 0.25)\n",
    "\n",
    "clr = CyclicLR(base_lr=0.0001, max_lr=0.003, step_size=2*len(trn_seq), mode='triangular2')\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=optimizers.Nadam())\n",
    "# model.compile(loss=focal_loss_keras, optimizer=optimizers.Nadam())\n",
    "# model.compile(loss=art_loss, optimizer=optimizers.Nadam())\n",
    "\n",
    "model.fit_generator(\n",
    "    generator=trn_seq, steps_per_epoch=len(trn_seq),\n",
    "    validation_data=val_seq, validation_steps=len(val_seq),\n",
    "    initial_epoch=0, epochs=epochs, shuffle=False, verbose=1,\n",
    "    callbacks=[roc_auc_eval, model_checkpoint, clr, early_stop],\n",
    "#     callbacks=[roc_auc_eval, model_checkpoint, lr_reduce, early_stop],\n",
    "    use_multiprocessing=False, workers=cpu_cores, max_queue_size=8*cpu_cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-19T13:32:51.523888Z",
     "start_time": "2018-03-19T13:32:51.522322Z"
    }
   },
   "outputs": [],
   "source": [
    "# matplotlib.rcParams['figure.figsize'] = (24,8)\n",
    "\n",
    "# plt.xlabel('Learning Rate')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.plot(clr.history['lr'], clr.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-19T13:32:53.053043Z",
     "start_time": "2018-03-19T13:32:51.524779Z"
    }
   },
   "outputs": [],
   "source": [
    "del model\n",
    "model = load_model(models_dir+model_name+'.h5', compile=True, \n",
    "                   custom_objects={'Attention':Attention, 'AttentionWeightedAverage':AttentionWeightedAverage, \n",
    "                                   'focal_loss_keras':focal_loss_keras})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-19T13:33:11.215086Z",
     "start_time": "2018-03-19T13:32:53.054104Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic: 0.0644790639332429\n",
      "severe_toxic: 0.018332696792754948\n",
      "obscene: 0.03288714078080629\n",
      "threat: 0.005449422432136256\n",
      "insult: 0.04469190897774125\n",
      "identity_hate: 0.014778284843220347\n",
      "\n",
      "\n",
      "avg_loss: 0.030103086293317\n",
      "ROC AUC: 0.9945767464582523\n"
     ]
    }
   ],
   "source": [
    "Y_trn_pred = model.predict(X[trn_inx], batch_size=1024, verbose=0)\n",
    "losses = compute_losses(Y[trn_inx], Y_trn_pred, eps=1e-5)\n",
    "for label, label_loss in zip(inx2label, losses):\n",
    "    print(\"{}: {}\".format(label, label_loss))\n",
    "print()\n",
    "\n",
    "trn_loss = sum(losses)/len(losses)\n",
    "trn_auc = metrics.roc_auc_score(Y[trn_inx], Y_trn_pred)\n",
    "\n",
    "print()\n",
    "print(\"avg_loss: {}\".format(trn_loss))\n",
    "print(\"ROC AUC: {}\".format(trn_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-19T13:33:13.247619Z",
     "start_time": "2018-03-19T13:33:11.216309Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic: 0.08041645608308227\n",
      "severe_toxic: 0.019020405890425866\n",
      "obscene: 0.03850926609167262\n",
      "threat: 0.006716398895765416\n",
      "insult: 0.05601283061553505\n",
      "identity_hate: 0.01628399130231081\n",
      "\n",
      "\n",
      "avg_loss: 0.03615989147979867\n",
      "ROC AUC: 0.9914585334927861\n"
     ]
    }
   ],
   "source": [
    "Y_val_pred = model.predict(X[val_inx], batch_size=1024, verbose=0)\n",
    "losses = compute_losses(Y[val_inx], Y_val_pred, eps=1e-5)\n",
    "for label, label_loss in zip(inx2label, losses):\n",
    "    print(\"{}: {}\".format(label, label_loss))\n",
    "print()\n",
    "\n",
    "val_loss = sum(losses)/len(losses)\n",
    "val_auc = metrics.roc_auc_score(Y[val_inx], Y_val_pred)\n",
    "\n",
    "print()\n",
    "print(\"avg_loss: {}\".format(val_loss))\n",
    "print(\"ROC AUC: {}\".format(val_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-19T13:33:13.393523Z",
     "start_time": "2018-03-19T13:33:13.248647Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-4f76a9dad686>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-19T13:33:13.394072Z",
     "start_time": "2018-03-19T12:48:23.098Z"
    }
   },
   "outputs": [],
   "source": [
    "epochs1=epochs+8\n",
    "clr = CyclicLR(base_lr=0.0001, max_lr=0.001, step_size=2*len(trn_seq), mode='triangular2')\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=4, verbose=1, mode='auto')\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=optimizers.Adam())\n",
    "# model.compile(loss=art_loss, optimizer=optimizers.Adam())\n",
    "model.fit_generator(\n",
    "    generator=trn_seq, steps_per_epoch=len(trn_seq),\n",
    "    validation_data=val_seq, validation_steps=len(val_seq),\n",
    "    initial_epoch=epochs, epochs=epochs1, shuffle=False, verbose=1,\n",
    "#     class_weight=weights,\n",
    "    callbacks=[model_checkpoint, clr, early_stop, roc_auc_eval],\n",
    "#     callbacks=[model_checkpoint, lr_reduce, early_stop, roc_auc_eval],\n",
    "    use_multiprocessing=False, workers=cpu_cores, max_queue_size=8*cpu_cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-19T13:33:13.394687Z",
     "start_time": "2018-03-19T12:48:23.099Z"
    }
   },
   "outputs": [],
   "source": [
    "del model\n",
    "model = load_model(models_dir+model_name+'.h5', compile=True, \n",
    "                   custom_objects={'Attention':Attention, 'AttentionWeightedAverage':AttentionWeightedAverage, 'art_loss':art_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-19T13:33:13.395295Z",
     "start_time": "2018-03-19T12:48:23.101Z"
    }
   },
   "outputs": [],
   "source": [
    "Y_val_pred = model.predict(X[val_inx], batch_size=1024, verbose=0)\n",
    "losses = compute_losses(Y[val_inx], Y_val_pred, eps=1e-5)\n",
    "for label, label_loss in zip(inx2label, losses):\n",
    "    print(\"{}: {}\".format(label, label_loss))\n",
    "print()\n",
    "\n",
    "val_loss = sum(losses)/len(losses)\n",
    "val_auc = metrics.roc_auc_score(Y[val_inx], Y_val_pred)\n",
    "\n",
    "print()\n",
    "print(\"avg_loss: {}\".format(val_loss))\n",
    "print(\"ROC AUC: {}\".format(val_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-19T13:33:13.395872Z",
     "start_time": "2018-03-19T12:48:23.103Z"
    }
   },
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-19T13:33:13.396435Z",
     "start_time": "2018-03-19T12:48:23.106Z"
    }
   },
   "outputs": [],
   "source": [
    "# np.save(\"data/new_preprocessing_Y_val_pred.npy\", Y_val_pred)\n",
    "# np.save(\"data/new_preprocessing_val_inx.npy\", val_inx)\n",
    "# FileLink(\"data/new_preprocessing_val_inx.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-19T13:33:13.397045Z",
     "start_time": "2018-03-19T12:48:23.107Z"
    }
   },
   "outputs": [],
   "source": [
    "submission_name = 'exp__focal_loss_a1.0_g0.5__fasttext__d0.3__submission_'+str(round(val_loss, 5))+'_'+str(round(val_auc, 5))+'.csv'\n",
    "\n",
    "sample_submission = pd.read_csv(data_dir + 'sample_submission.csv')\n",
    "test_pred = model.predict(test_X, batch_size=1024, verbose=1)\n",
    "sample_submission[inx2label] = test_pred\n",
    "sample_submission.to_csv(results_dir+submission_name, index=False)\n",
    "\n",
    "FileLink(results_dir+submission_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-19T13:33:13.397596Z",
     "start_time": "2018-03-19T12:48:23.109Z"
    }
   },
   "outputs": [],
   "source": [
    "# pseudo\n",
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-19T13:33:13.398162Z",
     "start_time": "2018-03-19T12:48:23.111Z"
    }
   },
   "outputs": [],
   "source": [
    "model_loss_checkpoint = ModelCheckpoint(models_dir+model_name+'_pseudo.h5', monitor='val_loss', verbose=1, mode='min', save_best_only=True)\n",
    "model.compile(optimizer=optimizers.Nadam(0.0001), loss='binary_crossentropy')\n",
    "\n",
    "ps_epochs = 3\n",
    "for ps_inx in range(0, ps_epochs):  \n",
    "    test_Y = model.predict(test_X, batch_size=1024, verbose=1)\n",
    "    \n",
    "    trn_ps_seq = PseudoFeatureSequence(X[trn_inx], X_meta[trn_inx], Y[trn_inx], 182, \n",
    "                                       test_X, np.zeros((test_X.shape[0], 2)), test_Y, 74,  \n",
    "                                       shuffle=True)\n",
    "    model.fit_generator(\n",
    "        generator=trn_ps_seq, steps_per_epoch=len(trn_ps_seq),  \n",
    "        validation_data=val_seq, validation_steps=len(val_seq),\n",
    "        initial_epoch=epochs+ps_inx, epochs=epochs+ps_inx+1, \n",
    "        shuffle=False, verbose=1,\n",
    "        class_weight=weights,\n",
    "        callbacks=[model_loss_checkpoint, roc_auc_eval],\n",
    "        use_multiprocessing=False, workers=cpu_cores, max_queue_size=4*cpu_cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-19T13:33:13.398732Z",
     "start_time": "2018-03-19T12:48:23.114Z"
    }
   },
   "outputs": [],
   "source": [
    "Y_val_pred = model.predict(X[val_inx], batch_size=512, verbose=0)\n",
    "losses = compute_losses(Y[val_inx], Y_val_pred, eps=1e-5)\n",
    "\n",
    "val_loss = sum(losses)/len(losses)\n",
    "val_auc = metrics.roc_auc_score(Y[val_inx], Y_val_pred)\n",
    "\n",
    "print()\n",
    "print(\"avg_loss: {}\".format(val_loss))\n",
    "print(\"ROC AUC: {}\".format(val_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-19T13:33:13.399294Z",
     "start_time": "2018-03-19T12:48:23.115Z"
    }
   },
   "outputs": [],
   "source": [
    "submission_name = 'fasttext__gru__max_pool2d__submission_'+str(round(val_loss, 5))+'_'+str(round(val_auc, 5))+'_pseudo'+ps_epochs+'.csv'\n",
    "\n",
    "sample_submission = pd.read_csv(data_dir + 'sample_submission.csv')\n",
    "test_pred = model.predict(test_X, batch_size=1024, verbose=1)\n",
    "sample_submission[inx2label] = test_pred\n",
    "sample_submission.to_csv(results_dir+submission_name, index=False)\n",
    "\n",
    "FileLink(results_dir+submission_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-19T13:33:13.399844Z",
     "start_time": "2018-03-19T12:48:23.118Z"
    }
   },
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-19T13:33:13.400435Z",
     "start_time": "2018-03-19T12:48:23.120Z"
    }
   },
   "outputs": [],
   "source": [
    "# oov_word_hits = {}\n",
    "# oov_doc_hits = {}\n",
    "# for w in text_analyzer._oov_vec.keys():\n",
    "#     oov_word_hits[w] = text_analyzer.word_hits[w]\n",
    "#     oov_doc_hits[w] = text_analyzer.doc_hits[w]\n",
    "\n",
    "# pickle.dump(oov_word_hits, open('data/oov_word_hits.pkl', 'wb'))\n",
    "# pickle.dump(oov_doc_hits, open('data/oov_doc_hits.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
